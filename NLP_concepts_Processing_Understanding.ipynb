{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e170bb5a",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## Sentence tokenization\n",
    "• sent_tokenize\n",
    "\n",
    "• Pretrained sentence tokenization models\n",
    "\n",
    "• PunktSentenceTokenizer\n",
    "\n",
    "• RegexpTokenizer\n",
    "\n",
    "## Word tokenization\n",
    "• word_tokenize\n",
    "\n",
    "• TreebankWordTokenizer\n",
    "\n",
    "• TokTokTokenizer\n",
    "\n",
    "• RegexpTokenizer\n",
    "\n",
    "• Inherited tokenizers from RegexpTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db1280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0984e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading text corpora\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \"\n",
    "                \"The US has unveiled the world's most powerful supercomputer called 'Summit', \"\n",
    "                \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "                \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,\"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "                \"which reportedly take up the size of two tennis courts.\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec418707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144395"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total characters in Alice in Wonderland\n",
    "len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd981c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 100 characters in the corpus\n",
    "alice[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe18f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 4\n",
      "Sample text sentences :-\n",
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\"\n",
      " \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\"\n",
      " 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.'\n",
      " \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\"\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "print('Total sentences in sample_text:', len(sample_sentences))\n",
    "print('Sample text sentences :-')\n",
    "print(np.array(sample_sentences))\n",
    "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
    "print('First 5 sentences in alice:-')\n",
    "print(np.array(alice_sentences[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e5b06bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
       "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73593009",
   "metadata": {},
   "source": [
    "## Removing Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8acd5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba29cc",
   "metadata": {},
   "source": [
    "## Expanding contractions\n",
    "Examples include “is not” to “isn’t” and\n",
    "“will not” to “won’t”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc28dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c68a70ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello you are you?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"Hello you're you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd875912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\",remove_digits=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7319a",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Word stems are also\n",
    "often known as the base form of a word and we can create new words by attaching affixes\n",
    "to them. This process is known as inflection. The reverse of this is obtaining the base\n",
    "form of a word from its inflected form and this is known as stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e6d118b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These stemmers are implemented in the stem module, which inherits the StemmerI interface in the nltk stem.api module\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14e96162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fad575",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "The process of lemmatization is very similar to stemming, where we remove word affixes\n",
    "to get to a base form of the word. However in this case, this base form is also known\n",
    "as the root word but not the root stem.\n",
    "\n",
    "This form will always be present in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b4dcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b849d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('cars', 'n')) # n noun, v verb, a adjective\n",
    "print(wnl.lemmatize('men', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7f7ce75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6443d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "510237e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() got an unexpected keyword argument 'tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-cd4e3d4cb29d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'My system keeps crashing his crashed yesterday, ours crashes daily'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: load() got an unexpected keyword argument 'tag'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core', tag=True, entity=True)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f990ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfca89bb",
   "metadata": {},
   "source": [
    "# Text Normalizer: All together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "237dea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9d0e520",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strip_html_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-39d7fcc948f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m {'Original': sample_text,\n\u001b[1;32m----> 2\u001b[1;33m  'Processed': normalize_corpus([sample_text])[0]}\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-e723319965d1>\u001b[0m in \u001b[0;36mnormalize_corpus\u001b[1;34m(corpus, html_stripping, contraction_expansion, accented_char_removal, text_lower_case, text_lemmatization, special_char_removal, stopword_removal, remove_digits)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# strip HTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhtml_stripping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrip_html_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m# remove accented characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maccented_char_removal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'strip_html_tags' is not defined"
     ]
    }
   ],
   "source": [
    "{'Original': sample_text,\n",
    " 'Processed': normalize_corpus([sample_text])[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30bfa59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
