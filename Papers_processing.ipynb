{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d50925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11d84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'dataset/nipstxt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ec1690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['idx', 'MATLAB_NOTES', 'nips00', 'nips01', 'nips02', 'nips03', 'nips04', 'nips05', 'nips06', 'nips07', 'nips08', 'nips09', 'nips10', 'nips11', 'nips12', 'orig', 'RAW_DATA_NOTES', 'README_yann']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09540eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "# Read all texts into a list.\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8',\n",
    "            errors='ignore', mode='r+') as f:\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ed1ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "CONNECTIVITY VERSUS ENTROPY \n",
      "Yaser S. Abu-Mostafa \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "How does the connectivity of a neural network (number of synapses per \n",
      "neuron) relate to the complexity of the problems it can handle (measured by \n",
      "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
      "functions can be implemented using a circuit with very low connectivity (e.g., \n",
      "using two-input NAND gates). However, for a network that learns a problem \n",
      "from examples using a local learning rule, we prove that the entropy of the \n",
      "problem becomes a lower bound for the connectivity of the network. \n",
      "INTRODUCTION \n",
      "The most distinguishing feature of neural networks is their ability to spon- \n",
      "taneously learn the desired function from 'training' samples, i.e., their ability \n",
      "to program themselves. Clearly, a given neural network cannot just learn any \n",
      "function, there must be some restrictions on which networks can learn which \n",
      "functions. One obv\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dcda1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]','', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "#norm_corpus = normalize_corpus(list(df['description']))\n",
    "#len(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd3f6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_papers2 = normalize_corpus(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Text Wrangling\n",
    "\n",
    "%%time\n",
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "#wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "#wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if\n",
    "        not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in\n",
    "        stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "    return norm_papers\n",
    "\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c398b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "#Basic Text Wrangling\n",
    "\n",
    "%%time\n",
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if\n",
    "        not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in\n",
    "        stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "    return norm_papers\n",
    "\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44403bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connectivity', 'versus', 'entropy', 'yaser', 'abu', 'mostafa', 'california', 'institute', 'technology', 'pasadena', 'ca', 'abstract', 'doe', 'connectivity', 'neural', 'network', 'number', 'synapsis', 'per', 'neuron', 'relate', 'complexity', 'problem', 'handle', 'measured', 'entropy', 'switching', 'theory', 'would', 'suggest', 'relation', 'since', 'boolean', 'function', 'implemented', 'using', 'circuit', 'low', 'connectivity', 'using', 'two', 'input', 'nand', 'gate', 'however', 'network', 'learns', 'problem', 'example', 'using']\n"
     ]
    }
   ],
   "source": [
    "# viewing a processed paper\n",
    "print(norm_papers[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3690b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n"
     ]
    }
   ],
   "source": [
    "print(len(norm_papers2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d6a76be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1740, 336664)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tfidf_matrix = tf.fit_transform(norm_papers2)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f94527c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1730</th>\n",
       "      <th>1731</th>\n",
       "      <th>1732</th>\n",
       "      <th>1733</th>\n",
       "      <th>1734</th>\n",
       "      <th>1735</th>\n",
       "      <th>1736</th>\n",
       "      <th>1737</th>\n",
       "      <th>1738</th>\n",
       "      <th>1739</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083499</td>\n",
       "      <td>0.078747</td>\n",
       "      <td>0.080570</td>\n",
       "      <td>0.090463</td>\n",
       "      <td>0.048501</td>\n",
       "      <td>0.017875</td>\n",
       "      <td>0.046790</td>\n",
       "      <td>0.036182</td>\n",
       "      <td>0.059026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049917</td>\n",
       "      <td>0.029344</td>\n",
       "      <td>0.088023</td>\n",
       "      <td>0.041253</td>\n",
       "      <td>0.020395</td>\n",
       "      <td>0.029355</td>\n",
       "      <td>0.055622</td>\n",
       "      <td>0.040407</td>\n",
       "      <td>0.059851</td>\n",
       "      <td>0.086451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.083499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110948</td>\n",
       "      <td>0.127377</td>\n",
       "      <td>0.088682</td>\n",
       "      <td>0.096855</td>\n",
       "      <td>0.034394</td>\n",
       "      <td>0.150005</td>\n",
       "      <td>0.051938</td>\n",
       "      <td>0.077650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062788</td>\n",
       "      <td>0.055224</td>\n",
       "      <td>0.051308</td>\n",
       "      <td>0.072480</td>\n",
       "      <td>0.048546</td>\n",
       "      <td>0.064043</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>0.073104</td>\n",
       "      <td>0.062457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.078747</td>\n",
       "      <td>0.110948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078376</td>\n",
       "      <td>0.096206</td>\n",
       "      <td>0.123258</td>\n",
       "      <td>0.027833</td>\n",
       "      <td>0.111593</td>\n",
       "      <td>0.040246</td>\n",
       "      <td>0.081065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052032</td>\n",
       "      <td>0.037419</td>\n",
       "      <td>0.049027</td>\n",
       "      <td>0.059544</td>\n",
       "      <td>0.045008</td>\n",
       "      <td>0.046485</td>\n",
       "      <td>0.036062</td>\n",
       "      <td>0.061051</td>\n",
       "      <td>0.053580</td>\n",
       "      <td>0.043428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080570</td>\n",
       "      <td>0.127377</td>\n",
       "      <td>0.078376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082286</td>\n",
       "      <td>0.077836</td>\n",
       "      <td>0.031717</td>\n",
       "      <td>0.104921</td>\n",
       "      <td>0.049698</td>\n",
       "      <td>0.073722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038507</td>\n",
       "      <td>0.025172</td>\n",
       "      <td>0.051936</td>\n",
       "      <td>0.041394</td>\n",
       "      <td>0.063053</td>\n",
       "      <td>0.027442</td>\n",
       "      <td>0.026377</td>\n",
       "      <td>0.060868</td>\n",
       "      <td>0.032222</td>\n",
       "      <td>0.035849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.090463</td>\n",
       "      <td>0.088682</td>\n",
       "      <td>0.096206</td>\n",
       "      <td>0.082286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.087120</td>\n",
       "      <td>0.035405</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>0.052509</td>\n",
       "      <td>0.058018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076428</td>\n",
       "      <td>0.039447</td>\n",
       "      <td>0.047417</td>\n",
       "      <td>0.057946</td>\n",
       "      <td>0.042237</td>\n",
       "      <td>0.047741</td>\n",
       "      <td>0.039756</td>\n",
       "      <td>0.071984</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.044160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1740 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  1.000000  0.083499  0.078747  0.080570  0.090463  0.048501  0.017875   \n",
       "1  0.083499  1.000000  0.110948  0.127377  0.088682  0.096855  0.034394   \n",
       "2  0.078747  0.110948  1.000000  0.078376  0.096206  0.123258  0.027833   \n",
       "3  0.080570  0.127377  0.078376  1.000000  0.082286  0.077836  0.031717   \n",
       "4  0.090463  0.088682  0.096206  0.082286  1.000000  0.087120  0.035405   \n",
       "\n",
       "       7         8         9     ...      1730      1731      1732      1733  \\\n",
       "0  0.046790  0.036182  0.059026  ...  0.049917  0.029344  0.088023  0.041253   \n",
       "1  0.150005  0.051938  0.077650  ...  0.062788  0.055224  0.051308  0.072480   \n",
       "2  0.111593  0.040246  0.081065  ...  0.052032  0.037419  0.049027  0.059544   \n",
       "3  0.104921  0.049698  0.073722  ...  0.038507  0.025172  0.051936  0.041394   \n",
       "4  0.102299  0.052509  0.058018  ...  0.076428  0.039447  0.047417  0.057946   \n",
       "\n",
       "       1734      1735      1736      1737      1738      1739  \n",
       "0  0.020395  0.029355  0.055622  0.040407  0.059851  0.086451  \n",
       "1  0.048546  0.064043  0.038762  0.089183  0.073104  0.062457  \n",
       "2  0.045008  0.046485  0.036062  0.061051  0.053580  0.043428  \n",
       "3  0.063053  0.027442  0.026377  0.060868  0.032222  0.035849  \n",
       "4  0.042237  0.047741  0.039756  0.071984  0.048200  0.044160  \n",
       "\n",
       "[5 rows x 1740 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "doc_sim = cosine_similarity(tfidf_matrix)\n",
    "doc_sim_df = pd.DataFrame(doc_sim)\n",
    "doc_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83f85bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.08349861, 0.07874696, ..., 0.04040665, 0.05985071,\n",
       "       0.08645105])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper0_similarities = doc_sim_df.iloc[0].values\n",
    "paper0_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "745a190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  29, 1633, 1613,  643,  542], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_papers_idxs = np.argsort(-paper0_similarities)[1:6]\n",
    "similar_papers_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "497f09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10960903594764616"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper0_similarities[1613]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5b485ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"278 hopfield model multilevel neurons michael fleisher depatmaent electrical engineering technion israel institute technology haifa 32000 israel abstract hopfield neural network model associative memory generaezed generalization replaces two state neurons neurons taking richer set values two classes neuron input output relations developed guaranteeing convergence stable states fust class continuous rela tions second class allowed quantization rules neurons information capacity networks second class found order 3 bits network neurons generalization sum outer products learning rule developed investigated well american institute physics 1988 279 introduction ability perform collective computation distributed system flexible structure without global synchronization important engineering objective hopfields neural network 1 model associative content addressable memory important property hopfield neural network guaranteed convergence stable states interpreted stored memories work introduce generalization hopfield model allowing outputs neurons take richer set values hopfields original binary neurons sufficient conditions preserving convergence property developed neuron input output relations two classes relations obtained first introduces neurons simulate multi thres hold functions networks neurons called quantized neural networks qnn secotad class introduces continuous neuron input output relations networks neurons called continuous neural networks cnn section ii introduce hopfields neural network show convergence property cnn introduced section hi sufficient condition neuron input output continuous relations developed preserving convergence section iv qnn introduced input output rela tions analyzed manner hi section iv look qnn using definition information capacity neural networks 2 obtain tight asymptotic estimate capacity qffq n n neurons section vi generalized sum outer products learning qnn section vii discussion ii hopfield neural network neural network consists n pairwise connected neurons th neuron one two states x 1 x 1 connections flxed real numbers denoted wij connection neuron neuron j define state vector x binary vector whose th component corresponds state th neuron randomly asynchronously neuron examines input decides next output following manner let threshold voltage th neuron weighted sum present n1 neuron outputs compose th neuron input 280 greater equal next x xi 1 xi 1 action given 1 give following theorem n xi sgn 2 wijxjti theorem 1 1 network described symmetric llvijw fi zero diagonal vii0 connection matrix w convergence property pmo f define quantity inn n ex xixj tixi jl i1 2 show e x decrea result action network suppose x k changed xff xk l k resulting change e given n axk e wkjxjt 3 jl eq 3 correct restrictions w tetra brackets exactly argument sgn function 1 therefore signs ax k tetra brackets z k 0 get ae 0 combining fact e j bounded shows eventually network remain local minimum fie completes proof technique used proof theorem 1 important tool analyzing neural networks network particular underlying e x function used solve optimization problems e object optimization thus see another use neural networks 281 cnn ask following question change sgn function 1 without affect ing convergence property new action rule th neuron n xi f z wijxj 4 jl attention focused possible choices forfi following theorem gives part answer theorem 2 network described 4 symmetric zero diagonal w convergence property fi strictly increasing bounded proof define 1 n n n xi e z wijxixj 4 filtlatl j i10 5 show e decrease since e bounded boundedhess fi theorem proved xi usingixi f ilu du n il axk 6 ) using intermediale value theorem get ae = - axi\\x7f [ \\x7f wkjxj-gk ( c ) = -axi\\x7f [ f ff x ( xk +axi\\x7f ) -f ff l ( c ) ] j=l ( 7 ) 282 c point x k xk+\\x7f k. , ff /\\x7fk > 0 c -\\x7f x k +l\\x7f ( k = > f\\x7f-l ( c ) -\\x7f f\\x7f-l ( x k +ax k ) term brackets greater equal zero '' - > ae _ < 0 . similar argument holds ax k < 0 ( course ax k : =0 = > ae =0 ) . completes proof . remarks : ( ) strictly increasing hounded neuron relalions whole class relations conserving conver- gence property . seen immediately fact hopfield 's original model ( 1 ) ' class . co ) e ( x_ ) c.n.n . coincides hopfield 's continuous neural network [ 3 ] . difference two networks lies updating scheme . c.n.n . neurons update outputs moments examine inputs [ 3 ] updating form set differential equa- tions featuring time evolution network outputs . ( c ) boundedness requirement neuron relations results boundedness e ( \\x7f. ) . possible impose restrictions w resulting unbounded neuron relations keeping e ( x ) hounded ( ) . done [ 4 ] neurons exhibit linear relations . iv . q.n.n . develop class quantization rules neurons , keeping convergence property . denote set possible neuron outputs yo < 1 < ' .. < yn set threshold values 1 < 2 < ' ' ' < tn action neurons given n xi +-1 < z wij\\x7fj - < tl+l /=o , ... , n ( 8 ) j= ! = -o % n + l = +oo . following theorem gives class quantization rules convergence property . 283 quantization rule neurons increasing step functioa yo < yi < ' '' yn ; tl < ... < n yields network convergence property ( g synunetric zero diagonal ) . ( 9 ) proof proceed prove . define 1n n n n e ( x ) =- i5 z wijxixj + tg ( xi ) + zdxi j=l i=1 i=1 ( lo ) g ( x ) piecewise linear convex u function defined relation g ( yi ) -g ( yi-1 ) +d=t l=l , ... , n ( 10 yi-yi_i show af\\x7f _ < 0. suppose change occurred x k thatx k =yi_i , x ; =yi . ae = -zsx k [ - j=l ( xd-c ( xk ) ( 12 ) similar argument follows whenxk=yi , x\\x7f=yi_ 1 < x k . bigger change inx k ( yi yj -j > 1 ) yields result since viewed sequence -j changes yi yj resulting ae - < 0 . proof completed noting zsx k -- -- o= -- > ae =0 e ( x ) 284 corollary hopfield 's original model special case ( 9 ) . v. information capacity q.n.n . use d\\x7ffinition [ 2 ] information capacity q.n.n . definition 1 information capacity q.n.n . ( bits ) log ( base 2 ) number distinguishable networks n neurons . two networks distinguishable observing state transitions neurons yields different observations . hopfield 's original model shown [ 2 ] capacity c network n neurons bounded c < log ( 2 ( n-1 ) ' ) ' v = 0 ( n3 ) b. also shown c - > \\x7f ( n3 ) b thus exactly order n3b . obvious case ( contains original model ) must c - > \\x7f ( n3 ) b well ( since lower bound decrease richer case ) . shown appendix number multi threshold functions n-1 variables n+l output levels ( n+l ) n2+n+i since n neurons ( ( n + ly v'+n +yv ti.ghe network thus c _ ( log ( ( n+l ) ' v'-+\\x7fv+\\x7ff = 0 ( n3 ) b ( 14 ) o\\x7f , c exactly 0 ( n 3 ) b. fact , rise c probably factor 0 ( log2n ) seen upper bound . vi . `` outer product '' learning rule hopfield 's original network two state neurons ( taking values 1 ) natural exten- sively investigated [ ] , [ 1 , [ ] learning mle called sum outer products construction . 1 tc 1 -- 1 x 1 , x k \\x7fre desired stable states network . well-known result ( 15 ) asymptotic capacity k network 285 n-1 k = \\x7f + 1 ( 16 ) 41ogn section introduce natural generalization ( 15 ) prove similar result asymp- totic capacity . first limit possible quantization rules : x = f ( tl ) = ' yo l > ui - > yn tn+l > ui - > tn ( 17 ) withyo < `` ' < yn =-oo ; tn+l =oo ( ) n+l even co ) v yi \\x7f ( c ) yi = - yn-i neat state desired stable vectors x 1 , - x_ k component picked independently random { yo , '' ' ym } equal probability . thus , k n components thex 's zero mean i.i.d random variables . modified learning rule wij = z xl ' ( 18 ) /=1 note orxi \\x7f { +1 , -1 } ( 18 ) identical ( 16 ) . define 286 =max \\x7f , j iyj l state proposition : asymptotic capacity network given n 16a 2 \\x7f log n proof : ( 19 ) define \\x7fk vectors chosen randomly p ( k , n ) = pr [ stable states w described } ( ) p ( k , n ) = 1 -pr ( l ) a/j ) > 1 -\\x7fpr ( aij ) =1 , ... , n ( 20 ) j=l ..... k aij event th component j th vector error . concentrate event 11 w.l.g . input u 1 x ' presented given n k-1 1 kn , , . u\\x7f= z w , \\x7f.xf =x\\x7f +\\x7fx\\x7f +- z z x\\x7f xf ( 2 , ) j=l n n /=2 j=2 xj first term mapped ( 17 ) itseft corresponds desired signal . last term sum ( k-1 ) ( n-1 ) i.i.d zero mean random variables corresponds noise . 287 k-1 x \\x7f disposed assuming middle term n . ( zero diagonal choice w ( using ( 18 ) ; \\x7fj ) term appear ) . pr ( ) =pr { noise gets us range } denoting noise pt ( ) < pr ( > \\x7f- ) < 2exp - ( k-1 ) ( n-1 ) 4a 2 first inequality der'tuition ayand second uses lemma [ 6 ] p. 58. thus get p ( k , n ) > 1 - k n 2exp { - 8 ( k-1 ) ( n-1 ) a2 substituting ( 19 ) taking n -- > oo get p ( k , n ) -- > 1 completes proof . ( 23 ) vii . discussion two classes generalization hopfield neural network model presented . give remarks : ( ) combination neurons two classes convergence property well . ( b ) definition information capacity c.n.n . useless since full observation pos- sible state transitions network impossible . 288 appendix prove fouowing theorem . theorem upper bound number malt/ threshold functions n inputs \\x7fff domain ( ( n +l ) tv possible poinls ) cn solution lhe recurrence relat/on proof c # - ' + . points let us look n dimensional weight space w. input point x divides weight space n theorem proved . solution recurrence case m= ( n+l ) n ( possible points ) bound number mult/threshold functions variables equal n-l i=l ( n+l ) n-1 ] ni < - result used established . c\\x7f= c\\x7f -i + n .c\\x7f_\\x7f l \\x7fv-\\x7f m-i isc\\x7f= ( n+l ) { n+l regions n parallel hyporplanes \\x7f wixi=t k k-1 , ... , n. keep adding points i=l way new tt hyperplanes corresponding added point partition w space many regions possible . assume m-1 points made cn m-i regions add 'th point . hyperplane ( n ) divided cnm_\\x7f 1 regiont ( n- ] dimensional space divided ( m-1 ) n hyperlines ) . thus passing n hyperplanes : 289 references [ ] hopfield j. j. , `` neural networks physical systems emergent collective computational abili- ties '' , proc . nat . acad . sci . usa , vol . 79 ( 1982 ) , pp . 2554-2558 . [ 2 ] abu-mostafa y.s . jacques j. st. , `` inforotation capacity hopfield model '' , ieee trans . info . theory , vol . it-31 ( 1985 , pp . 461-464 . [ 3 ] hopfield j. j. , `` neurons graded response collective computational properties like two state neurons '' , proc . nat . acad . sci . usa , vol . 81 ( 1984 ) . [ 4 ] fleisher m. , `` fast processing autoregressive signals neural network '' , presented ieee conference , israel 1987 . [ 5 ] levin , e. , private communication . [ 6 ] petroy , `` sums independent random variables '' .\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_papers2[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392afe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
