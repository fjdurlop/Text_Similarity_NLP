{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b99610b",
   "metadata": {},
   "source": [
    "# Final project NLP\n",
    "\n",
    "## Text similarity using scientific papers\n",
    "\n",
    "## Proposed project pipeline\n",
    "\n",
    "1. Get automatically text from PDF\n",
    "\t\n",
    "2. Tokenization\n",
    "3. Text Cleaning\n",
    "4. Vectorization\n",
    "5. Feature engineering step\n",
    "6. Algorithms \n",
    "    - AI algorithms\n",
    "    - Similarities\n",
    "7. Represent results\n",
    "8. Interpret results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc295055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f812bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c999d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=reload(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a254e6ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text1 = u.get_text_from_pdf('../references/ref1.pdf')\n",
    "text2 = u.get_text_from_pdf('../references/ref2.pdf')\n",
    "text3 = u.get_text_from_pdf('../references/ref11.pdf')\n",
    "\n",
    "#text2 = u.get_text_from_pdf('../references/book_Text_analytics.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "563fb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array([text1,text2,text3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f59f32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.DataFrame({'Document': corpus})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6d1556c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() got an unexpected keyword argument 'tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-7b5b21043a00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"spec not found for the module {name!r}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;31m# The module may have replaced itself in sys.modules!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_exec\u001b[1;34m(spec, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Escuela\\procesamiento_textos\\Text Similarity\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;31m#text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: load() got an unexpected keyword argument 'tag'"
     ]
    }
   ],
   "source": [
    "u = reload(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3071f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = u.corpus_to_df([text1,text2,text3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "462eef3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019 IEEE/ACM 41st International Conference on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comparative analysis of the government plans o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J.Inst.Eng.IndiaSer.B\\nhttps://doi.org/10.1007...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document\n",
       "0  2019 IEEE/ACM 41st International Conference on...\n",
       "1  Comparative analysis of the government plans o...\n",
       "2  J.Inst.Eng.IndiaSer.B\\nhttps://doi.org/10.1007..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "29897c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=reload(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cc47d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ieeeacm st international conference software engineering icse ieeeacm st international conference software engineering icse guiding deep learning system testing using surprise adequacy jinhan kim robert feldt shin yoo school computing dept computer science engineering dept software engineering kaist chalmers university blekinge inst technology daejeon republic korea gothenburg sweden karlskrona sweden jinhankim shin yoo kaist ac kr robert feldt chalmers se robert feldt bth se abstractdeep learning dl system rapidly anothervehicletoyieldinoneoftherarercircumstances adopted safety security critical domain urgently calling crashed vehicle expectation proved way test correctness robustness testing incorrect [ ] urgent need verify validate dl system ha traditionally relied manual collection behaviour dl system however significant part labelling data recently number coverage criterion based neuron activation value proposed criterion existing software testing technique directly applicable essentiallycountthenumberofneuronswhoseactivationduring dl system notably traditional whitebox testing execution dl system satisfied certain property technique aim increase structural coverage [ ] predefined threshold however existing cover veryusefulfordlsystems astheirbehaviourisnotexplicitly age criterion sufficiently fine grained capture subtle encoded control flow structure behavioursexhibitedbydlsystems moreover evaluationshave focused showing correlation adversarial example number novel approach towards testing veri proposed criterion rather evaluating guiding fication dl system recently proposed fill use actual testing dl system propose novel test gap [ ] [ ] [ ] [ ] technique adequacy criterion testing dl system called surprise share two assumption first assumption essentially adequacy deep learning system sadl based generalisation essence metamorphic testing [ ] behaviour dl system respect training data measure surprise input difference two input dl system similar respect dl system behaviour input training humansense theoutputsshouldalsobesimilar forexample data e wa learnt training subsequently deeptest [ ] check whether autonomous driving system developthisasanadequacycriterion agoodtestinputshouldbe behavesinthesamewaywhentheinputimageistransformed sufficientlybutnotovertlysurprisingcomparedtotrainingdata asifthesamesceneisunderadifferentweathercondition empirical evaluation using range dl system simple image classifier autonomous driving car platform show second assumption also based traditional software systematic sampling input based surprise testing result [ ] diverse set input improveclassificationaccuracyofdlsystemsagainstadversarial effective testing dl system one perform example via retraining forexample deepxplore [ ] presentedtheneuroncoverage index termstest adequacy deep learning system ratio neuron whose activation value predefined threshold measure diversity neuron introduction behaviour subsequently showed input violating deeplearning dl [ ] systemshaveachievedsignificant first assumption also increase neuron coverage progress many domain including image recognition [ ] recently introduced technique made sig [ ] [ ] speech recognition [ ] machine transla nificant advance manual ad hoc testing dl system tion [ ] [ ] based capability match even major limitation coverage criterion proposed surpass human performance dl system increasingly fararenotsufficientlyfinegrained inasensethatallofthem adopted part larger system safety simply count neuron whose activation value satisfy certain securitycriticaldomainssuchasautonomousdriving [ ] [ ] condition aggregation counting doe allow malware detection [ ] tester quantify test effectiveness given input adoption dl system call new challenge set itconveyslittleinformationaboutindividualinputs critically important larger system example itisnotimmediatelyclearwhenaninputwithhigher correctandpredictable despitetheirimpressive experimental nc considered better another lower nc performance dl system known exhibit unexpected certain input may naturally activate neuron behaviour certain circumstance example threshold others vice versa another reported incident autonomous driving vehicle expected example kmultisection neuron coverage [ ] iieeeeee ddooii iiccssee authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply partitionstherangesofactivationvaluesofneurons observed ing additional adversarial example sampling additional training k bucket count number total input broader sa value improve accuracy bucketscoveredbyasetofinputs whenmeasuredforasingle retraining input thecoveragewillbeeither iftheinputactivateseach weundertakeallourexperimentsusingpubliclyavailable k neuronwithavaluefromoneofthekbuckets orsmallerthan dlsystemsrangingfromsmallbenchmarks mnistand thatifsomeneuronsactivateoutsidetherangeobservedduring cifar large system autonomous driving training theinformationabouthowfarsuchactivations vehicle dave [ ] chauffeur [ ] implemen go beyond observed range lost aggregation making tations available online hard evaluate relative value input test remaining paper organised follows sec adequacy criterion practically useful able tion ii introduces surprise adequacy dl system sadl toguidetheselectionofindividualinputs eventuallyresulting two variant sadl presented along algorithm improvement accuracy dl system measure section iii set research question investigation section iv describes experimental setup overcome limitation propose new test empirical evaluation section v present result adequacy dl system called surprise adequacy dl empiricalevaluations sectionviaddressesthreatstovalidity system sadl intuitively good test input set sectionviipresentsrelatedwork andsectionviiiconcludes dl system systematically diversified include input ranging similar training data ii surpriseadequacyfordeeplearningsystems significantly different adversarial individual input existing test adequacy criterion dl system aim granularity sadl measure surprising input measure diversity input set neuron coverage dl system respect data system wa trained nc [ ] posit higher number neuron theactualmeasureofsurprisecanbeeitherbasedonthe activated predefined threshold diverse likelihood system seen similar input inputthedlsystemhasbeenexecutedwith deepgauge [ ] training respect probability density distribution proposed range finer grained adequacy criterion including extrapolated training process using kernel density kmultisection neuron coverage measure ratio estimation [ ] distance vector representing activation value bucket covered across theneuronactivationtracesofthegiveninputandthetraining neuron andneuronboundarycoverage whichmeasuresthe data heresimplyusingeuclideandistance subsequently ratioofneuronsthatareactivatedbeyondtherangesobserved surpriseadequacy sa ofasetoftestinputsismeasuredby training therangeofindividualsurprisevaluesthesetcovers weshow argue diversity testing dl system sadl sufficiently fine grained training adversarial meaningful measured respect training example classifier based sadl value produce data asdlsystemsarelikelytobemoreerrorproneforinputs higher accuracy compared state art also unfamiliar e diverse furthermore neuron show sampling input according sadl retraining activation threshold beyond observed range may dl system result higher accuracy thus showing closely related diversity given input sadlisanindependentvariablethatcanpositivelyaffectthe measure degree activation network effectiveness dl system testing one input differs activation another input technical contribution paper follows fundamentally discretisations utilize fact propose sadl fine grained test adequacy metric neuron activation continuous quantity contrast thatmeasuresthesurpriseofaninput e thedifference aim define adequacy criterion quantitatively behaviour dl system given input measure behavioural difference observed given set training data two concrete instance sadl input relative training data areproposedbasedondifferentwaystoquantifysurprise activation trace surprise adequacy shown correlated existing coverage criterion dl system let n n n set neuron constitutes show sadl sufficiently fine grained cap dl system let x x x set input turing behaviour dl system training highly denote activation value single neuron n accurate adversarial example classifier adversarial respect input x n x ordered sub set example classifier show much neuron let n n n x denote vector activation rocauc score applied mnist [ ] value eachelementcorrespondingtoanindividualneuronin cifar [ ] dataset respectively n cardinality n x equal n call n x show sadl metric used sample activation trace x neuron n similarly effective test input set retraining dl system us letan x beasetofactivationtraces observedoverneurons inn forasetofinputsx x n x xx experimentsshowbenefitsofdiversityforgeneraltesting [ ] andbenefits ofascaleofdistancesoftestinputsforrobustnesstestingintroducedin [ ] pleaserefertohttps github comcoinsesadl authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply note activation trace trivially available since want measure surprise input x execution network given input needametricthatincreaseswhenprobabilitydensitydecreases since behaviour dl system driven along data e input rarer compared training data flow controlflow assume activation trace vice versa e input similar training data observedoverallnwithrespecttox x fullycaptures adopting common approach converting probability density behaviour dl system investigation measure rareness [ ] [ ] define lsa executed using x negative log density surprise adequacy sa aim measure relative nov elty e surprise given new input respect lsa x log f x input used training given training set first computean byrecordingactivationvaluesofallneurons note extra information input type used tomakelsamoreprecise forexample givenadlclassifier usingeveryinputinthetrainingdataset subsequently givena expect input share class label newinputx wemeasurehowsurprisingxiswhencompared comparing activation trace x sreimplialcairnagtts wwiethca nxexpltoit thdis xb ycomc puftoinrgcllasssacp ewrcelaussse quantitative similarity measure called surprise adequacy perclass lsa dl classifier empirical evaluation sa introduce two variant sa different way measuring similarity x note certain type dl task allow u focus xb xb partsofthetrainingsetttogetmorepreciseandmeaningful measurement sa example suppose testing c boundary classifier new input x classified dl x b b learnt dl system investigation class c case c x surpriseofxismoremeaningfullymeasuredagainstan tc xa tc subset member classified xa c basically input might surprising example fig anexampleofdistancebasedsa blackdotsrepresent class c even surprising relation full set training data input whereas grey dot represent training example ofnewinputs xandx comparedtodistancesfromxaand b likelihoodbased surprise adequacy xa toclassc atofx isfartheroutfromclassc thanthat kernel density estimation kde [ ] way esti ofx e ab ab seeequations consequently decide x surprising x w r class c mating probability density function given random variable resulting density function allows estimation ofrelativelikelihoodofaspecificvalueoftherandomvariable c distancebased surprise adequacy likelihoodbased sa lsa us kde estimate proba analternativetolsaissimplytousethedistancebetween bility density activation value obtains measure surprise define distance surprise new input respect estimated basedsurprise adequacy dsa usingtheeuclideandistance density extension existing work us kde new input x observed detect adversarial example [ ] reduce dimensionality training distance metric dsa ideally suited computational cost consider neuron exploit boundary input seen selected layer nl n yield set activation classificationexampleinfigure bycomparingthedistances trace anl x reduce computational cost e distance new input filter neuron whose activation value show variance thereferencepoint whichisthenearestatoftrainingdatain lowerthanapredefinedthreshold astheseneuronswillnot c distance b b e distance c measured contribute much information kde cardinality reference point get sense close class tracewillbe nl givenabandwidthmatrixh andgaussian boundary new input posit classification kernelfunctionk theactivationtraceofthenewinputx problem input closer class boundary xi kde produce density function fa follows surprising valuable term test input diversity hand task without boundary cid input prediction appropriate steering angle f x anl xitkh nl x nl xi awuittohnonmoocluasssdbriovuinngdacriaers dansaatmoafyannoetwbienpeuatsibleyinagppfalircfarbolme sake simplicity assume possible get another training input doe guarantee new completeactivationtracesfromalltheneuronsinadlsystem fornetwork input surprising may still located crowded architecture loop recurrent neural net rnns [ ] part space consequently apply dsa possibletounrolltheloopsuptoapredefinedbound [ ] classification task effective however themainideaisgeneralandother specificvariantswouldresult ifusingothersimilarityfunctions lsa see section va vb detail authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply let u assume dl system consists surprising however input arbitrarily high sa value set neuron n trained classification task may simply irrelevant least le interesting set class c using training dataset given set problem domain e g image traffic sign activationtracesan anewinputx andapredictedclass irrelevant testing animal photo classifier new input cx c define reference point xa sc measured respect predefined upper closest neighbour x share class bound inthesamewaythetheoreticallyinfinitepathcoverage distance x xa follows definition bounded parameter [ ] second sc doe render itselftoacombinatorialsetcoverproblem whichthetestsuite xa argmin cid n x n xi cid minimisation often formulated [ ] xi cx single input yield single sa value belong dista cid n x n xa cid tomultiplesabuckets thesenseofredundancywithrespect sc coverage criterion weaker structural subsequently fromxa wefindtheclosestneighbourofxa coverage single input cover multiple target inaclassotherthancx xb andthedistancedistb asfollows aim show sa guide better selection input rigorous study optimisation test suite dl xb argmin cid n xa n xi cid systemsremainsafuturework however asweshowwithour xi c\\\\ cx empirical study sc still guide test input selection distb cid n xa n xb cid iii researchquestions intuitively dsa aim compare distance new input x known belonging class empirical evaluation designed answer follow cx known distance class cx ing research question class c\\\\ cx former relatively larger rq surprise sadl capable capturing relative latter x would surprising input class cx surprise input dl system classifying dl system multiple way formalise select simple one calculate dsa provide answer rq different angle first ratio dista distb investigation wecomputethesaofeachtestinputincludedintheoriginal complicated formulation left future work dataset see dl classifier find input higher surprise difficult correctly classify expect dsa x dista surprising input harder correctly classify second distb evaluate whether possible detect adversarial example surprise coverage based sa value expect adversarial example surprising well cause different behaviour givenaset ofinputs wecanalsomeasuretherangeofsa dl system using different technique multiple set value set cover called surprise coverage sc since adversarial example generated compared lsa dsa defined continuous space sa value finally train adversarial example classifier use bucketing discretise space surprise define using logistic regression sa value adversarial bothlikelihoodbasedsurprisecoverage lsc anddistance attackstrategy wegenerateadversarialexamplesusing based surprise coverage dsc given upper bound u bucket b b b bn divide u ] n sa original test image provided mnist cifar using original test image adversarial segment sc set input x defined follows example allchosenrandomly wetrainthelogisticregression classifiers finally weevaluatethetrainedclassifiersusingthe sc x bi xx sa x u u ni ] remaining original test image adversarial n examples ifsavaluescorrectlycapture thebehaviourofdl set input high sc diverse set input system expect sa based classifier successfully ranging similar seen training e low detect adversarial example use area curve sa different wa seen training receiver operator characteristic rocauc evaluation e high sa argue input set dl system capture true false positive rate [ ] diversified systematically diversified rq layer sensitivity doe selection layer considering sa recent result also validate notion neuron used sa computation impact showingthatmoredistanttestinputsweremorelikelytolead accurately sa reflects behaviour dl system exception might relevant testing [ ] whileweusethetermcoverandcoverage theimplications bengio et al suggest deeper layer represent higher ofsabasedcoverageisdifferentfromthetraditionalstructural levelfeaturesoftheinput [ ] subsequentworkthatintroduced coverage first unlikemostofthestructuralcoveragecriteria kde based adversarial example detection technique [ ] finite set target cover statement assumes deepest e last hidden layer contain branchcoverage aninputcan atleastintheory bearbitrarily information helpful detection evaluate authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply table list datasets model used study dataset description dnnmodel ofneuron syntheticinputs performance mnist handwritten digit image composed afivelayerconvnetwith fgsm bima bim ac imagesfortrainingandim maxpooling dropout b jsma c w curacy agesfortest layer cifar object recognition dataset ten different layer convnet fgsm bima bim ac class composed image maxpooling dropout b jsma c w curacy trainingandimagesfortest layer udacity selfdrivingcardatasetthatcontains dave [ ] architecture deepxplores test mse selfdrivingcar cameraimagesfromthevehicle composed fromnvidia input generation via challenge ofimagesfortrainingand jointoptimization imagesfortest thegoalofthechallenge chauffeur [ ] architecture deeptests combined mse istopredictsteeringwheelangle withcnnandlstm transformation assumption context sa calculating lsa u upper bound used rq compute sc dsa individual layer subsequently comparing divide range sa [ u ] four overlapping subset adversarialexampleclassifierstrainedonsafromeachlayer first subset including low sa value [ u ] u rq correlation sc correlated existing coverage second including lower half [ ] third including u criterion dl system lower [ ] finally entire range [ u ] thesefoursubsetsareexpectedtorepresentincreasinglymore addition capturing input surprise want sc diversesetsofinputs wesettherangertooneofthesefour consistent existing coverage criterion based counting randomly sample image r train existing aggregation ifnot thereisariskthatscisinfactmeasuring model five additional epoch finally measure something input diversity check model performance accuracy mnist cifar whether sc correlated criterion control mse dave entire adversarial synthetic input diversity cumulatively adding input generated input respectively expect retraining diverse differentmethod e differentadversarialexamplegeneration subset result higher performance technique input synthesis technique execute studied dl system input compare observed iv experimentalsetup change various coverage criterion including sc four evaluate sadl four different dl system using existing one deepxplores neuron coverage nc [ ] original test set b adversarial example generated three neuronlevel coverage nlcs introduced deep five attack strategy c synthetic input generated gauge [ ] kmultisection neuron coverage kmnc neu deepxplore [ ] deeptest [ ] section describes ronboundarycoverage nbc andstrongneuronactivation studied dl system input generation method coverage snac mnist cifar start original test datasets dl system data provided dataset image add tableiliststhesubjectdatasetsandmodelsofdlsystems adversarial example generated fgsm bima bimb mnist [ ] cifar [ ] widely used datasets jsma andc w ateachstep fordave westartfromthe machine learning research collection originaltestdata images andaddsyntheticimages image ten different class mnist adopt generated deepxplore step chauffeur widely studied five layer convolutional neural network stepaddssyntheticimages settoset eachproduced convnet withmaxpoolinganddropoutlayersandtrainitto applying random number deeptest transformation achieve accuracy provided test set similarly adopted model cifar layer convnet rq guidance sa guide retraining dl system maxpooling dropout layer trained achieve improve accuracy adversarial example accuracy provided test set synthetic test input generated deepxplore evaluation sadl dl system safety criti evaluate whether sadl guide additional training cal domain use udacity selfdriving car challenge existing dl system aim improved accuracy dataset [ ] contains collection camera image againstadversarialexamples weaskwhethersacanguidethe driving car aim predict steering wheel selectionofinputforadditionaltraining fromtheadversarial angle model accuracy measured using mean squared examplesandsynthesisedinputsforthesemodels wechoose error mse actual predicted steering angle four set image four different sa range given use pretrained dave model [ ] public artefactprovidedbydeepxplore andapretrainedchauffeur could resume training chauffeur model additional five epoch whichiswhyitisabsentfromrq deepxploreisavailablefrom http github compeikexindeepxplore authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply model [ ] madepubliclyavailablebytheudacityselfdriving neuron computationally infeasible due precision car challenge dave consists nine layer including five loss rq set activation variance threshold convolutional layer achieves mse chauffeur layer activation activation cifar consists convnet lstm submodel reduces number neuron used computa achieves mse tion lsa consequently computational cost computation coverage criterion rq use b adversarial example synthetic input configuration table ii threshold nc set sadl evaluated using adversarial example fornlcs weallsetthenumberofsections k synthetic test input adversarial example crafted ap lsc dsc manually choose layer number plying totheoriginalinput smallperturbationsimperceptible bucket n upper bound ub rq human dl system investigation behaves layer chosen mnist cifar activation incorrectly [ ] werelyonadversarialattackstogeneratein activation respectively weperformrunsofretrainingfor putimagesformnistandcifar thesegeneratedimages subject report statistic likely reveal robustness issue dl system allexperimentswereperformedonmachinesequippedwith test input provided original datasets use intelicpugbram runningubuntu lts five widely studied attack strategy evaluate sadl fast mnist cifar implemented using kera v gradient sign method fgsm [ ] basic iterative method bima bimb [ ] jacobianbased saliency map attack v result jsma [ ] carlini wagner c w [ ] imple duetothespacelimit wecannotincludeallplotsandtables mentation strategy based cleverhans [ ] make available online http coinse github iosadl framework et al [ ] input surprise rq dave chauffeur use stateoftheart synthetic input generation algorithm deepxplore [ ] figure show classification accuracy change deeptest [ ] bothalgorithmsaredesignedtosynthesisenew classify set image growing size test input existing one aim detecting erro test input included mnist cifar dataset neous behaviour autonomous driving vehicle dave set image corresponding red dot ascending use deepxplores input generation via joint optimization sa start image lowest sa increasingly algorithm whose aim generate input lead multiple include image higher sa ascending order sa dlsystemstrainedindependently butusingthesametraining set image corresponding blue dot grow data disagree using dave two opposite direction e image highest sa variant davedropout davenorminit collect syn lower sa reference green dot show mean theticinputsgeneratedbylightingeffect light occlusionby accuracy randomly growing set across repetition asingleblackrectangle singleocc andocclusionbymultiple clear including image higher lsa value e black rectangle multiocc chauffeur synthesise surprising image lead lower accuracy visual new input iteratively applying random transformation confirmation another dataset also chose set input provided deeptest original input image translation synthesised chauffeur deeptest three distinct scale shear rotation contrast brightness blur levelsoflsavalues figureshowsthatthehigherthelsa value harder recognise image visually table ii configuration rq quantitatively visually observed trend support claim sadl capture input surprise even unseen dnn nc nlcs lsc dsc model th k layer n ub n ub input sa measure surprising given input mnist activation whichisdirectlyrelatedtotheperformanceofthedlsystem cifar activation figureshowsplotsofsorteddsavaluesofadver dave block conv na chauffeur convolutiond na sarial example generated five technique wellastheoriginaltestinputs figurecontainssimilarplots based lsa value randomly selected adversarial c configuration example original test set different layer research question default activation variance mnist cifar mnist cifar threshold lsa set bandwidth test input provided datasets represented blue kde set using scott rule [ ] remaining colour tend least surprising whereas majority section detail rq specific configuration rq use adversarial example clearly separated test theactivation layerformnist andactivation forcifar inputsbytheirhighersavalues thissupportsourclaimthat computing lsa value computation lsa based sadl capture difference dl system behaviour adversarial example atthetimeofourexperiments thepubliclyavailableversionofdeeptest finally table iii show rocauc result dsa didnotinternallysupportrealisticimagetransformationssuchasfogandrain effect based classification using neuron mnist cifar authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply lowlsa selectedtestinputsbasedonlsainmnist b mediumlsa c highlsa b selectedtestinputsbasedondsainmnist fig synthetic image chauffeur model generated deeptest image higher lsa value tend harder recognise interpret visually c selectedtestinputsbasedonlsaincifar selectedtestinputsbasedondsaincifar fig accuracy test input mnist cifar dataset selected input lowest sa increas fig sorteddsavaluesofadversarialexamplesformnist ingly including input higher sa vice versa e cifar fromtheinputwiththehighestsatoinputswithlowersa sadl capture relative surprise input input theresultsshowthatthegapindsavaluesobservedin higher sa harder correctly classify adversarial figurecanbeusedtoclassifyadversarialexampleswithhigh example show higher sa value classified based accuracy relatively simpler mnist model dsa sa accordingly based classifier detect adversarial example roc auc ranging dsabased clas b impact layer selection rq sification complicated cifar model show table iv show rocauc classification adver lowerrocaucvalues butanswerstorqsuggestthatdsa sarial example resulting row corresponding fromspecificlayerscanproducesignificantlyhigheraccuracy classifier trained lsa dsa specific layer see section vb mnist respectively row ordered depth e basedonthreedifferentanalyses theanswertorqisthat activation isthedeepestandthelasthiddenlayerinmnist highest rocauc value attack strategy lsabased classification possible subset neuron due typeset bold mnist clear evidence thecomputationalcostofkde henceweintroducetheresultsoflsabased classificationwhenansweringtheimpactoflayerselectionforrq deepest layer effective authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply fig sorted lsa randomly selected adversarial example mnist cifar different layer table iii rocauc dsabased classification adver themostaccurateclassifierforbima moreimportantly per sarial example mnist cifar layer dsa value produce much accurate classification dataset fgsm bima bimb jsma c w result neuron dsa value seen comparison table iii table iv v identical mnist cifar model used produce result table tablev rocaucresultsofsaperlayersoncifar table iv rocauc result sa per layer mnist sa layer fgsm bima bimb jsma c w sa layer fgsm bima bimb jsma c w activation activation activation lsa activation pool pool activation activation activation activation lsa pool activation activation dsa pool activation activation pool activation activation activation case rocauc explained activation figure lsa value activation mnist pool activation example show clear separation original test activation input fgsm bima bimb choosing appro dsa pool activation priatethreshold itispossibletocompletelyseparatetestinputs activation adversarial example similarly plot lsa pool activation mnist show c w lsa line crossing activation activation original test data e c w adversarial example le surprising original test data based result answer rq dsa result low rocauc value sensitive selection layer computed table v contains rocauc value lsa dsa benefit choosing deeper layer however basedclassifiers trainedoneachlayerofthecifarmodel lsa clear evidence supporting deeper attack strategy highest rocauc value layer assumption layer sensitivity varies across different typeset bold interestingly lsa dsa show different adversarial example generation strategy trendswithcifar withlsa thereisnostrongevidence c correlation sc criterion rq deepest layer produce accurate classifier however dsa deepest layer produce table vi show different coverage criterion respond accurateclassifiersforthreeoutoffiveattackstrategies bim increasing diversity levels column represent step b jsma andc w whiletheseconddeepestlayerproduces seehttps coinse github iosadlforplots authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply input added original test set highest accuracy mnist cifar lowest increaseincoverageatastepislessthan percentagepoint mse dave best performance typeset bold compared previous step value underlined thefullrange producesthebestretrainingperformance threshold percentage point based finest configuration followed configuration step change possible lsc dsc well kmnc configuration configuration note threeusebucketingwithk weacknowledgethatthe configuration cifar bimb range threshold arbitrary provide supporting aid produce best retraining performance notethatdsccannotbecomputedforthesetwodlsystems largest improvement observed retraining mnist classifier see section iic againstfgsmusingdsa theaccuracyofthe rangeshows overall studied criterion increase additional increasefromthatof e input added step notable exception nc retraining mnist bimb using dsa show whichplateausagainstmanysteps thisisinlinewithresults evengreaterimprovement wesuspect existing work [ ] exists interplay outlier accuracy range type added input different criterion respond significantly smaller compared configuration snac kmnc nbc show significant increase observation limited dl system addition bimb example cifar change little inputgenerationtechniquesstudiedhere weanswerrqthat whenc winputsareadded however onlysnacandnbc sa provide guidance effective retraining exhibit similar increase addition input set adversarial example based interpretation chauffeur whilekmncincreasesmoresteadily overall observed trend exception nc answer rq sc correlated dnn sa r fgsm bima bimb jsma c w coverage criterion introduced far model dnn criterion test step step step step step lsc fgs bim bim b jsm c w mnist lsa dsc nc mnist kmnc dsa nbc snac lsc dsc cifar nknsncmbacncc cifar lsa dnn criterion test singleocc multiocc light dsa lsc nc mnistandcifar dave kmnc nbc snac dnn sa r singleocc multiocc light dnn criterion test set set set model lsc nc chauffeur kmnc dave nbc lsa snac table vi change various coverage criterion b dave increasing input diversity put additional input table vii retraining guided sa sample input original test input observe change coverage value u u four increasingly wider range sa [ ] [ ] u [ ] [ u ] andretrainforfiveadditionalepochsusing retraining guidance rq sample training data measure accuracy table vii show impact sabased guidance mse entire adversarial synthetic input retraining mnist cifar dave model samplingfromwiderrangesimprovestheretrainingaccuracy column r represents increasingly wider range sa input additional training sampled row r show performance vi threatstovalidity dl system retraining overall retraining primary threat internal validity study configuration sa type dl system adversarial correctness implementation studied dl system attackstrategies andsatypedlsystemthreeinput well computation sa value used publicly synthesismethods eachofwhichisevaluatedagainstfoursa available architecture pretrained model subject rangeswithrepetitions columnsandcontainthemean avoid incorrect implementation sa computation depends standard deviation observed performance metric e widely used computation library scipy ha authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply stood public scrutiny threat external validity mostly dl system et al proposed deepct view concern number model input generation range neuron activation value parameter choice technique study possible sadl le appliescombinatorialinteractiontesting cit tomeasurein effectiveagainstotherdlsystems whilewebelievethecore teractioncoverage [ ] scisdifferentfromdeepctassadl principleofmeasuringinputsurpriseisuniversallyapplicable aimsto quantify amountofsurprise ratherthan simply experimentation reduce particular risk detectsurpriseviaincreaseincoverage deepmutationapplies finally threat construct validity asks whether principle mutation testing dl system mutating measuring correct factor draw conclusion training data test data well dl system based studied dl system activation trace immediate artefact source model level mutation operator [ ] oftheirexecutionsandthemeaningofoutputaccuracyiswell viii conclusion established minimising risk threat propose sadl surprise adequacy framework dl vii relatedwork system quantitatively measure relative surprise adversarial example pose significant threat perfor input respect training data call mance dl system [ ] existing work surpriseadequacy sa usingsa wealsodevelopsurprise machine learning community detection input coverage sc measure coverage discretised feinman et al [ ] first introduced kde mean input surprise range rather count neuron similarity measurement aim detecting adversarial specific activation trait empirical evaluation show example sadl improves upon existing work sa sc capture surprise input accurately number different way first generalise concept aregoodindicatorsofhowdlsystemswillreacttounknown ofsurpriseadequacy sa andintroducedistancebasedsa input sa correlated difficult dl system find second ourevaluationisinthecontextofdlsystemtesting input used accurately classify adversarial third ourevaluationofsadlincludesmorecomplicatedand examples sccanbeusedtoguideselectionofinputsformore practical dl system well testing technique effectiveretrainingofdlsystemsforadversarialexamplesas deepxplore deeptest finally show choice well input synthesised deepxplore neuron ha limited impact lsa acknowledgement arangeoftechniqueshasbeenrecentlyproposedtotestand work wa supported engineering research verify dl system existing technique largely based center program national research foundation two assumption first assumption variation korea funded korean government msit nrf metamorphictesting [ ] [ ] [ ] supposeadlsystemn raa institute information commu producesanoutputowhengiveniastheinput e n expect n cid cid cid cid huang et al [ ] nications technology promotion grant funded ko rean government msit next proposed verification technique automatically gen generation information computing development program erate counterexamples violate assumption pei et national research foundation korea funded al introduced deepxplore [ ] whitebox technique korean government msit mca generates test input cause disagreement among set dl system e nm cid nn independently trained robert feldt acknowledges project tocsyc swedish knowledge foundation kks num baseit dl system nm nn tian et al presented deeptest swedishsciencecouncil vr num forfunding whose metamorphic relation include simple geometric part work paper perturbation well realistic weather effect [ ] secondassumptionisthatthemorediverseasetofinputis reference moreeffectiveitwillbefortestingandvalidatingdlsystems [ ] autonomous driving model chauffeur http github comudacity pei et al proposed neuron coverage nc measure selfdrivingcartreemastersteeringmodelscommunitymodels ratio neuron whose activation value chauffeur predefined threshold [ ] ha shown adding test [ ] udacity open source selfdriving car project http github com udacityselfdrivingcar input violate first assumption increase diversity [ ] google accident google selfdriving car caused crash measured nc similarly deepgauge introduced first time http www theverge com set multigranularity coverage criterion thought googleselfdrivingcarcrashreport [ ] paul ammann jeff offutt introduction software testing reflect behaviour dl system finer granularity [ ] cambridgeuniversitypress criterion capture input diversity [ ] yoshuabengio gre goiremesnil yanndauphin andsalahrifai better essentially count neuron unlike sa therefore mixingviadeeprepresentations corr abs [ ] mariusz bojarski davide del testa daniel dworakowski bernhard bedirectlylinkedtobehavioursofdlsystems weshowthat firner beatflepp prasoongoyal lawrencedjackel mathewmon sa closely related behaviour training accurate fort ursmuller jiakaizhang etal endtoendlearningforselfdriving adversarial example classifier based sa car arxivpreprintarxiv [ ] nicholas carlini david wagner adversarial example apart coverage criterion concept traditional easily detected proceeding th acm workshop artificial softwaretestinghavebeenreformulatedandappliedtotesting intelligenceandsecurityaisec authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply [ ] nicholas carlini david wagner adversarial example [ ] lei fuyuan zhang minhui xue bo li yang liu jianjun zhao easily detected bypassing ten detection method proceeding yadong wang combinatorial testing deep learning system th acm workshop artificial intelligence security page arxivpreprintarxiv acm [ ] xingjun bo li yisen wang sarah erfani sudanthi wi [ ] nicholascarlinianddavida wagner towardsevaluatingtherobust jewickrema michael e houle grant schoenebeck dawn song nessofneuralnetworks corr abs jamesbailey characterizingadversarialsubspacesusinglocalintrinsic [ ] chenyichen ariseff alainkornhauser andjianxiongxiao deepdriv dimensionality arxivpreprintarxiv ing learningaffordancefordirectperceptioninautonomousdriving [ ] christian murphy kuang shen gail kaiser automatic system proceedingsoftheieeeinternationalconferenceoncomputervision testing program without test oracle proceeding th pages internationalsymposiumonsoftwaretestingandanalysis issta [ ] chen f c kuo h tse zhi quan zhou metamorphic pages acmpress testing beyond proceeding international workshop [ ] nicolas papernot fartash faghri nicholas carlini ian goodfellow softwaretechnologyandengineeringpractice step pages reuben feinman alexey kurakin cihang xie yash sharma tom september brown aurkoroy alexandermatyasko vahidbehzadan karenham [ ] zhihuacui feixue xingjuancai yangcao gaigewang andjinjun bardzumyan zhishuai zhang yilin juang zhi li ryan sheatsley chen detection malicious code variant based deep learning abhibhav garg jonathan uesato willi gierke yinpeng dong david ieeetransactionsonindustrialinformatics berthelot paul hendricks jonas rauber rujun long technical [ ] clementfarabet camillecouprie laurentnajman andyannlecun report cleverhans v adversarial example library arxiv learninghierarchicalfeaturesforscenelabeling ieeetransactionson preprintarxiv patternanalysisandmachineintelligence [ ] nicolaspapernot patrickd mcdaniel someshjha mattfredrikson z berkay celik ananthram swami limitation deep [ ] reuben feinman ryan r curtin saurabh shintre andrew b learninginadversarialsettings corr abs gardner detecting adversarial sample artifact arxiv preprint [ ] kexin pei yinzhi cao junfeng yang suman jana deepxplore arxiv automatedwhiteboxtestingofdeeplearningsystems inproceedingsof [ ] robert feldt simon poulding david clark shin yoo test set thethsymposiumonoperatingsystemsprinciples sosp page diameter quantifyingthediversityofsetsoftestcases inproceedings newyork ny usa acm oftheieeeinternationalconferenceonsoftwaretesting verification [ ] simonpouldingandrobertfeldt generatingcontrollablyinvalidand andvalidation icst pages atypicalinputsforrobustnesstesting insoftwaretesting verification [ ] ian goodfellow jonathon shlens christian szegedy explaining validation workshop icstw ieee international confer harnessing adversarial example international conference enceon pages ieee learningrepresentations [ ] david w scott multivariate density estimation theory practice [ ] geoffrey hinton li deng dong yu george e dahl abdelrahman visualization johnwiley sons mohamed navdeepjaitly andrewsenior vincentvanhoucke patrick [ ] ilya sutskever oriol vinyals quoc v le sequence sequence nguyen tara n sainath et al deep neural network acoustic learning neural network advance neural information modeling speech recognition shared view four research processingsystems pages group ieeesignalprocessingmagazine [ ] christianszegedy weiliu yangqingjia pierresermanet scottreed [ ] sepp hochreiter ju rgen schmidhuber long shortterm memory dragomiranguelov dumitruerhan vincentvanhoucke andandrew neuralcomputation rabinovich going deeper convolution proceeding [ ] xiaoweihuang martakwiatkowska senwang andminwu safety ieeeconferenceoncomputervisionandpatternrecognition pages verification deep neural network rupak majumdar viktor kuncak editor computeraidedverification pages cham [ ] l tarassenko biosigntm multiparameter monitoring early springerinternationalpublishing warning patient deterioration iet conference proceeding page [ ] se bastienjean kyunghyuncho rolandmemisevic andyoshuaben january gio onusingverylargetargetvocabularyforneuralmachinetranslation [ ] yuchi tian kexin pei suman jana baishakhi ray deeptest proceeding rd annual meeting association automatedtestingofdeepneuralnetworkdrivenautonomouscars computational linguistics th international joint conference proceedingsofthethinternationalconferenceonsoftwareengineer onnaturallanguageprocessing volume longpapers volume ing pages acm pages [ ] matt p wand chris jones kernel smoothing chapman [ ] alex krizhevsky vinod nair geoffrey hinton cifar hallcrc dataset online http www cs toronto edukrizcifar html [ ] shin yoo metamorphic testing stochastic optimisation pro [ ] alex krizhevsky ilya sutskever geoffrey e hinton imagenet ceedingsoftherdinternationalworkshoponsearchbasedsoftware classification deep convolutional neural network advance testing sbst pages inneuralinformationprocessingsystems pages [ ] shinyooandmarkharman regressiontestingminimisation selection [ ] alexey kurakin ian j goodfellow samy bengio adversarial andprioritisation asurvey softwaretesting verification andrelia examplesinthephysicalworld corr abs bility march [ ] yann lecun yoshua bengio geoffrey hinton deep learning [ ] hongzhu patricka v hall andjohnh r may softwareunittest nature coverageandadequacy acmcomput surv december [ ] yann lecun corinna cortes cj burges mnist handwritten digit database lab [ online ] available http yann lecun comexdbmnist [ ] stijn luca peter karsmakers kris cuppens tom croonenborghs anoukvandevel bertenceulemans lievenlagae sabinevanhuffel andbartvanrumste detectingrareeventsusingextremevaluestatistics applied epileptic convulsion child artificial intelligence medicine [ ] leima felixjuefeixu jiyuansun chunyangchen tingsu fuyuan zhang minhui xue bo li li li yang liu jianjun zhao yadongwang deepgauge comprehensiveandmultigranularitytesting criterion gauging robustness deep learning system corr abs [ ] leima fuyuanzhang jiyuansun minhuixue boli felixjuefeixu chaoxie lili yangliu jianjunzhao etal deepmutation mutation testing deep learning system arxiv preprint arxiv authorized licensed use limited universidad nacional autonoma de mexico unam downloaded march utc ieee xplore restriction apply', 'comparative analysis government plan peruvian presidential candidate sdo un state policy national agreement based nlp honorio apaza alanoca josimar chire jimy oblitas data science research group national university moquegua ilo moquegua peru institute mathematics computer science icmc r university sa paulo usp sa carlos sp brazil p facultad de ingenier universidad privada del norte cajamarca peru hapazaa unam edu pe jecs usp br jimy oblitas upn edu pe ] abstract analysis government proposal election c political party vital choose next authority city country paper use text mining approach analyze c documentsandprovideaneasyvisualizationtosupportaneasyanalysis [ besides comparison national plan based sustainable devel opmentobjectivesofun unitednations fromagendaisperfomed v using natural language technique keywords natural language processing text mining data science system recommender election politics peru south america introduction election authority important event citizen choose peoplewhowillrepresentthemandpurposeprojectstoimprovethenational v gional context traditionally political party promote candidate x mass medium e radio television social network candidate travel visit city gain elector r peru participate president election requirement send gov ernment proposal plan jurado nacional de elecciones national election jury document summarizes proposal candidate considering themostimportantproblemsforthepartyandsolutionsthattheypurpose usu ally document dozen page read citizen choose next authority besides united nation un purposed agenda summarize important issue need special attention government related poverty communication discrimination united nation un adopted new international develop ment agenda agenda includes sustainable development honorio apaza alanoca josimar chire jimy oblitas goal target agenda specifies need action strengthen sustainable economic growth decent employment industrialization country [ caribbean ] theagendaconsidersacomplexcombinationoffairlydetailedthematic target comprehensive approach requires addressing sustainable development necessary integration social economic environmen tal ax [ nieto ] although recognized country ha priority agenda reference government plan seeking adequate sustainabledevelopmentofperu therefore measuringthealignmentorpossible evolution government plan presidential candidate necessary task context use software tool text mining emerges quick interesting proposal measure trend addition fact peruvian context tool used yet contrast global trend use software tool already established cam paignsoftrumpandbolsonaro intheunitedstates usa andbrazil whichil lustratepolicyfactsthathavebeenfavoredbyicts [ garcianunes et al ] natural language processing ha shown potential promising tool ex ploit urban data source author [ cai ] suggest use urban big data source still starting studied area urban governance management public health land use functional zone mo bility urban design useful expanding study scale reducing research cost text mining area us wellknow data mining approach data col lection exploration analysis visualization text mining focus text analysis us natural language technique nlp many study per formed analyze different problem different area e epidemiology [ chire saire oblitas cruz ] politics [ sharma shekhar ] mar keting etc applicationsoftextmininginpoliticsandelections e anticipatingpolit ical behaviour [ sangar et al ] study voting pattern [ bagui et al ] fraudidentification [ poloni formolo ] sentimentalanalysisofcitizens [ sharma ghose ] electionresultprediction [ ramteke et al ] objective paper analyze government proposal peruvian candidate president election using text mining approach support easyunderstandingofthedocuments besides performamatchingprocesswith national plan adapted agenda check important objective political party sectioniincludesthereviewofthebibliography sectioniidevelopsthework proposal section iii discloses result research section iv give conclusion last section present future work analysis government plan peruvian presidential candidate proposal natural language processing process transformation text information numeric data [ di giuda et al ] work based following research process data collection data analysis reporting select retrieve data comparative analysis government plan report research result algorithm jaro candidate finding winkler presidency peru fig research process process planed used [ kim et al ] data collection present work government plan candidate presidency republic peru collected also sustainable development goal policy state national agreement sustainable devel opment goal sdgs promoted united nation whose predecessor millennium development goal constitute inclusive global agenda goal [ secretaria ejecutivo del acuerdo nacional ] data analysis jarowinkleristhemainalgorithmtoperformcomparativetextanalysisofdoc uments governmentplansofthecandidates withthesustainabledevelopment goal sdgs promoted united nation cid sim j mt objective calculate distance string text written plan government candidate objective policy sustainable development state national agreement first preliminary test research interested knowing result obtained jaro winkler reporting finally thelaststageoftheresearchistomakeareportontheresultsobtained case result jaro winkler distance plan candidatesgovernmentandtheobjectivesandsustainabledevelopmentpolicies state national agreement honorio apaza alanoca josimar chire jimy oblitas result section show result frequency term word cloud seen candidate highlight particular topic system health program etc result due fact currently nation world suffering global pandemic therefore plan candi date government propose proposal solve problem related health also show important issue education economics etc beenneglected especiallyissuesrelatedtosustainabledevelopmentgoals sdg promoted united nation accion popular partido morado avanza pais alianza para el progreso apra democracia directa frente amplio frente esperanza fuerza popular junto por el peru partido nacionalista peru libre patria segura podemos peru ppc renovavion popular somos peru union por el peru victoria nacional fig cloud word plan government candidate among candidate plan one stand gov ernment plan political party avanza pais economic issue also seen accion popular political party ha uniform distribution government plan issue economy health education politics seen figure inthiscasewecanvarytheissueswewanttomeasure thiscanbeaccording tothecontextofthemomentanddifferentsectorsofsociety theyhavedifferent problem need important analyze point view social class thought present graphical figure representation similar government plan candidate presidency peru figure seen identical see degree similarity due fact government plan clearly address similar issue translate social problem health economy program etc government judiciary corruption congress etc experiment difference prolific class also denoted case distance noticeable political party considered analysis government plan peruvian presidential candidate gobierno politica educacion salud economia religion accion popular partido morado avanza pais alianza para el progreso apra democracia directa frente amplio frente esperanza fuerza popular junto por el peru partido nacionalista peru libre patria segura podemos peru ppc renovavion popular somos peru union por el peru victoria nacional fig important area document beontheleftwiththoseontheright whichcanbesimilarinthedailyexercise obviously different thought therefore different proposal two side peruvian politics fig document similarity honorio apaza alanoca josimar chire jimy oblitas section going analyze distance chain text writteninthegovernmentplansofthecandidatesandtheobjectivesandpolicies sustainable development state national agreement try differentiatethesimilaritiesbetweenthesetwodocuments whenachainoftexts issimilartoanothermeansthatthedocumentcontainstextssimilartotheother could say government plan address one many sustainable development goal policy state national agreement fin de la pobreza hambre cero salud bienestar educacion de calidad igualdad de genero agua limpia saneamiento energia asequible contaminante trabajo decente crecimiento economico industria innovacion e infreestructura reduccion de la desigualdades ciudades comunidades sostenibles produccion consumo responsables accion por el clima vida submarina vida de ecosistemas terrestres paz justicia e instituciones solidas alianzas para lograr los objetivos accion popular partido morado avanza pais alianza para el progreso apra democracia directa frente amplio frente esperanza fuerza popular junto por el peru partido nacionalista peru libre patria segura podemos peru ppc renovavion popular somos peru union por el peru victoria nacional fig document similarity plan graph seen government plan political partyavanzapaisaddressesmuchmorethanothersthegoalofpeace justiceand solid institution paz justicia e instituciones solidas followed political party renovacion ppular however little addressed objective underwater life vida submarina health wellbeing salud bienestar end poverty fin de la probreza etc conclusion algorithm jaro winkler based measuring distance text chain show u interesting preliminary result show u difference government plan candidate presidency peru well objective sustainable development goal state policy national agreement however result refined advanced artificial intelligence method algorithm analysis government plan peruvian presidential candidate present want highlight way difference government plan document graphically demonstrated way showing document difference important electorate withouthavingtoreadallthegovernmentplans theycanobtainamoregeneral vision graphically future work oneofthefuturejobsistoexperimentwithhighlyadvancedartificialintelligence technique discipline natural language processing text mining would interesting study experience coherent argu ments candidate debate government plan must coherence idea proposal written government plan candidate express debate interview press etc reference bagui et al bagui mink cash p data mining technique study voting pattern u data science journal cai cai naturallanguageprocessingforurbanresearch asystem atic review heliyon e caribbean caribbean e c f l agenda sustainable development last modified publisher cepal chire saire oblitas cruz chiresaire j andoblitascruz j study ofcoronavirusimpactonparisianpopulationfromapriltojuneusingtwitterand text mining approach page di giuda et al di giuda g locatelli schievano pellegrini l pattini g giana p e seghezzi e natural language processing informationandprojectmanagement pages springerinternationalpublish ing cham garcianunes et al garcianunes p rodrigues p oliveira k g da silva e computational tool weak signal classification detectingthreatsandopportunitiesonpoliticsinthecasesoftheunitedstatesand brazilian presidential election future kim et al kim k joungpark yun andyun h whatmakes touristsfeelnegativelyabouttourismdestinations applicationofhybridtextmining methodologytosmartdestinationmanagement technologicalforecastingandsocial change nieto nieto crecimiento econo mico e industrial izacio n en la agenda perspectivas para xico problemas del desarrollo poloni formolo poloni andformolo dataminingtoiden tify fraud suspected electronic election ninth international conference complex intelligent software intensive system page ramteke et al ramteke j shah godhia andshaikh elec tion result prediction using twitter sentiment analysis international con ference inventive computation technology icict volume page honorio apaza alanoca josimar chire jimy oblitas sangar et al sangar b khaze r ebrahimi l participa tion anticipating election using data mining method secretaria ejecutivo del acuerdo nacional secretaria ejecutivo del acuerdo na cional objetivos de desarrollo dostenible politicas del estado del acuerdo nacional sharma ghose sharma ghose u sentimental analysis twitter data respect general election india procedia computer science international conference smart sustainable intelligent computing application icitetm sharma shekhar sharma shekhar h intelligent learning basedopinionminingmodelforgovernmentaldecisionmaking procediacomputer science', 'j inst eng indiaser b http doi org original contribution text similarity measure news article vector space model using nlp ritika singh satwinder singh receivedjuneacceptedoctober cid theinstitutionofengineers india abstract present global size online news website keywords bilingual news article similarity cid million according marketingprofs cosine similarity cid jaccard similarity cid euclidean distance morethanmillionarticlesarepublishedeverydayonthe web online news website also circulated edi torialcontent overthe internetthat specifies article introduction display website home page article highlight e g broad text size main news article huge increase number online newspaper pub many article posted news website lishing digital technology innova similar many news website selective tions modern world much information reporting top news headline also similarity appears tremendous speed reader need find among news across various news association well reading true news false news false news identifiedbutnotverywellcalculated thispaperidentifies information endanger confuse person top news item news site measure life butalsoanentiresociety soitisveryimportanttofind similarity two news item two language source information compare hindi english referring event news study ha interest extracting online accomplish highlighted headline link extractor news platform specifically measure similarity ha created extract top news hindi news article across various site article provides englishfromgooglesnewsfeed first translatethehindi detailsaboutwhatnewsisbeingconsidered howitisbeing news article english using google translator presented highlighted website [ ] news compare english news article second article published website usually appear used cosine similarity jaccard similarity euclidean similar rectified form several different website distance measure calculate news similarity score similar almost identical news confusing user frequency noun next word noun similarity slows process discovering new news article also extracted methodology clearly information topic potentially lead missing showsthatwecanefficientlyidentifytopnewsarticlesand information iftheusermistakenlyrecognizestwonewsas measure similarity news report similarwheninfactonecontainsnewdata itismuchmore difficult locate similar news item website large amount miscellaneous content material article although main news article ritikasingh text similar two different web page extra ritikasingh outlook com neous material page may satwindersingh fore traditional approach equivalent news satwinder singh cup edu determination would fail [ ] first paper developed departmentofcomputerscienceandtechnology central method scraping top news headline text web universityofpunjabbathinda bathinda india page e googlenewsfeedwebsiteswhicharepresentin j inst eng indiaser b two different language hindi english referring first created headline link extractor sameevent use extracted textto classify news par selected news website searched ten us pair content avoiding irrelevant based news site home page month use informationonthearticles bymeasuringasimilarityscore parser extract k news site max news pair based method called cosine similarity imum number article second author us cal jaccard similarity euclidean similarity culation cosine similarity quantify similarity research distinguish similar news article well news also provide technique work differentones thepurposeofthispaperisalsotodiscover assistinanalyzingarchivednewswebpagesbyintroducing bilingual news article comparable corpus [ ] tool parsing select html news site hero particular study dealing representation headlinestoriesusingcssselectors authorsstudiesover news measurement similarity among new month shown overall similarity decreased article experiment us similarly named entity asthenumberofarticlesincreased studiesfromtheauthor include representative feature news indicate would set synchronous story toassessthesimilaritybetweenarticlesofthesamenews given day besides relevant national event approach research proposing new method focused canbeusedtofurtherexaminetheoccasionalelectionsthat knowledge base framework aim provide human held information value category named entity katarzyna baraniak marcin sydow work tool within news [ ] comparable corpus news would support detection analysis hindi english compared approach tradi information bias [ ] author us method auto tional one obtains better result similarity also matically identify article reporting sub distance measure calculate similarity two docu ject event entity use comparative mentsorsentencesintoasinglenumericalvalueandbrings analysisortoconstructatestortrainingcollection within degree semantic similarity [ ] distance paper author explains representation doc oneanother severalsimilaritymeasureshavebeenusedby ument text method similarity measure text researcher much work ha done clustering include test cosine similarity similarity newspaper study aim compare euclideandistance jaccardcoefficient pearsoncoefficient semanticsimilaritybetweentwoarticlesofthesamenews correlation averaged kullbackleibler diver present two different language hindi english gence author also applies machine learning optimize human understanding basic concept approach recognize similar article develop measuring news similarity identify feature article machine learning model detects similar article auto vector thereafter measure difference matically identifyingfragments oftext concerningsimilar feature low distance feature eventsandidentifyingbiasinthemisexpected theauthor implies high level similarity value large dis also working expand research study lan tance feature implies low level guages e g polish english similarity value [ ] euclidean distance cosine distance maake benard magara et al suggest system use jaccardcoefficientmetricsaresomeofthedistancemetrics artificial intelligent research paper written artifi used document similarity computation study cial intelligence expert [ ] work us recursive explores twoseparatemethods ofgeneratingfeatures partitioning random forest improved machine text tfidf vector bag word also learning algorithm average accuracy implement two method calculating textual similarity timing efficiency second news article cosine similarity jaccard algorithm typically performed quite well compared similarity tfidf vector euclidean distance boosted even random forest algorithm using bag word sophisticated model used future study much like latent semanticanalysis lsa sincedocuments canbeidentifiedasbelongingtothesameclassevenifthey literature rereview similar word phrase vikas thada dr vivek jaglan author used cosine similarity dice literature similarity measure used coefficient jaccard similarity algorithm [ ] work various purpose section proposal completedonthefirstpagesofthegooglesearchresult reviewed expanded page reliable effi atkins et al [ ] describe technique ass top ciency estimate future study cosine similarity news headline story selected set usbased news eventually concluded wa best fitness compared website calculate correlation across others dataset summary initial j inst eng indiaser b finding promising still long way go achieve greatest crawling efficiency possible sys tematicmethodproposedbynasabetal [ ] thefollowing point determine similarity article text divided three section heading abstract keywords abstract keywords based linkto title article weighing weighted mean esti mated based description abstract keyword use pearsons correlation method find similarity person machine score accuracy proposed technique use specialized fig aframeworkforcomparativeanalysis wordnetitcanalsoconcentrateonarticlesimilarities proposedframeworkcanbeusedforothertextsthatrequire wordnet language text persian language snover et al explore new way usingmonolingualtargetdatatoenhancetheefficiencyofa statisticalorpredictivemachinetranslationfornewsstories jaccard similarity measure final step frame [ ] thismethodemployscomparabletextvarioustextsin work compare analyze produced result target language explore equivalent explain step detail story mentioned source language document dataset used paper known google large monolingual datasetfor source documenttobe news publicly available [ ] google news translated target language searched google offering special experience google news document may similar source document combine news item one provides experimental result paper generated constant personalized flow newspaper thousand thedifferenceofthelanguageandtranslationmodelsshow ofpublishersandmagazinesgroupedaround googlenews vital improvement baseline framework combination global event local news news qian et al [ ] using comparable corpus bilingual story youve reading turn dependency mapping model bilingual lexicon building headline show top news world english chinese model considers additional section allow delve various dependent word relationship measuring topic sport business technology similarity bilingual word thus offer greatest value service delivered news precise le noisy representation author also language using google news experiment extract illustrated bilingual dependency mapping news article hindi english language created optimized automatically without human input contributing mediumsized set dependency map headline link extractor ping impact bilingual lexicon con struction blc fully exploited weight basic python library searching downloading live learningusingasimplebuteffectiveperceptronalgorithm news article google news feed googlenews making approach quickly adaptable several gnewsclient [ ] using one pick top language pair headline running google news website check top headline particular subject keyword experiment use extract link hindi methodology english news related event major step methodology given article scraping figure present framework work textual news data first preprocessed rep newspaperisapythonmoduleusedtoextractnewspaper resented structural format two represen article parse newspaper using special tationmethodsofgeneratingfeaturesfromthetextthatare ized web scrapping algorithm extract valuable investigatedinthisstudyaretfidf andbagofword textfromawebsite thisworksextremelywellonwebsites represented three representation method online newspaper experiment ha extracted represented method compared three similarity link hindi english news also measure shown fig e cosine euclidean extract text using newspaper module j inst eng indiaser b translator idf vector used feature vector measure similarity article newsresults using package google offer language translation package python word taken similarity measure hindi news article translated different language english language either hindi corpus translated similarityfunctionisarealvaluedfunctionthatcalculates intoenglishorenglishcorpuscanbetranslatedintohindi similarity two item calculation sim translated hindi corpus english ilarity achieved mapping distance similarity translation performed level sentence withinthevectorspace thisexperimentprovidestwotests translation also generates map word various lan ofsimilarity cosinesimilarity similaritywithjaccard guages english research used bilingual dic euclidean distance tionaries ranging hindi english cosine similarity cosine angle ndi mensionalspace betweentwondimensionalvectors preprocessing data cleaning isthedotproductofthetwovectors dividedbyproductof two vector length magnitude [ ] simi preprocessingstepssuchastheeliminationofstopwords larity cosine measured using following lemmatization parsing letter punctuation mark formula number completed word lemma b pn cid b tized wordnetlemmatizer nltk library took similaritya bjjajj cid jjbjjpffipffiffiffiffiffinffiffiffiiffiffiffiaffiffiffiffiffipi ffipffiffiffiffiffinffiiffiffiffiffiffibffiffiffiffiffi english stopwords [ ] vector space model asshowninfig supposethere istwopointspand p distance within point increase amathematicalmodelisalsocalledthetermvectormodel similarity point decrease vice versa whichdescribestextdocumentsasidentifiervariables cid cosinesimilaritycosinedistance term token course term depends comparison usually word keywords sen theresultoftheanglewillshow theresult ifthe angle tences compared isbetweenthedocumentvectorsthenthecosinefunction document angel feature vector value cosine function le doe angle reach document artificial intelligence feature vector ndimen completely different thus way calculating sional vector computational feature describe cosine angle vector p p decides entity really important method calculating vector pointing direction semanticsimilarityamongtexts methodswereusedduring jaccard similarity jaccard similarity calculates experiment measure function vector tfidf similaritiesamongsets itsdefinedastheintersectionsize term frequencyinverse document frequency sim ple algorithm transforming text meaningful representation number tfidf weight measure factwhichevaluatestheimportanceofaspecificwordina text mathematics cid cid x n tfidf weight tf cid log df id document tf number occurrence theithterm df isthenumber ofdocumentswhichcontain ithterm ni total number documents thesklearn vectorized function wasused toconstructatfidffunction thiswholemodelwasconstructedbyusingthedocuments andagroupofsuchtfidfvectors wasgeneratedconsisting tf idf weight term document tf fig cosinesimilarity j inst eng indiaser b divided bythe unionsize oftwo sets jaccard similitude ac abbc determined using formula [ ] pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ac abbc b b ja b qffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi b jajjbj cid ja bj ac x cid xy cid where\\\\representsintersectionand [ representstheunion sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi inthispaper aandbarebagsofwordsthatcontainnews jx yj xx cid article jaccard table show comparative analysis method jaccard b b based relative pro con table also b dont size describes application area selected always assign number technique used jaccard distance instead similarity measure similarity score dissimilarity found subtracting jaccard similarity coefficient similarityscoremeansthattwodatasetsarehowsimilarto jda b cid jda b one another data collection include two separate ja [ bj cid ja [ bj textsasinthiscase thesimilaritybetweenthetwotextsis jda b ja [ bj evaluated according scoring system euclidean dis tance doe find similarity text euclidean distance another similarity measure findsthemetric thedistancebetweenbothtexts [ ] vector space model euclidean distance l different way calculate similarity distance euclidean norm similarity measure differentiates similarity measurement similaritya b distancea b vector space model judging angle like rest rather direct distance vector noun phrase extraction input asshowninfig iftherearetwopointslike x noun phrase extraction technique text analysis x let u consider dimension point consistingoftheautomatedextractionofnounsinatext one want find distance x help summarize content text identify x basically use particular parameter like key topic discussed paper concludes euclideandistancetocheckthatifthisparticularpointsare extraction frequency noun phrase fre nearertoeachotherthanitwillconsiderthatthistwopoint quencyofthenextwordofthenounfromnewsarticlescan similar euclidean distance calcu considerably improve similarity measure textblob latedbasedonthepythagorastheorem letdrepresentthe python module used extract noun [ ] measureofdistancesbetween x x hence distance c expressed proposed method paper introduces two method calculating similaritybetweentwoarticlesofthesamenews whichare present two different language hindi english based method calculating feature vector similarity measure cosine similarity jaccard similarity tf idf vector thepreprocessednewsarticleswereturnedintovectorsof tfidf using vectorized model tfidf vector obtainedwereasparsematrixcontainingtfidfweightsfor news article word dimension [ number fig euclideandistance j inst eng indiaser b table comparisonoftheprosandconsofdifferentmeasuresandtheirapplicationarea si similarity pro con applicationarea measure cosine bothcontinuousandcategoricalvariablesmaybe doesntworkeffectively textmining documentsimilarity similarity used withnominaldata [ ] jaccard bothcontinuousandcategoricalvariablesmaybe doesntworkeffectively documentclassification coefficient used withnominaldata euclidean easytocomputeandworkwellwithadatasetwith doesnotworkwithimage applicationinvolvingintervaldata dna distance compactorisolatedclusters [ ] dataefficiently analysis kmeanalgorithm news article number feature distinct word ] [ ] bag word euclidean distance tfidf weight matrix wa used featureforeverytext andsimilarityamongnewsarticlesis preprocessed document described calculated using cosine similarity jaccard similarity vector frequency word compare sklearnsbuiltincosineandjaccardsimilaritymodulewas similar comparing bag vector word used measure similarity experiment us bagofwords model computer process vector much faster vast file table samplepairofcompletelysimilarnews table samplepairofdifferentnewsstoriesaboutthesametopic j inst eng indiaser b table samplepairofcompletelydissimilarnews performedonpairsofnewsheadlineobtainedfromgoogle news [ ] thechosennewsarticlesarelistedintables thenewsarticlesweregiventoahumanexpertto judge similarity dissimilarity result human expert ha determined pair pair com pletely similar news pair pair different news topic pair pair completely dissimilar news expert judg ment used benchmark evaluate automatic similarity calculation news article cosine similarity jaccard coefficient euclidean distance applied result three measure shown table toprovideabetterunderstandingofthethreecompared measure theresults areshownonabargraph asdepicted fig figure show similarity measure bar graph different news story topic figure show similarity measure bar graph completely dissimilar news performance measure used experiment accuracy precision recallandfmeasures thesemeasures calculated determining number news article correctlyidentifiedassimilarordissimilarcomparedtothe decisionsbyhumanexperts [ ] inotherwords usingthe human decision benchmark number true pos itive tp equivalent actual similar news cor rectly identified similar true negative tn equivalent actual dissimilar news correctly identified dissimilar falsepositive fp whichisequivalenttoactual similar news incorrectly identified dissimilar false text lot data [ ] paper load news negative fn whichisequivalenttoactualdissimilarnews article list called corpus calculate feature incorrectly identified similar determined vector document finally compute accuracy calculated tp tn data precision euclideandistanceandthentocheckhowsimilartheyare tp tp fp recallistp tp fn andthefmeasures greaterthedistance lesssimilartheyare thispaperusesa harmonic mean precision recall module library called sklearn machine equal tp tp fp fn [ ] result pre learning library sented next section result discussion result analysis figure present graph similarity measurement proposed algorithm implemented using python sample pair news article using euclidean jaccard bit fortheexperiment aroundnewsstories cosine similarity measure representation wererandomlypickedfromthedataset thealgorithmruns scheme e tfidf bag word representation dataset measure compare various learned fig cosine performs similar similarity score every news article similarity ha benchmarkfornewswiththesamemeaning pair calculated every article different news topic pair everforcompletelydissimilarnews pair jaccards comparative analysis euclidean score similar human benchmark toproveourpointfurther wecalculatedthecorrelation analyze performance representation method score similarity measure human different similarity measure experiment wa benchmark shown table j inst eng indiaser b table similaritymeasuresofcompletelysamenews cosine jaccard euclidean cosinesimilarity jaccardsimilarity euclideansimilarity news news news news news fig comparison similarity coefficient different news articlesaboutthesametopic table similarity measure different news story cosine jaccard euclidean sametopic cosinesimilarity jaccardsimilarity euclideansimilarity news news news news news fig comparison similarity coefficient completely dissim ilarnews table similaritymeasuresofcompletelydissimilarnews cosinesimilarity jaccardsimilarity euclideansimilarity human cosine jaccard euclidean e ur ea arit mil si cosine jaccard euclidean news hadlines fig similarityscoregraph table find accuracy precision recallandfmeasuresasexplainedintheprevioussection table give clear picture performance similarity measure analyzing result see news news news news news news theprecisionvalueofjaccardmeasuresis fig comparison similarity coefficient article le euclidean distance however euclidean news give high value recall compared precision cosine measuregives good accuracy level andf score correlation score table per difference recall value precision ceived cosine jaccard similarity cor high among three method cosine similarity related benchmark score analyze usingtfidfshowedgreateraccuracy recallandfmeasure produced result calculating confusion matrix [ ] score respectively j inst eng indiaser b conclusion table correlationofthesimilarityscorestothebenchmark method correlation ongoing research conducted comparison three cosineandbenchmark different method estimate semantic similarity jaccardandbenchmark among two news article nearly topicevent euclideanandbenchmark measure similarity two different language hindi english experiment wa tested using googlenews data set three methodology arethesimilarityofcosinewithtfidfvectors similarityof jaccard tfidf vector bag word euclidean dis table confusionmatrixforcosinesimilarity tance allthreeofthesemethodsshowedpromisingresults among three method cosine similarity using tf news predicted predicted yes idf showed greater accuracy recall fmeasure score total respectively accuracy actual tn fp two method may improved actual yes fn tp docvec model [ ] take text corpus input thresholdvalue generates document vector output experiment totalrefinednews also looking expand work language table confusionmatrixforjaccardsimilarity reference news predicted predicted yes g atkins weigle andm nelson measuringnewssimilarity acrosstenu newssites arxivpreprintarxiv pp total j gibson b wellner lubar identification duplicate actual tn fp news story web page mitre corporation actual yes fn tp burlington rd bedford usa burlington rd thresholdvalue bedfordmausa singh kumar v goyal review technique totalrefinednews extraction bilingual lexicon comparable corpus int j eng technol montalvo r mart nez casilla bilingualnewsclustering using named entity fuzzy similarity springer heidel table confusionmatrixforeuclideansimilarity berg pp mohd saad kamarudin comparative analysis news predicted predicted yes similarity measuresforsentence levelsemanticmeasurement total text ieee international conference control system computingandengineering pp actual tn fp p sitikhu comparison semantic similarity method actual yes fn tp maximum human interpretability arxiv v thresholdvalue [ cs ir ] k baraniak sydow news article similarity auto totalrefinednews maticmediabiasdetectioninpolishnewsportals inproceedings federated conference computer science infor mationsystems b magara zuva comparative analysis text similarity measure algorithm research paper recom mendersystems inconferenceoninformationcommunications table accuracylevelofeachsimilaritymeasures technologyandsociety ictas similaritymeasures performancemeasures v thada v jaglan comparison jaccard dice cosine similarity coefficient find best fitness value web retrieved accuracy precision recall fmeasure document using genetic algorithm int j innov eng technol ijiet cosine nasab anewapproachforfindingsemanticsimilarscien jaccard tific article j adv comput sci technol jacst euclidean snover b dorr andr schwartz languageandtranslation thehighestvalueisshowninbold model adaptation using comparable corpus proceeding j inst eng indiaser b theconferenceonempiricalmethodsinnaturallanguage http dataaspirant comfivemostpopular processing honolulu similaritymeasuresimplementationinpython q longhuaandw hongling bilinguallexiconconstruction goswami babu b purkayastha comparative analysis fromcomparable corpus viadependencymapping inproceed similarity measure int j manag technol eng xi ingsofcoling ydanideahl danideahl googlenewsisgettinganoverhaul ali textualsimilarity issn andcustomizednewsfeeds thevergemay [ online ] textblob simplified text processing [ online ] available available http textblob readthedocs ioendev http www theverge comgooglenews bag word euclidean distance [ online ] available updatenewfeaturesnewsstandio http pythonprogramminglanguage combagofwords h hu googlenews pypi pypi org mar [ online ] euclideandistance available http pypi orgprojectgooglenews kamaruddi graphbased representation sentence simi j brownlee howtocleantextformachinelearningwithpython larity measure comparative analysis int j eng technol machine learning mastery october [ online ] avail able http machinelearningmastery comcleantextmachine publisher note springer nature remains neutral regard learningpython jurisdictionalclaimsinpublishedmapsandinstitutionalaffiliations polamuri five popular similarity measure implemen']\n",
      "Duration: 0:00:00.578111\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "norm_corpus = u.normalize_corpus(corpus)\n",
    "print(norm_corpus)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4da4eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus2 = u.preprocess_corpus(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5cb64410",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_special_characters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-c4f38b7aeac3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mspecial_char_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'([{.(-)!}])'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecial_char_pattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \\\\1 \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_special_characters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_digits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremove_digits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'remove_special_characters' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# insert spaces between special characters to isolate them    \n",
    "special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "doc = special_char_pattern.sub(\" \\\\1 \", text1)\n",
    "doc = remove_special_characters(doc, remove_digits=remove_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2f728024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ieeeacm st international conference software engineering icse ieeeacm st international conference software engineering icse guiding deep learning system testing using surprise adequacy jinhan kim robert feldt shin yoo school computing dept computer science engineering dept software engineering kaist chalmers university blekinge inst technology daejeon republic korea gothenburg sweden karlskrona sweden jinhankimshinyookaistackr robertfeldtchalmersse robertfeldtbthse abstractdeep learning dl systems rapidly anothervehicletoyieldinoneoftherarercircumstancesand adopted safety security critical domains urgently calling crashed vehicle expectation proved ways test correctness robustness testing incorrect urgent need verify validate dl systems traditionally relied manual collection behaviours dl systems however signicant part labelling data recently number coverage criteria based neuron activation values proposed criteria existing software testing technique directly applicable essentiallycountthenumberofneuronswhoseactivationduring dl systems notably traditional whitebox testing execution dl system satised certain properties techniques aim increase structural coverage predened thresholds however existing cover veryusefulfordlsystemsastheirbehaviourisnotexplicitly age criteria sufciently ne grained capture subtle encoded control ow structures behavioursexhibitedbydlsystemsmoreoverevaluationshave focused showing correlation adversarial examples number novel approaches towards testing veri proposed criteria rather evaluating guiding cation dl systems recently proposed use actual testing dl systems propose novel test gap techniques adequacy criterion testing dl systems called surprise share two assumptions rst assumption essentially adequacy deep learning systems sadl based generalisation essence metamorphic testing behaviour dl systems respect training data measure surprise input difference two inputs dl system similar respect dl systems behaviour input training humansensetheoutputsshouldalsobesimilarforexample data ie learnt training subsequently deeptest checks whether autonomous driving system developthisasanadequacycriterionagoodtestinputshouldbe behavesinthesamewaywhentheinputimageistransformed sufcientlybutnotovertlysurprisingcomparedtotrainingdata asifthesamesceneisunderadifferentweatherconditionthe empirical evaluation using range dl systems simple image classiers autonomous driving car platforms shows second assumption also based traditional software systematic sampling inputs based surprise testing results diverse set input improveclassicationaccuracyofdlsystemsagainstadversarial effective testing dl system one perform examples via retraining forexampledeepxplorepresentedtheneuroncoverage index termstest adequacy deep learning systems ratio neurons whose activation values predened threshold measure diversity neuron introduction behaviour subsequently showed inputs violating deeplearningdlsystemshaveachievedsignicant rst assumption also increase neuron coverage progress many domains including image recognition recently introduced techniques made sig speech recognition machine transla nicant advances manual ad hoc testing dl systems tion based capability match even major limitation coverage criteria proposed surpass human performance dl systems increasingly fararenotsufcientlynegrainedinasensethatallofthem adopted part larger systems safety simply count neurons whose activation values satisfy certain securitycriticaldomainssuchasautonomousdriving conditions aggregation counting allow malware detection tester quantify test effectiveness given input adoption dl systems calls new challenges setitconveyslittleinformationaboutindividualinputsfor critically important larger systems exampleitisnotimmediatelyclearwhenaninputwithhigher correctandpredictable despitetheirimpressive experimental nc considered better another lower nc performances dl systems known exhibit unexpected certain inputs may naturally activate neurons behaviours certain circumstances example threshold others vice versa another reported incident autonomous driving vehicle expected example kmultisection neuron coverage 22255 // 1199 //$$ 3311 .. 0000 ©© 22001199 iieeeeee 11003399 ddooii 1100 .. 11110099 // iiccssee .. 22001199 .. 0000110088 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . partitionstherangesofactivationvaluesofneurons , observed ing additional adversarial examples , sampling additional training , k buckets , count number total inputs broader sa values improve accuracy bucketscoveredbyasetofinputs . whenmeasuredforasingle retraining 77 . 5 %. 1 input , thecoveragewillbeeither iftheinputactivateseach • weundertakeallourexperimentsusingpubliclyavailable k neuronwithavaluefromoneofthekbuckets , orsmallerthan dlsystemsrangingfromsmallbenchmarks ( mnistand thatifsomeneuronsactivateoutsidetherangeobservedduring cifar - 10 ) large system autonomous driving training . , theinformationabouthowfarsuchactivations vehicles ( dave - 2 [ 6 ] chauffeur [ 1 ]). implemen - go beyond observed range lost aggregation , making tations available online . 2 hard evaluate relative value input . test remaining paper organised follows . sec - adequacy criterion practically useful , able tion ii introduces surprise adequacy dl systems , sadl : toguidetheselectionofindividualinputs , eventuallyresulting two variants sadl presented along algorithms improvements accuracy dl system measure . section iii sets research questions investigation . section iv describes experimental set - overcome limitations , propose new test empirical evaluations . section v presents results adequacy dl systems , called surprise adequacy dl empiricalevaluations . sectionviaddressesthreatstovalidity . systems ( sadl ). intuitively , good test input set sectionviipresentsrelatedwork , andsectionviiiconcludes . dl system systematically diversiﬁed include inputs ranging similar training data ii . surpriseadequacyfordeeplearningsystems signiﬁcantly different adversarial . 1 individual input existing test adequacy criteria dl systems aim granularity , sadl measures surprising input measure diversity input set . neuron coverage dl system respect data system trained ( nc ) [ 34 ] posits higher number neurons : theactualmeasureofsurprisecanbeeitherbasedonthe activated predeﬁned threshold , diverse likelihood system seen similar input inputthedlsystemhasbeenexecutedwith . deepgauge [ 27 ] training ( respect probability density distributions proposed range ﬁner grained adequacy criteria including extrapolated training process using kernel density k - multisection neuron coverage , measures ratio estimation [ 41 ]), distance vectors representing activation value buckets covered across theneuronactivationtracesofthegiveninputandthetraining neurons , andneuronboundarycoverage , whichmeasuresthe data ( heresimplyusingeuclideandistance ). subsequently , ratioofneuronsthatareactivatedbeyondtherangesobserved surpriseadequacy ( sa ) ofasetoftestinputsismeasuredby training . therangeofindividualsurprisevaluesthesetcovers . weshow argue diversity testing dl systems sadl sufﬁciently ﬁne grained training adversarial meaningful measured respect training example classiﬁers based sadl values produce data , asdlsystemsarelikelytobemoreerrorproneforinputs higher accuracy compared state art . also unfamiliar , . e ., diverse . furthermore , neuron show sampling inputs according sadl retraining activation thresholds , beyond observed ranges , may dl systems result higher accuracy , thus showing closely related diversity given input , sadlisanindependentvariablethatcanpositivelyaffectthe measure degree activations network effectiveness dl system testing . one input differs activations another input . technical contributions paper follows : fundamentally discretisations utilize fact • propose sadl , ﬁne grained test adequacy metric neuron activations continuous quantities . contrast , thatmeasuresthesurpriseofaninput , . e ., thedifference aim deﬁne adequacy criterion quantitatively behaviour dl system given input measures behavioural differences observed given set training data . two concrete instances sadl inputs , relative training data . areproposedbasedondifferentwaystoquantifysurprise . . activation trace surprise adequacy shown correlated existing coverage criteria dl systems . let n = { n1 , n2 ,...} set neurons constitutes • show sadl sufﬁciently ﬁne grained cap - dl system , let x ={ x1 , x2 ,...} set inputs . turing behaviour dl systems training highly denote activation value single neuron n accurate adversarial example classiﬁer . adversarial respect input x αn ( x ). ordered ( sub ) set example classiﬁer shows much 100 % 94 . 53 % neurons , let n ⊆ n , αn ( x ) denote vector activation roc - auc score applied mnist [ 25 ] values , eachelementcorrespondingtoanindividualneuronin cifar - 10 [ 21 ] dataset , respectively . n : cardinality αn ( x ) equal | n |. call αn ( x ) • show sadl metrics used sample activation trace ( ) x neurons n . similarly , effective test input sets . retraining dl systems us - letan ( x ) beasetofactivationtraces , observedoverneurons inn , forasetofinputsx : ( x )={ αn ( x )| x ∈ x }. 1experimentsshowbeneﬁtsofdiversityforgeneraltesting [ 15 ] andbeneﬁts ofa ‘ scaleofdistances ’ oftestinputsforrobustnesstestingintroducedin [ 35 ]. 2pleaserefertohttps :// github . com / coinse / sadl . 11004400 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . note activation trace trivially available since want measure surprise input x , execution network given input . needametricthatincreaseswhenprobabilitydensitydecreases since behaviours dl systems driven along data - ( . e ., input rarer compared training data ), ﬂow control - ﬂow , assume activation traces vice versa ( . e ., input similar training data ). observedoverallnwithrespecttox , ( x ), fullycaptures adopting common approach converting probability density behaviours dl system investigation measure rareness [ 26 ], [ 39 ], deﬁne lsa executed using x . 3 negative log density : surprise adequacy ( sa ) aims measure relative nov - elty ( . e ., surprise ) given new input respect lsa ( x )=− log ( fˆ ( x )) ( 2 ) inputs used training . given training set , ﬁrst computean ( ) byrecordingactivationvaluesofallneurons note extra information input types used tomakelsamoreprecise . forexample , givenadlclassiﬁer usingeveryinputinthetrainingdataset . subsequently , givena , expect inputs share class label newinputx , wemeasurehowsurprisingxiswhencompared comparing activation trace x ( ). sreimplialcairnagtts . wwiethca { nxex ∈ pltoit | thdis ( xb ) = comc } puftoinrgcllasssacp . ewrcelaussse , quantitative similarity measure called surprise adequacy per - class lsa dl classiﬁers empirical evaluation . ( sa ). introduce two variants sa , different way measuring similarity x ( ). 4 note certain types dl tasks allow us focus x1b x2b partsofthetrainingsetttogetmorepreciseandmeaningful measurement sa . example , suppose testing c2 boundary classiﬁer new input x , classiﬁed dl x1 b1 b2 learnt dl system investigation class c . case , c1 a1 x2 surpriseofxismoremeaningfullymeasuredagainstan ( tc ), x1a a2 tc subset members classiﬁed x2a c . basically , input might surprising example fig . 1 : anexampleofdistance - basedsa . blackdotsrepresent class c even surprising relation full set ats training data inputs , whereas grey dots represent ats training examples . ofnewinputs , x1andx2 . comparedtodistancesfromx1aand b . likelihood - based surprise adequacy x2a toclassc2 , atofx1 isfartheroutfromclassc1 thanthat kernel density estimation ( kde ) [ 41 ] way esti - ofx2 , . e ., ab11 > ab22 ( seeequations3 , 4 , and5 ). consequently , decide x1 surprising x2 w . r . . class c1 . mating probability density function given random variable . resulting density function allows estimation ofrelativelikelihoodofaspeciﬁcvalueoftherandomvariable . c . distance - based surprise adequacy likelihood - based sa ( lsa ) uses kde estimate proba - analternativetolsaissimplytousethedistancebetween bility density activation value ( ), obtains ats measure surprise . , deﬁne distance - surprise new input respect estimated basedsurprise adequacy ( dsa ) usingtheeuclideandistance density . extension existing work uses kde new input x ats observed detect adversarial examples [ 14 ]. reduce dimensionality training . distance metric , dsa ideally suited computational cost , consider neurons exploit boundaries inputs , seen selected layer nl ⊆ n , yields set activation classiﬁcationexampleinfigure1 . bycomparingthedistances traces , anl ( x ). reduce computational cost , a1 a2 ( . e ., distance new input ﬁlter neurons whose activation values show variance thereferencepoint , whichisthenearestatoftrainingdatain lowerthanapre - deﬁnedthreshold , , astheseneuronswillnot c1 ) distances b1 b2 ( . e ., distance c2 measured contribute much information kde . cardinality reference point ), get sense close class tracewillbe | nl |. givenabandwidthmatrixh andgaussian boundary new inputs . posit , classiﬁcation kernelfunctionk , theactivationtraceofthenewinputx , problems , inputs closer class boundaries xi ∈ , kde produces density function fˆas follows : surprising valuable terms test input diversity . hand , tasks without boundaries ( cid : 2 ) 1 inputs , prediction appropriate steering angle fˆ ( x )= | anl ( )| xi ∈ tkh ( αnl ( x )− αnl ( xi )) ( 1 ) awuittohnonmoocluasssdbriovuinngdacriaers ,, dansaatmoafyannoetwbienpeuatsibleyinagppfalircfarbolme . 3for sake simplicity , assume possible get another training input guarantee new completeactivationtracesfromalltheneuronsinadlsystem . fornetwork input surprising , may still located crowded architectures loops , recurrent neural nets ( rnns ) [ 18 ], parts space . consequently , apply dsa possibletounrolltheloopsuptoapredeﬁnedbound [ 40 ]. classiﬁcation tasks , effective 4however , themainideaisgeneralandother , speciﬁcvariantswouldresult ifusingothersimilarityfunctions . lsa ( see section v - v - b details ). 11004411 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . let us assume dl system , consists surprising . however , input arbitrarily high sa value set neurons n , trained classiﬁcation task may simply irrelevant , least less interesting , set classes c , using training dataset . given set problem domain ( e . g ., image trafﬁc sign activationtracesan ( ), anewinputx , andapredictedclass irrelevant testing animal photo classiﬁers ). , new input cx ∈ c , deﬁne reference point xa sc measured respect pre - deﬁned upper closest neighbour x shares class . bound , inthesamewaythetheoreticallyinﬁnitepathcoverage distance x xa follows deﬁnition : bounded parameter [ 44 ]. second , sc render itselftoacombinatorialsetcoverproblem , whichthetestsuite xa = argmin ( cid : 4 ) αn ( x )− αn ( xi )( cid : 4 ), minimisation often formulated [ 43 ]. ( xi )= cx ( 3 ) single input yields single sa value cannot belong dista =( cid : 4 ) αn ( x )− αn ( xa )( cid : 4 ) tomultiplesabuckets . thesenseofredundancywithrespect sc coverage criteria weaker structural subsequently , fromxa , weﬁndtheclosestneighbourofxa coverage , single input cover multiple targets . inaclassotherthancx , xb , andthedistancedistb , asfollows : aim show sa guide better selection inputs , rigorous study optimisation test suites dl xb = argmin ( cid : 4 ) αn ( xa )− αn ( xi )( cid : 4 ), systemsremainsafuturework . however , asweshowwithour ( xi )∈ c \\\\{ cx } ( 4 ) empirical studies , sc still guide test input selection . distb =( cid : 4 ) αn ( xa )− αn ( xb )( cid : 4 ) iii . researchquestions intuitively , dsa aims compare distance new input x known ats belonging class , empirical evaluation designed answer follow - cx , known distance ats class cx ats ing research questions . classes c \\\\{ cx }. former relatively larger rq1 . surprise : sadl capable capturing relative latter , x would surprising input class cx surprise input dl system ? classifying dl system . multiple ways formalise select simple one calculate dsa provide answers rq1 different angles . first , ratio dista distb . investigation wecomputethesaofeachtestinputincludedintheoriginal complicated formulations left future work . dataset , see dl classiﬁer ﬁnds inputs higher surprise difﬁcult correctly classify . expect dsa ( x )= dista ( 5 ) surprising input harder correctly classify . second , distb evaluate whether possible detect adversarial examples . surprise coverage based sa values , expect adversarial examples surprising well cause different behaviours givenaset ofinputs , wecanalsomeasuretherangeofsa dl systems . using different techniques , multiple sets values set covers , called surprise coverage ( sc ). since adversarial examples generated compared lsa dsa deﬁned continuous spaces , sa values . finally , train adversarial example classiﬁers use bucketing discretise space surprise deﬁne using logistic regression sa values . adversarial bothlikelihood - basedsurprisecoverage ( lsc ) anddistance - attackstrategy , wegenerate10 , 000adversarialexamplesusing based surprise coverage ( dsc ). given upper bound u , buckets b ={ b1 , b2 ,..., bn } divide ( 0 , u ] n sa 10 , 000 original test images provided mnist cifar - 10 . using 1 , 000 original test images 1 , 000 adversarial segments , sc set inputs x deﬁned follows : examples , allchosenrandomly , wetrainthelogisticregression classiﬁers . finally , weevaluatethetrainedclassiﬁersusingthe sc ( x )= |{ bi |∃ x ∈ x : sa ( x )∈( u · − n1 , u · ni ]}| ( 6 ) remaining 9 , 000 original test images 9 , 000 adversarial n examples . ifsavaluescorrectlycapture thebehaviourofdl set inputs high sc diverse set inputs systems , expect sa based classiﬁers successfully ranging similar seen training ( . e ., low detect adversarial examples . use area curve sa ) different seen training receiver operator characteristics ( roc - auc ) evaluation ( . e ., high sa ). argue input set dl system captures true false positive rates [ 8 ]. diversiﬁed , systematically diversiﬁed rq2 . layer sensitivity : selection layers considering sa . recent results also validate notion neurons used sa computation impact showingthatmoredistanttestinputsweremorelikelytolead accurately sa reﬂects behaviour dl systems ? exceptions might relevant testing [ 35 ]. whileweusethetermcoverandcoverage , theimplications bengio et al . suggest deeper layers represent higher ofsabasedcoverageisdifferentfromthetraditionalstructural levelfeaturesoftheinput [ 5 ]: subsequentworkthatintroduced coverage . first , unlikemostofthestructuralcoveragecriteria , kde based adversarial example detection technique [ 14 ] ﬁnite set targets cover , statement assumes deepest ( . e ., last hidden ) layer contain branchcoverage : aninputcan , atleastintheory , bearbitrarily information helpful detection . evaluate 11004422 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . table : list datasets models used study . dataset description dnnmodel # ofneuron syntheticinputs performance mnist handwritten digit images composed aﬁvelayerconvnetwith 320 fgsm , bim - , bim - 99 . 31 % ( ac - 50 , 000imagesfortrainingand10 , 000im - max - pooling dropout b , jsma , c & w . curacy ) agesfortest . layers . cifar - 10 object recognition dataset ten different 12 layer convnet 2 , 208 fgsm , bim - , bim - 82 . 27 % ( ac - classes composed 50 , 000 images max - pooling dropout b , jsma , c & w . curacy ) trainingand10 , 000imagesfortest . layers . udacity self - drivingcardatasetthatcontains dave - 2 [ 6 ] architecture 1 , 560 deepxplore ’ test 0 . 09 ( mse ) self - drivingcar cameraimagesfromthevehicle , composed fromnvidia . input generation via challenge of101 , 396imagesfortrainingand5 , 614 jointoptimization . imagesfortest . thegoalofthechallenge chauffeur [ 1 ] architecture 1 , 940 deeptest ’ combined 0 . 10 ( mse ) istopredictsteeringwheelangle . withcnnandlstm . transformation . assumption context sa calculating lsa u upper bound used rq3 compute sc , dsa individual layers , subsequently comparing divide range sa [ 0 , u ] four overlapping subsets : adversarialexampleclassiﬁerstrainedonsafromeachlayer . ﬁrst subset including low 25 % sa values ([ 0 , u ]), 4 2u rq3 . correlation : sc correlated existing coverage second including lower half ([ 0 , 4 ]), third including 3u criteria dl systems ? lower 75 % ([ 0 , 4 ]), ﬁnally entire range , [ 0 , u ]. thesefoursubsetsareexpectedtorepresentincreasinglymore addition capturing input surprise , want sc diversesetsofinputs . wesettherangertooneofthesefour , consistent existing coverage criteria based counting randomly sample 100 images r , train existing aggregation . ifnot , thereisariskthatscisinfactmeasuring models ﬁve additional epochs . finally , measure something input diversity . , check model ’ performance ( accuracy mnist cifar - 10 , whether sc correlated criteria . control mse dave - 2 ) entire adversarial synthetic input diversity cumulatively adding inputs generated inputs , respectively . expect retraining diverse differentmethod ( . e ., differentadversarialexamplegeneration subset result higher performance . techniques input synthesis techniques ), execute studied dl systems input , compare observed iv . experimentalsetup changes various coverage criteria including sc four evaluate sadl four different dl systems using ( ) existing ones : deepxplore ’ neuron coverage ( nc ) [ 40 ] original test sets , ( b ) adversarial examples generated three neuron - level coverages ( nlcs ) introduced deep - ﬁve attack strategies , ( c ) synthetic inputs generated gauge [ 27 ]: k - multisection neuron coverage ( kmnc ), neu - deepxplore [ 34 ] deeptest [ 40 ]. section describes ronboundarycoverage ( nbc ), andstrongneuronactivation studied dl systems input generation methods . coverage ( snac ). mnist cifar - 10 , start original test . datasets dl systems data provided dataset ( 10 , 000 images ), add 1 , 000 tableiliststhesubjectdatasetsandmodelsofdlsystems . adversarial examples , generated fgsm , bim - , bim - b , mnist [ 25 ] cifar - 10 [ 21 ] widely used datasets jsma , andc & w , ateachstep . fordave - 2 , westartfromthe machine learning research , collection originaltestdata ( 5 , 614images ) andadd700syntheticimages images ten different classes . mnist , adopt generated deepxplore step . chauffeur , widely studied ﬁve layer convolutional neural network stepadds1 , 000syntheticimages ( set1toset3 ), eachproduced ( convnet ) withmax - poolinganddropoutlayersandtrainitto applying random number deeptest transformations . achieve 99 . 31 % accuracy provided test set . similarly , adopted model cifar 12 - layer convnet rq4 . guidance : sa guide retraining dl systems max - pooling dropout layers , trained achieve 82 . 27 % improve accuracy adversarial examples accuracy provided test set . synthetic test inputs generated deepxplore ? evaluation sadl dl systems safety criti - evaluate whether sadl guide additional training cal domains , use udacity self - driving car challenge existing dl systems aim improved accuracy dataset [ 2 ], contains collection camera images againstadversarialexamples , weaskwhethersacanguidethe driving car . aim predict steering wheel selectionofinputforadditionaltraining . fromtheadversarial angle , model accuracy measured using mean squared examplesandsynthesisedinputsforthesemodels5 , wechoose error ( mse ) actual predicted steering angles . four sets 100 images four different sa ranges . given use pre - trained dave - 2 model [ 6 ], public artefactprovidedbydeepxplore6 , andapre - trainedchauffeur 5we could resume training chauffeur model additional ﬁve epochs , whichiswhyitisabsentfromrq4 . 6deepxploreisavailablefrom : https :// github . com / peikexin9 / deepxplore . 11004433 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . model [ 1 ], madepubliclyavailablebytheudacityself - driving neurons computationally infeasible due precision car challenge . dave - 2 consists nine layers including ﬁve loss . rq2 , set activation variance threshold convolutional layers , achieves 0 . 09 mse . chauffeur layers activation 7 activation 8 cifar - 10 10 − 4 , consists convnet lstm sub - model , reduces number neurons used computa - achieves 0 . 10 mse . tion lsa , consequently , computational cost . computation coverage criteria rq3 , use b . adversarial examples synthetic inputs conﬁgurations table ii . threshold nc set 0 . 5 . sadl evaluated using adversarial examples fornlcs , weallsetthenumberofsections ( k ) to1 , 000 . synthetic test inputs . adversarial examples crafted ap - lsc dsc , manually choose layer , number plying , totheoriginalinput , smallperturbationsimperceptible buckets ( n ), upper bound ( ub ). rq4 , humans , dl system investigation behaves layers chosen mnist cifar - 10 activation 3 incorrectly [ 16 ]. werelyonadversarialattackstogeneratein - activation 5respectively . weperform20runsofretrainingfor putimagesformnistandcifar - 10 : thesegeneratedimages subject report statistics . likely reveal robustness issues dl systems allexperimentswereperformedonmachinesequippedwith test inputs provided original datasets . use inteli7 - 8700cpu , 32gbram , runningubuntu16 . 04 . 4lts . ﬁve widely studied attack strategies evaluate sadl : fast mnist cifar - 10 implemented using keras v . 2 . 2 . 0 . gradient sign method ( fgsm ) [ 16 ], basic iterative method ( bim - , bim - b ) [ 23 ], jacobian - based saliency map attack v . result ( jsma ) [ 33 ], carlini & wagner ( c & w ) [ 9 ]. imple - duetothespacelimit , wecannotincludeallplotsandtables mentation strategies based cleverhans [ 32 ] make available online : https :// coinse . github . io / sadl . framework et al . [ 30 ]. . input surprise ( rq1 ) dave - 2 chauffeur , use state - - - art synthetic input generation algorithms , deepxplore [ 34 ] figure 2 shows classiﬁcation accuracy changes deeptest [ 40 ]. bothalgorithmsaredesignedtosynthesisenew classify sets images growing sizes test input existing ones aim detecting erro - test inputs included mnist cifar - 10 dataset . neous behaviours autonomous driving vehicle . dave - 2 , sets images corresponding red dots ( ascending use deepxplore ’ input generation via joint optimization sa ) start images lowest sa , increasingly algorithm , whose aim generate inputs lead multiple include images higher sa ascending order sa ; dlsystemstrainedindependently , butusingthesametraining sets images corresponding blue dots grow data , disagree . using dave - 2 two opposite direction ( . e ., images highest sa variants , dave - dropout dave - norminit , collect syn - lower sa ). reference , green dots show mean theticinputsgeneratedbylightingeffect ( light ), occlusionby accuracy randomly growing sets across 20 repetitions . asingleblackrectangle ( singleocc ), andocclusionbymultiple clear including images higher lsa values , . e ., black rectangles ( multiocc ). chauffeur , synthesise surprising images , leads lower accuracy . visual new inputs iteratively applying random transformations conﬁrmation another dataset , also chose sets inputs provided deeptest original input images : translation , synthesised chauffeur deeptest , three distinct scale , shear , rotation , contrast , brightness , blur . 7 levelsoflsavalues : figure3showsthatthehigherthelsa values , harder recognise images visually . table ii : conﬁgurations rq3 . quantitatively visually , observed trend supports claim sadl captures input surprise : even unseen dnn nc nlcs lsc dsc model th k layer n ub n ub inputs , sa measure surprising given input , mnist 0 . 5 1 , 000 activation 3 1 , 000 2 , 000 1 , 000 2 . 0 whichisdirectlyrelatedtotheperformanceofthedlsystem . cifar - 10 0 . 5 1 , 000 activation 3 1 , 000 100 1 , 000 2 . 0 figure4showsplotsofsorteddsavaluesof10 , 000adver - dave - 2 0 . 5 1 , 000 block1 conv2 1 , 000 150 n / chauffeur 0 . 5 1 , 000 convolution2d 11 1 , 000 5 n / sarial examples , generated ﬁve techniques , wellastheoriginaltestinputs . figure5containssimilarplots based lsa values 2 , 000 randomly selected adversarial c . conﬁgurations examples original test set , different layers research questions , default activation variance mnist cifar - 10 . mnist cifar - 10 , threshold lsa set 10 − 5 , bandwidth test inputs provided datasets ( represented blue kde set using scott ’ rule [ 36 ]. remaining colour ) tend least surprising , whereas majority section details rq speciﬁc conﬁgurations . rq1 , use adversarial examples clearly separated test theactivation 2layerformnist , andactivation 6forcifar - inputsbytheirhighersavalues . thissupportsourclaimthat 10 , computing lsa values . computation lsa based sadl capture differences dl system ’ behaviours adversarial examples . 7atthetimeofourexperiments , thepubliclyavailableversionofdeeptest finally , table iii shows roc - auc results dsa - didnotinternallysupportrealisticimagetransformationssuchasfogandrain effects . based classiﬁcation using neurons mnist cifar - 11004444 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . ( ) lowlsa ( ) selectedtestinputsbasedonlsainmnist ( b ) mediumlsa ( c ) highlsa ( b ) selectedtestinputsbasedondsainmnist fig . 3 : synthetic images chauffeur model generated deeptest . images higher lsa values tend harder recognise interpret visually . ( c ) selectedtestinputsbasedonlsaincifar - 10 ( ) selectedtestinputsbasedondsaincifar - 10 fig . 2 : accuracy test inputs mnist cifar - 10 dataset , selected input lowest sa , increas - fig . 4 : sorteddsavaluesofadversarialexamplesformnist ingly including inputs higher sa , vice versa ( . e ., cifar - 10 . fromtheinputwiththehighestsatoinputswithlowersa ). sadl capture relative surprise inputs . inputs 10 . 8 theresultsshowthatthegapindsavaluesobservedin higher sa harder correctly classify ; adversarial figure4canbeusedtoclassifyadversarialexampleswithhigh examples show higher sa values classiﬁed based accuracy . relatively simpler mnist model , dsa - sa accordingly . based classiﬁer detect adversarial examples roc - auc ranging 96 . 97 % 99 . 38 %. dsa - based clas - b . impact layer selection ( rq2 ) siﬁcation complicated cifar - 10 model shows table iv shows roc - auc classiﬁcation adver - lowerroc - aucvalues , butanswerstorq2suggestthatdsa sarial examples , resulting row corresponding fromspeciﬁclayerscanproducesigniﬁcantlyhigheraccuracy classiﬁer trained lsa dsa speciﬁc layer ( see section v - b ). mnist , respectively . rows ordered depth , . e ., basedonthreedifferentanalyses , theanswertorq1isthat activation 3isthedeepestandthelasthiddenlayerinmnist . highest roc - auc values attack strategy 8lsa - based classiﬁcation possible subsets neurons due typeset bold . mnist , clear evidence thecomputationalcostofkde ; henceweintroducetheresultsoflsa - based classiﬁcationwhenansweringtheimpactoflayerselectionforrq2 . deepest layer effective . 11004455 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . fig . 5 : sorted lsa randomly selected 2 , 000 adversarial examples mnist cifar - 10 different layers table iii : roc - auc dsa - based classiﬁcation adver - themostaccurateclassiﬁerforbim - . moreimportantly , per - sarial examples mnist cifar - 10 layer dsa values produce much accurate classiﬁcation dataset fgsm bim - bim - b jsma c & w results neuron dsa values , seen comparison table iii table iv & v . identical mnist 98 . 34 % 99 . 38 % 96 . 97 % 97 . 10 % 99 . 04 % cifar - 10 76 . 81 % 72 . 93 % 71 . 66 % 88 . 96 % 92 . 84 % models used produce results tables . tablev : roc - aucresultsofsaperlayersoncifar - 10 . table iv : roc - auc results sa per layers mnist . sa layer fgsm bim - bim - b jsma c & w sa layer fgsm bim - bim - b jsma c & w activation 1 72 . 91 % 61 . 59 % 63 . 30 % 76 . 85 % 74 . 01 % activation 1 100 . 00 % 99 . 94 % 100 . 00 % 98 . 17 % 99 . 48 % activation 2 89 . 59 % 62 . 17 % 73 . 20 % 80 . 33 % 75 . 98 % lsa activation 2 100 . 00 % 99 . 46 % 100 . 00 % 94 . 42 % 99 . 23 % pool 1 93 . 31 % 61 . 79 % 78 . 89 % 82 . 64 % 73 . 48 % pool 1 100 . 00 % 99 . 73 % 100 . 00 % 99 . 08 % 99 . 61 % activation 3 86 . 75 % 62 . 69 % 76 . 93 % 80 . 33 % 79 . 02 % activation 3 93 . 29 % 81 . 70 % 86 . 73 % 94 . 45 % 37 . 96 % activation 4 83 . 31 % 62 . 73 % 86 . 15 % 80 . 86 % 80 . 42 % activation 1 100 . 00 % 99 . 85 % 100 . 00 % 97 . 79 % 99 . 39 % lsa pool 2 82 . 82 % 61 . 16 % 89 . 69 % 80 . 61 % 73 . 85 % activation 2 100 . 00 % 99 . 39 % 99 . 99 % 97 . 59 % 99 . 69 % activation 5 83 . 80 % 60 . 64 % 96 . 31 % 79 . 56 % 64 . 60 % dsa pool 1 100 . 00 % 99 . 32 % 99 . 99 % 98 . 21 % 99 . 69 % activation 6 63 . 85 % 51 . 90 % 99 . 74 % 66 . 99 % 60 . 40 % activation 3 98 . 45 % 99 . 43 % 97 . 40 % 97 . 07 % 99 . 10 % pool 3 63 . 46 % 51 . 86 % 99 . 77 % 67 . 62 % 56 . 21 % activation 7 67 . 96 % 61 . 09 % 92 . 18 % 83 . 02 % 76 . 85 % activation 8 59 . 28 % 52 . 66 % 99 . 60 % 73 . 26 % 62 . 15 % activation 1 65 . 00 % 62 . 25 % 61 . 57 % 73 . 85 % 79 . 09 % cases roc - auc 100 % explained activation 2 77 . 63 % 64 . 73 % 67 . 95 % 78 . 16 % 81 . 59 % figure 5 : lsa values activation 1 mnist , pool 1 80 . 22 % 64 . 89 % 70 . 94 % 78 . 96 % 82 . 03 % activation 3 83 . 25 % 68 . 48 % 73 . 49 % 79 . 89 % 84 . 16 % example , show clear separation original test activation 4 81 . 77 % 68 . 94 % 77 . 94 % 80 . 55 % 84 . 62 % inputs fgsm , bim - , bim - b : choosing appro - dsa pool 2 82 . 51 % 69 . 28 % 81 . 43 % 80 . 92 % 84 . 81 % activation 5 81 . 45 % 70 . 29 % 83 . 28 % 82 . 15 % 85 . 15 % priatethreshold , itispossibletocompletelyseparatetestinputs activation 6 71 . 71 % 70 . 92 % 71 . 15 % 84 . 05 % 85 . 42 % adversarial examples . similarly , plot lsa pool 3 71 . 75 % 70 . 35 % 74 . 65 % 83 . 57 % 85 . 17 % activation 3 mnist shows c & w lsa line crossing activation 7 71 . 04 % 71 . 44 % 81 . 46 % 89 . 94 % 92 . 98 % activation 8 70 . 35 % 70 . 65 % 90 . 47 % 90 . 46 % 94 . 53 % original test data ( . e ., c & w adversarial examples less surprising original test data ): based results , answer rq2 dsa results low roc - auc value 37 . 96 %. sensitive selection layers computed , table v contains roc - auc values lsa - dsa - beneﬁts choosing deeper layer . however , basedclassiﬁers , trainedoneachlayerofthecifar - 10model : lsa , clear evidence supporting deeper attack strategy , highest roc - auc values layer assumption . layer sensitivity varies across different typeset bold . interestingly , lsa dsa show different adversarial example generation strategies . trendswithcifar - 10 . withlsa , thereisnostrongevidence c . correlation sc criteria ( rq3 ) deepest layer produces accurate classiﬁers . however , dsa , deepest layer produces table vi shows different coverage criteria respond accurateclassiﬁersforthreeoutofﬁveattackstrategies ( bim - increasing diversity levels9 . columns represent steps , b , jsma , andc & w ), whiletheseconddeepestlayerproduces 9seehttps :// coinse . github . io / sadlforplots . 11004466 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . inputs added original test set . highest accuracy mnist cifar - 10 , lowest increaseincoverageatastepislessthan0 . 1percentagepoint mse dave - 2 ). best performance typeset bold . 4 compared previous step , value underlined . thefullrange , , producesthebestretrainingperformance 4 2 3 threshold 0 . 1 percentage point based ﬁnest 13 conﬁgurations , followed ( 5 conﬁgurations ), ( 3 4 4 1 step change possible lsc , dsc , well kmnc , conﬁgurations ), ( 3 conﬁgurations ). note 4 2 threeusebucketingwithk = 1 , 000 . weacknowledgethatthe conﬁguration cifar - 10 bim - b , ranges 4 3 threshold arbitrary , provide supporting aid . produces best retraining performance . 4 notethatdsccannotbecomputedforthesetwodlsystems , largest improvement observed retraining mnist 4 classiﬁers ( see section ii - c ). againstfgsmusingdsa : theaccuracyofthe rangeshows 4 overall , studied criteria increase additional 77 . 5 % increasefromthatof 1 ( . e ., from15 . 60 % to28 . 69 %). 4 inputs added step . notable exception nc , retraining mnist bim - b using dsa shows whichplateausagainstmanysteps . thisisinlinewithresults evengreaterimprovement ( from9 . 40 % to40 . 94 %), wesuspect existing work [ 27 ]. exists interplay outlier accuracy ranges 1 2 4 4 type added inputs different criteria respond : signiﬁcantly smaller compared conﬁgurations . snac , kmnc , nbc show signiﬁcant increases observations limited dl systems addition bim - b examples cifar - 10 , change little inputgenerationtechniquesstudiedhere , weanswerrq4that whenc & winputsareadded . however , onlysnacandnbc sa provide guidance effective retraining exhibit similar increase addition input set 1 adversarial examples based interpretation chauffeur , whilekmncincreasesmoresteadily . overall , observed trend . exception nc , answer rq3 sc correlated dnn sa r fgsm bim - bim - b jsma c & w coverage criteria introduced far . model μ σ μ σ μ σ μ σ μ σ ∅ 11 . 65 - 9 . 38 - 9 . 38 - 18 . 88 - 8 . 92 - dnn criteria test step1 step2 step3 step4 step5 1 / 4 25 . 81 1 . 95 95 . 14 0 . 69 41 . 00 0 . 01 72 . 67 3 . 09 92 . 51 0 . 51 lsc 29 . 50 + f3g4s . 9m0 + bi3m7 .- 1a0 + b5im6 . 3 - b0 + j6s1m . 9a0 + c62 &. 0w0 mnist lsa 234 /// 444 222893 ... 467560 234 ... 969138 999555 ... 789170 000 ... 497189 444000 ... 999873 000 ... 111208 777557 ... 043387 221 ... 667805 999222 ... 545516 010 ... 607737 dsc 46 . 00 56 . 10 65 . 00 67 . 20 70 . 90 72 . 30 nc 42 . 73 42 . 73 43 . 03 43 . 03 43 . 03 45 . 45 1 / 4 15 . 60 2 . 12 93 . 67 3 . 42 9 . 90 1 . 05 74 . 56 2 . 62 12 . 80 0 . 96 mnist kmnc 68 . 42 70 . 96 72 . 24 75 . 82 77 . 31 77 . 37 dsa 23 // 44 1296 .. 6377 46 .. 3125 9955 .. 7387 00 .. 7903 490 .. 4801 00 .. 0252 7768 .. 1061 21 .. 6897 1122 .. 4367 11 .. 0104 nbc 6 . 52 14 . 55 16 . 36 36 . 06 38 . 03 43 . 48 4 / 4 27 . 69 5 . 59 95 . 31 0 . 98 40 . 94 0 . 04 76 . 60 2 . 38 13 . 61 1 . 19 snac 10 . 91 19 . 39 19 . 39 53 . 33 57 . 27 57 . 27 ∅ 6 . 13 - 0 . 00 - 0 . 00 - 2 . 68 - 0 . 31 - lsc 46 . 20 54 . 70 55 . 8 57 . 70 61 . 10 63 . 20 dsc 66 . 20 70 . 10 70 . 6 80 . 90 83 . 40 84 . 10 1 / 4 11 . 07 1 . 20 32 . 34 1 . 70 0 . 59 1 . 76 32 . 80 2 . 05 34 . 38 2 . 83 cifar - 10 nknsncmbacncc 2216862 .... 17555768 2216973 .... 23278061 226971 ... 3253 . 8108 23246437 .... 20919861 2326444 ... 7330 . 3112 23247447 .... 04871140 cifar - 10 lsa 2341 //// 4444 11112224 .... 97586936 2212 .... 11118796 33322225 .... 61798494 2222 .... 04297099 0000 .... 88609901 2210 .... 11700060 33333554 .... 88894132 2222 .... 58502141 44442554 .... 95729841 2222 .... 72008342 dnn criteria test + singleocc + multiocc + light dsa 23 // 44 1143 .. 6841 11 .. 9855 2391 .. 5993 32 .. 5727 00 .. 0011 00 .. 0000 3345 .. 4691 12 .. 8490 4446 .. 7196 22 .. 3425 4 / 4 13 . 12 1 . 41 32 . 17 2 . 36 0 . 60 1 . 76 37 . 32 1 . 58 46 . 21 2 . 72 lsc 30 . 00 42 . 00 42 . 00 76 . 00 nc 79 . 55 80 . 26 80 . 45 83 . 14 ( ) mnistandcifar - 10 dave - 2 kmnc 33 . 53 35 . 15 35 . 91 37 . 94 nbc 0 . 51 5 . 29 5 . 32 6 . 60 snac 1 . 03 10 . 58 10 . 64 13 . 21 dnn sa r singleocc multiocc light dnn criteria test + set1 + set2 + set3 model μ σ μ σ μ σ lsc 48 . 90 53 . 50 56 . 10 58 . 40 ∅ 0 . 4212 - 0 . 0964 - 0 . 3822 - nc 22 . 14 22 . 65 22 . 70 22 . 83 1 / 4 0 . 0586 0 . 0142 0 . 0539 0 . 0003 0 . 0573 0 . 0057 chauffeur kmnc 48 . 08 50 . 79 52 . 20 53 . 21 dave - 2 2 / 4 0 . 0540 0 . 0012 0 . 0562 0 . 0060 0 . 0560 0 . 0042 nbc 3 . 05 16 . 88 17 . 96 19 . 13 lsa 3 / 4 0 . 0554 0 . 0041 0 . 0544 0 . 0009 0 . 0570 0 . 0133 snac 3 . 93 18 . 37 19 . 41 20 . 93 4 / 4 0 . 0553 0 . 0028 0 . 0561 0 . 0042 0 . 0601 0 . 0111 table vi : changes various coverage criteria ( b ) dave - 2 increasing input diversity . put additional inputs table vii : retraining guided sa : sample 100 inputs original test inputs observe changes coverage values . u 2u four increasingly wider ranges sa : [ 0 , ], [ 0 , ], 4 4 3u [ 0 , ], [ 0 , u ], andretrainforﬁveadditionalepochsusing 4 . retraining guidance ( rq4 ) samples training data , measure accuracy table vii shows impact sa - based guidance mse entire adversarial synthetic inputs . retraining mnist , cifar - 10 , dave - 2 models . samplingfromwiderrangesimprovestheretrainingaccuracy . 1 4 column r represents increasingly wider 4 4 ranges sa inputs additional training sampled ; rows r = ∅ show performance vi . threatstovalidity dl system retraining . overall , 23 retraining primary threat internal validity study conﬁgurations ( 2 sa types × 2 dl systems × 5 adversarial correctness implementation studied dl systems , attackstrategies , and1satype × 1dlsystem × threeinput well computation sa values . used publicly synthesismethods ), eachofwhichisevaluatedagainstfoursa available architectures pre - trained models subjects rangeswith20repetitions . columnsμandσcontainthemean avoid incorrect implementation . sa computation depends standard deviation observed performance metric ( . e ., widely used computation library , scipy , 11004477 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . stood public scrutiny . threats external validity mostly dl systems . et al . proposed deepct , views concerns number models input generation ranges neuron activation values parameter choices techniques study . possible sadl less appliescombinatorialinteractiontesting ( cit ) tomeasurein - effectiveagainstotherdlsystems . whilewebelievethecore teractioncoverage [ 29 ]. scisdifferentfromdeepctassadl principleofmeasuringinputsurpriseisuniversallyapplicable , aimsto quantify amountofsurprise , ratherthan simply experimentations reduce particular risk . detectsurpriseviaincreaseincoverage . deepmutationapplies finally , threats construct validity asks whether principle mutation testing dl systems mutating measuring correct factors draw conclusion . training data , test data , well dl system , based studied dl systems , activation traces immediate artefacts source model level mutation operators [ 28 ]. oftheirexecutionsandthemeaningofoutputaccuracyiswell viii . conclusion established , minimising risk threat . propose sadl , surprise adequacy framework dl vii . relatedwork systems quantitatively measure relative surprise adversarial examples pose signiﬁcant threats perfor - input respect training data , call mance dl systems [ 7 ]. existing work surpriseadequacy ( sa ). usingsa , wealsodevelopsurprise machine learning community detection inputs . coverage ( sc ), measures coverage discretised feinman et al . [ 14 ] ﬁrst introduced kde means input surprise ranges , rather count neurons similarity measurement , aim detecting adversarial speciﬁc activation traits . empirical evaluation shows examples . sadl improves upon existing work sa sc capture surprise inputs accurately number different ways . first , generalise concept aregoodindicatorsofhowdlsystemswillreacttounknown ofsurpriseadequacy ( sa ) andintroducedistance - basedsa . inputs . sa correlated difﬁcult dl system ﬁnds second , ourevaluationisinthecontextofdlsystemtesting . input , used accurately classify adversarial third , ourevaluationofsadlincludesmorecomplicatedand examples . sccanbeusedtoguideselectionofinputsformore practical dl systems , well testing techniques effectiveretrainingofdlsystemsforadversarialexamplesas deepxplore deeptest . finally , show choice well inputs synthesised deepxplore . neurons limited impact lsa . acknowledgement arangeoftechniqueshasbeenrecentlyproposedtotestand work supported engineering research verify dl systems . existing techniques largely based center program national research foundation two assumptions . ﬁrst assumption variation korea funded korean government ( msit ) ( nrf - metamorphictesting [ 11 ],[ 31 ],[ 42 ]. supposeadlsystemn 2018r1a5a1059921 ), institute information & commu - producesanoutputowhengiveniastheinput , . e ., n ( )= . expect n ( ( cid : 3 )) ( cid : 7 ) ( cid : 3 ) ( cid : 7 ) . huang et al . [ 19 ] nications technology promotion grant funded ko - rean government ( msit ) ( . 1711073912 ), next - proposed veriﬁcation technique automatically gen - generation information computing development program erate counter - examples violate assumption . pei et national research foundation korea funded al . introduced deepxplore [ 34 ], white - box technique korean government ( msit ) ( 2017m3c4a7068179 ). generates test inputs cause disagreement among set dl systems , . e ., nm ( ) ( cid : 8 )= nn ( ) independently trained robert feldt acknowledges projects tocsyc ( swedish knowledge foundation , kks , num . 20130085 ) baseit dl systems nm nn . tian et al . presented deeptest , ( swedishsciencecouncil , vr , num . 2015 - 04913 ) forfunding whose metamorphic relations include simple geometric parts work paper . perturbations well realistic weather effects [ 40 ]. secondassumptionisthatthemorediverseasetofinputis , references moreeffectiveitwillbefortestingandvalidatingdlsystems . [ 1 ] autonomous driving model : chauffeur . https :// github . com / udacity / pei et al . proposed neuron coverage ( nc ), measures self - driving - car / tree / master / steering - models / community - models / ratio neurons whose activation values chauffeur . predeﬁned threshold [ 34 ]. shown adding test [ 2 ] udacity open source self - driving car project . https :// github . com / udacity / self - driving - car . inputs violate ﬁrst assumption increases diversity [ 3 ] google accident 2016 : google self - driving car caused crash measured nc . similarly , deepgauge introduced ﬁrst time http :// www . theverge . com / 2016 / 2 / 29 / 11134344 / set multi - granularity coverage criteria thought google - self - driving - car - crash - report , 2016 . [ 4 ] paul ammann jeff offutt . introduction software testing . reﬂect behaviours dl systems ﬁner granularity [ 27 ]. cambridgeuniversitypress , 2016 . criteria capture input diversity , [ 5 ] yoshuabengio , gre ´ goiremesnil , yanndauphin , andsalahrifai . better essentially count neurons unlike sa , therefore cannot mixingviadeeprepresentations . corr , abs / 1207 . 4404 , 2012 . [ 6 ] mariusz bojarski , davide del testa , daniel dworakowski , bernhard bedirectlylinkedtobehavioursofdlsystems . weshowthat firner , beatflepp , prasoongoyal , lawrencedjackel , mathewmon - sa closely related behaviours training accurate fort , ursmuller , jiakaizhang , etal . endtoendlearningforself - driving adversarial example classiﬁers based sa . cars . arxivpreprintarxiv : 1604 . 07316 , 2016 . [ 7 ] nicholas carlini david wagner . adversarial examples apart coverage criteria , concepts traditional easily detected . proceedings 10th acm workshop artiﬁcial softwaretestinghavebeenreformulatedandappliedtotesting intelligenceandsecurity - aisec ’ 17 , 2017 . 11004488 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . [ 8 ] nicholas carlini david wagner . adversarial examples [ 29 ] lei , fuyuan zhang , minhui xue , bo li , yang liu , jianjun zhao , easily detected : bypassing ten detection methods . proceedings yadong wang . combinatorial testing deep learning systems . 10th acm workshop artiﬁcial intelligence security , pages arxivpreprintarxiv : 1806 . 07723 , 2018 . 3 – 14 . acm , 2017 . [ 30 ] xingjun , bo li , yisen wang , sarah erfani , sudanthi wi - [ 9 ] nicholascarlinianddavida . wagner . towardsevaluatingtherobust - jewickrema , michael e houle , grant schoenebeck , dawn song , nessofneuralnetworks . corr , abs / 1608 . 04644 , 2016 . jamesbailey . characterizingadversarialsubspacesusinglocalintrinsic [ 10 ] chenyichen , ariseff , alainkornhauser , andjianxiongxiao . deepdriv - dimensionality . arxivpreprintarxiv : 1801 . 02613 , 2018 . ing : learningaffordancefordirectperceptioninautonomousdriving . [ 31 ] christian murphy , kuang shen , gail kaiser . automatic system proceedingsoftheieeeinternationalconferenceoncomputervision , testing programs without test oracles . proceedings 18th pages2722 – 2730 , 2015 . internationalsymposiumonsoftwaretestingandanalysis , issta2009 , [ 11 ] . . chen , f .- c . kuo , . h . tse , zhi quan zhou . metamorphic pages189 – 200 . acmpress , 2009 . testing beyond . proceedings international workshop [ 32 ] nicolas papernot , fartash faghri , nicholas carlini , ian goodfellow , softwaretechnologyandengineeringpractice ( step2003 ), pages94 – reuben feinman , alexey kurakin , cihang xie , yash sharma , tom 100 , september2004 . brown , aurkoroy , alexandermatyasko , vahidbehzadan , karenham - [ 12 ] zhihuacui , feixue , xingjuancai , yangcao , gai - gewang , andjinjun bardzumyan , zhishuai zhang , yi - lin juang , zhi li , ryan sheatsley , chen . detection malicious code variants based deep learning . abhibhav garg , jonathan uesato , willi gierke , yinpeng dong , david ieeetransactionsonindustrialinformatics , 14 ( 7 ): 3187 – 3196 , 2018 . berthelot , paul hendricks , jonas rauber , rujun long . technical [ 13 ] clementfarabet , camillecouprie , laurentnajman , andyannlecun . report cleverhans v2 . 1 . 0 adversarial examples library . arxiv learninghierarchicalfeaturesforscenelabeling . ieeetransactionson preprintarxiv : 1610 . 00768 , 2018 . patternanalysisandmachineintelligence , 35 ( 8 ): 1915 – 1929 , 2013 . [ 33 ] nicolaspapernot , patrickd . mcdaniel , someshjha , mattfredrikson , z . berkay celik , ananthram swami . limitations deep [ 14 ] reuben feinman , ryan r curtin , saurabh shintre , andrew b learninginadversarialsettings . corr , abs / 1511 . 07528 , 2015 . gardner . detecting adversarial samples artifacts . arxiv preprint [ 34 ] kexin pei , yinzhi cao , junfeng yang , suman jana . deepxplore : arxiv : 1703 . 00410 , 2017 . automatedwhiteboxtestingofdeeplearningsystems . inproceedingsof [ 15 ] robert feldt , simon poulding , david clark , shin yoo . test set the26thsymposiumonoperatingsystemsprinciples , sosp ’ 17 , pages diameter : quantifyingthediversityofsetsoftestcases . inproceedings 1 – 18 , newyork , ny , usa , 2017 . acm . oftheieeeinternationalconferenceonsoftwaretesting , veriﬁcation , [ 35 ] simonpouldingandrobertfeldt . generatingcontrollablyinvalidand andvalidation , icst2016 , pages223 – 233 , 2016 . atypicalinputsforrobustnesstesting . insoftwaretesting , veriﬁcation [ 16 ] ian goodfellow , jonathon shlens , christian szegedy . explaining validation workshops ( icstw ), 2017 ieee international confer - harnessing adversarial examples . international conference enceon , pages81 – 84 . ieee , 2017 . learningrepresentations , 2015 . [ 36 ] david w scott . multivariate density estimation : theory , practice , [ 17 ] geoffrey hinton , li deng , dong yu , george e dahl , abdel - rahman visualization . johnwiley & sons , 2015 . mohamed , navdeepjaitly , andrewsenior , vincentvanhoucke , patrick [ 37 ] ilya sutskever , oriol vinyals , quoc v le . sequence sequence nguyen , tara n sainath , et al . deep neural networks acoustic learning neural networks . advances neural information modeling speech recognition : shared views four research processingsystems , pages3104 – 3112 , 2014 . groups . ieeesignalprocessingmagazine , 29 ( 6 ): 82 – 97 , 2012 . [ 38 ] christianszegedy , weiliu , yangqingjia , pierresermanet , scottreed , [ 18 ] sepp hochreiter ju ¨ rgen schmidhuber . long short - term memory . dragomiranguelov , dumitruerhan , vincentvanhoucke , andandrew neuralcomputation , 9 ( 8 ): 1735 – 1780 , 1997 . rabinovich . going deeper convolutions . proceedings [ 19 ] xiaoweihuang , martakwiatkowska , senwang , andminwu . safety ieeeconferenceoncomputervisionandpatternrecognition , pages1 – veriﬁcation deep neural networks . rupak majumdar viktor 9 , 2015 . kuncˇak , editors , computeraidedveriﬁcation , pages3 – 29 , cham , 2017 . [ 39 ] l . tarassenko . biosign ™ : multi - parameter monitoring early springerinternationalpublishing . warning patient deterioration . iet conference proceedings , pages [ 20 ] se ´ bastienjean , kyunghyuncho , rolandmemisevic , andyoshuaben - 71 – 76 ( 5 ), january2005 . gio . onusingverylargetargetvocabularyforneuralmachinetranslation . [ 40 ] yuchi tian , kexin pei , suman jana , baishakhi ray . deeptest : proceedings 53rd annual meeting association automatedtestingofdeep - neural - network - drivenautonomouscars . computational linguistics 7th international joint conference proceedingsofthe40thinternationalconferenceonsoftwareengineer - onnaturallanguageprocessing ( volume1 : longpapers ), volume1 , ing , pages303 – 314 . acm , 2018 . pages1 – 10 , 2015 . [ 41 ] matt p wand chris jones . kernel smoothing . chapman [ 21 ] alex krizhevsky , vinod nair , geoffrey hinton . cifar - 10 hall / crc , 1994 . dataset . online : http :// www . cs . toronto . edu / kriz / cifar . html , 2014 . [ 42 ] shin yoo . metamorphic testing stochastic optimisation . pro - [ 22 ] alex krizhevsky , ilya sutskever , geoffrey e hinton . imagenet ceedingsofthe3rdinternationalworkshoponsearch - basedsoftware classiﬁcation deep convolutional neural networks . advances testing , sbst2010 , pages192 – 201 , 2010 . inneuralinformationprocessingsystems , pages1097 – 1105 , 2012 . [ 43 ] shinyooandmarkharman . regressiontestingminimisation , selection [ 23 ] alexey kurakin , ian j . goodfellow , samy bengio . adversarial andprioritisation : asurvey . softwaretesting , veriﬁcation , andrelia - examplesinthephysicalworld . corr , abs / 1607 . 02533 , 2016 . bility , 22 ( 2 ): 67 – 120 , march2012 . [ 24 ] yann lecun , yoshua bengio , geoffrey hinton . deep learning . [ 44 ] hongzhu , patricka . v . hall , andjohnh . r . may . softwareunittest nature , 521 ( 7553 ): 436 , 2015 . coverageandadequacy . acmcomput . surv ., 29 ( 4 ): 366 – 427 , december [ 25 ] yann lecun , corinna cortes , cj burges . mnist handwritten 1997 . digit database . & labs [ online ]. available : http :// yann . lecun . com / exdb / mnist , 2 , 2010 . [ 26 ] stijn luca , peter karsmakers , kris cuppens , tom croonenborghs , anoukvandevel , bertenceulemans , lievenlagae , sabinevanhuffel , andbartvanrumste . detectingrareeventsusingextremevaluestatistics applied epileptic convulsions children . artiﬁcial intelligence medicine , 60 ( 2 ): 89 – 96 , 2014 . [ 27 ] leima , felixjuefei - xu , jiyuansun , chunyangchen , tingsu , fuyuan zhang , minhui xue , bo li , li li , yang liu , jianjun zhao , yadongwang . deepgauge : comprehensiveandmulti - granularitytesting criteria gauging robustness deep learning systems . corr , abs / 1803 . 07519 , 2018 . [ 28 ] leima , fuyuanzhang , jiyuansun , minhuixue , boli , felixjuefei - xu , chaoxie , lili , yangliu , jianjunzhao , etal . deepmutation : mutation testing deep learning systems . arxiv preprint arxiv : 1805 . 05206 , 2018 . 11004499 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply .'\n",
      " 'comparative analysis government plans peruvian presidential candidates sdoun state policies national agreement based nlp honorio apaza alanoca josimar chire jimy oblitas data science research group national university moquegua ilo moquegua peru institute mathematics computer science icmc r university sao paulo usp sao carlos sp brazil p facultad de ingeniera universidad privada del norte cajamarca peru hapazaaunamedupe jecsuspbr jimyoblitasupnedupe abstract analysis government proposal elections c political parties vital choose next authorities city country paper use text mining approach analyze c documentsandprovideaneasyvisualizationtosupportaneasyanalysis besides comparison national plan based sustainable devel opmentobjectivesofununitednationsfromagendaisperfomed v using natural language techniques keywords natural language processing text mining data science system recommender elections politics peru south america introduction election authorities important event citizens choose peoplewhowillrepresentthemandpurposeprojectstoimprovethenationalre v gional context traditionally political parties promote candidates x mass media ie radio television social networks candidates travel visit cities gain electors r peru participate president elections requirement send gov ernment proposal plan jurado nacional de elecciones national elections jury document summarizes proposal candidates considering themostimportantproblemsforthepartyandsolutionsthattheypurposeusu ally documents dozens pages read citizens choose next authority besides united nations un purposed agenda summarize important issues need special attention governments related poverty communication discrimination united nations un adopted new international develop ment agenda agenda includes sustainable development honorio apaza alanoca josimar chire jimy oblitas goals targets agenda species need actions strengthen sustainable economic growth decent employment industrialization countriescaribbean theagendaconsidersacomplexcombinationoffairlydetailedthematic targets comprehensive approach requires addressing sustainable development necessary integration social economic environmen tal axes nieto although recognized country priorities agenda reference government plans seeking adequate sustainabledevelopmentofperuthereforemeasuringthealignmentorpossible evolution government plans presidential candidates necessary task context use software tools text mining emerges quick interesting proposal measure trends addition fact peruvian context tools used yet contrasts global trends use software tools already established cam paignsoftrumpandbolsonarointheunitedstatesusaandbrazilwhichil lustratepolicyfactsthathavebeenfavoredbyictsgarcianunes et al natural language processing shown potential promising tool ex ploit urban data sources authors cai suggest use urban big data sources still starting studied areas urban governance management public health land use functional zones mo bility urban design useful expanding study scales reducing research costs text mining area uses wellknow data mining approach data col lection exploration analysis visualization text mining focuses text analysis uses natural language techniques nlp many studies per formed analyze dierent problems dierent areas ie epidemiology chire saire oblitas cruz politicssharma shekhar mar keting etc applicationsoftextmininginpoliticsandelectionsieanticipatingpolit ical behaviour sangar et al study voting patterns bagui et al ., 2007 ], fraudidentiﬁcation [ poloni formolo , 2015 ], sentimentalanalysisofcitizens [ sharma ghose , 2020 ], electionresultprediction [ ramteke et al ., 2016 ] . objective paper analyze government proposal peruvian candidates president elections using text mining approach support easyunderstandingofthedocuments . besides , performamatchingprocesswith national plan adapted 2030 agenda , check important objective political parties . sectioniincludesthereviewofthebibliography , sectioniidevelopsthework proposal , section iii discloses results research section iv gives conclusions , last section presents future work . analysis government plans peruvian presidential candidates 3 2 proposal natural language processing process transformation text information numeric data [ di giuda et al ., 2020 ]. work based following research process : data collection data analysis reporting select retrieve data comparative analysis ( government plan report research results algorithm jaro candidates findings . winkler . presidency peru ). fig . 1 : research process , process planed used [ kim et al ., 2017 ] 2 . 1 data collection present work , 18 government plans candidates presidency republic peru collected . also sustainable development goals policies state national agreement , sustainable devel - opment goals ( sdgs ) promoted united nations , whose predecessor millennium development goals , constitute inclusive global agenda goals 2030 [ secretaria ejecutivo del acuerdo nacional , 2017 ]. 2 . 2 data analysis jarowinkleristhemainalgorithmtoperformcomparativetextanalysisofdoc - uments ( governmentplansofthecandidates ) withthesustainabledevelopment goals ( sdgs ) promoted united nations . ( cid : 40 ) 0 = 0 sim ( , )= ( 1 ) j 1 2 1 ( + + − ) 3 s1 s2 objective calculate distance strings texts written plans government candidates objectives policies sustainable development state national agreement . ﬁrst preliminary test research interested knowing results obtained jaro winkler . 2 . 3 reporting finally , thelaststageoftheresearchistomakeareportontheresultsobtained , case results jaro winkler distance plans candidates ’ governmentandtheobjectivesandsustainabledevelopmentpolicies state national agreement . 4 honorio apaza alanoca , josimar chire jimy oblitas 3 results section shows result frequency terms word cloud , seen candidate highlights particular topic , : system , health , program , etc . result due fact currently nation world suﬀering global pandemic , therefore , plans candi - dates ’ government propose proposals solve problems related health . also shows important issues education , economics , etc . beenneglected . especiallyissuesrelatedtosustainabledevelopmentgoals ( sdg ) promoted united nations . accion popular partido morado avanza pais alianza para el progreso apra democracia directa frente amplio frente esperanza fuerza popular juntos por el peru partido nacionalista peru libre patria segura podemos peru ppc renovavion popular somos peru union por el peru victoria nacional fig . 2 : cloud words plans government candidates among candidates ’ plans , one stands gov - ernment plan political party avanza pais economic issue , also seen accion popular political party uniform distribution government plan issues economy , health , education politics . seen figure 3 . inthiscasewecanvarytheissueswewanttomeasure , thiscanbeaccording tothecontextofthemomentanddiﬀerentsectorsofsociety , theyhavediﬀerent problems needs , important analyze points view , social classes thoughts . present graphical ( figure 4 ) representation similar government plans candidates presidency peru , figure 4 seen identical , see degree similarity , due fact government plans clearly address similar issues translate social problems ( health , economy , programs , etc .) government ( judiciary , corruption , congress , etc .). experiment , diﬀerences proliﬁc class also denoted , cases distance noticeable political parties considered analysis government plans peruvian presidential candidates 5 0 . 00035 0 . 00030 gobierno 0 . 00025 política educación 0 . 00020 salud 0 . 00015 economía religión 0 . 00010 accion popular partido morado avanza pais alianza para el progreso apra democracia directa frente amplio frente esperanza fuerza popular juntos por el peru partido nacionalista peru libre patria segura podemos peru ppc renovavion popular somos peru union por el peru victoria nacional 00 .. 0000000005 fig . 3 : important areas documents beontheleftwiththoseontheright . whichcanbesimilarinthedailyexercise , obviously diﬀerent thoughts , therefore diﬀerent proposals two sides peruvian politics . 1 . 0 0 . 0 2 . 5 0 . 9 5 . 0 7 . 5 0 . 8 10 . 0 0 . 7 12 . 5 15 . 0 0 . 6 17 . 5 0 . 0 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 17 . 5 fig . 4 : documents similarity 6 honorio apaza alanoca , josimar chire jimy oblitas section going analyze distance chains texts writteninthegovernmentplansofthecandidatesandtheobjectivesandpolicies sustainable development state national agreement , try diﬀerentiatethesimilaritiesbetweenthesetwodocuments , whenachainoftexts issimilartoanothermeansthatthedocumentcontainstextssimilartotheother . , could say government plan addresses one many sustainable development goals policies state national agreement . fin de la pobreza hambre cero 0 . 66 salud bienestar educacion de calidad 0 . 64 igualdad de genero agua limpia saneamiento 0 . 62 energia asequible contaminante trabajo decente crecimiento economico 0 . 60 industria innovacion e infreestructura reduccion de las desigualdades 0 . 58 ciudades comunidades sostenibles produccion consumo responsables 0 . 56 accion por el clima vida submarina 0 . 54 vida de ecosistemas terrestres paz justicia e instituciones solidas alianzas para lograr los objetivos 0 . 52 accion popular partido morado avanza pais alianza para el progreso apra democracia directa frente amplio frente esperanza fuerza popular juntos por el peru partido nacionalista peru libre patria segura podemos peru ppc renovavion popular somos peru union por el peru victoria nacional fig . 5 : documents similarity plan graph , seen government plan political partyavanzapaisaddressesmuchmorethanothersthegoalofpeace , justiceand solid institutions ( paz justicia e instituciones solidas ), followed political party renovacion ppular . however , little addressed objectives : underwater life ( vida submarina ), health well - ( salud bienestar ), end poverty ( ﬁn de la probreza ), etc . 4 conclusions algorithm jaro winkler based measuring distance text chains shows us interesting preliminary results , shows us diﬀerences government plans candidates presidency peru , well objectives sustainable development goals state policies national agreement . however , results reﬁned advanced artiﬁcial intelligence methods algorithms . analysis government plans peruvian presidential candidates 7 present want highlight way diﬀerences government plan documents graphically demonstrated , way showing document diﬀerences important electorate , withouthavingtoreadallthegovernmentplans , theycanobtainamoregeneral vision graphically . 5 future work oneofthefuturejobsistoexperimentwithhighlyadvancedartiﬁcialintelligence techniques discipline natural language processing text mining . would interesting study experience coherent argu - ments candidates debate government plan . must coherence ideas proposals written government plan candidate expresses debate , interviews press , etc . references bagui et al ., 2007 . bagui , ., mink , ., cash , p . ( 2007 ). data mining techniques study voting patterns us . data science journal , 6 ( 0 ): 46 – 63 . cai , 2021 . cai , .( 2021 ). naturallanguageprocessingforurbanresearch : asystem - atic review . heliyon , 7 ( 3 ): e06322 . caribbean , 2017 . caribbean , e . c . f . l . . . . ( 2017 ). 2030 agenda sustainable development . last modiﬁed : 2017 - 06 - 28t13 : 23 - 04 : 00 publisher : cepal . chire saire oblitas cruz , 2020 . chiresaire , j . andoblitascruz , j .( 2020 ). study ofcoronavirusimpactonparisianpopulationfromapriltojuneusingtwitterand text mining approach . pages 242 – 246 . di giuda et al ., 2020 . di giuda , g . ., locatelli , ., schievano , ., pellegrini , l ., pattini , g ., giana , p . e ., seghezzi , e . ( 2020 ). natural language processing informationandprojectmanagement , pages95 – 102 . springerinternationalpublish - ing , cham . garcia - nunes et al ., 2020 . garcia - nunes , p . ., rodrigues , p . ., oliveira , k . g ., da silva , . e . . ( 2020 ). computational tool weak signals classiﬁcation – detectingthreatsandopportunitiesonpoliticsinthecasesoftheunitedstatesand brazilian presidential elections . futures , 123 : 102607 . kim et al ., 2017 . kim , k ., joungpark , ., yun , ., andyun , h .( 2017 ). whatmakes touristsfeelnegativelyabouttourismdestinations ? applicationofhybridtextmining methodologytosmartdestinationmanagement . technologicalforecastingandsocial change , 123 : 362 – 369 . nieto , 2017 . nieto , . . ( 2017 ). crecimiento econo ´ mico e industrial - izacio ´ n en la agenda 2030 : perspectivas para ´ xico . problemas del desarrollo , 48 ( 188 ): 83 – 111 . poloni formolo , 2015 . poloni , . . andformolo , .( 2015 ). dataminingtoiden - tify fraud suspected electronic elections . 2015 ninth international conference complex , intelligent , software intensive systems , pages 19 – 23 . ramteke et al ., 2016 . ramteke , j ., shah , ., godhia , ., andshaikh , .( 2016 ). elec - tion result prediction using twitter sentiment analysis . 2016 international con - ference inventive computation technologies ( icict ), volume 1 , pages 1 – 5 . 8 honorio apaza alanoca , josimar chire jimy oblitas sangar et al ., 2013 . sangar , . b ., khaze , . r ., ebrahimi , l . ( 2013 ). participa - tion anticipating elections using data mining methods . secretaria ejecutivo del acuerdo nacional , 2017 . secretaria ejecutivo del acuerdo na - cional ( 2017 ). objetivos de desarrollo dostenible politicas del estado del acuerdo nacional . sharma ghose , 2020 . sharma , . ghose , u . ( 2020 ). sentimental analysis twitter data respect general elections india . procedia computer science , 173 : 325 – 334 . international conference smart sustainable intelligent computing applications icitetm2020 . sharma shekhar , 2020 . sharma , . shekhar , h . ( 2020 ). intelligent learning basedopinionminingmodelforgovernmentaldecisionmaking . procediacomputer science , 173 : 216 – 224 .'\n",
      " 'jinstengindiaserb httpsdoiorgs original contribution text similarity measures news articles vector space model using nlp ritika singh satwinder singh receivedjuneacceptedoctober cidtheinstitutionofengineersindia abstract present global size online news websites keywords bilingual news article similarity cid million according marketingprofs cosine similarity cid jaccard similarity cid euclidean distance morethanmillionarticlesarepublishedeverydayonthe web online news websites also circulated edi torialcontent overthe internetthat species articles introduction display websites home pages articles highlight eg broad text size main news articles huge increase number online newspaper pub many articles posted news website lishing digital technology innova similar many news websites selective tions modern world much information reporting top news headlines also similarity appears tremendous speed readers need nd among news across various news associations well reading true news false news false news identiedbutnotverywellcalculatedthispaperidenties information endanger confuse persons top news items news sites measures lifebutalsoanentiresocietysoitisveryimportanttond similarity two news items two languages source information compare hindi english referring event news study interest extracting online accomplish highlighted headline link extractor news platforms specically measure similarity created extract top news hindi news articles across various sites article provides englishfromgooglesnewsfeedfirsttranslatethehindi detailsaboutwhatnewsisbeingconsideredhowitisbeing news article english using google translator presented highlighted website news compare english news articles second articles published website usually appear used cosine similarity jaccard similarity euclidean similar rectied form several different websites distance measure calculate news similarity score similar almost identical news confusing users frequency nouns next word nouns similarity slows process discovering new news articles also extracted methodology clearly information topic potentially leads missing showsthatwecanefcientlyidentifytopnewsarticlesand informationiftheusermistakenlyrecognizestwonewsas measure similarity news reports similarwheninfactonecontainsnewdataitismuchmore difcult locate similar news items websites large amount miscellaneous content material articles although main news article ritikasingh text similar two different web pages extra ritikasinghoutlookcom neous material pages may satwindersingh fore traditional approaches equivalent news satwindersinghcupeduin determination would fail first paper developed departmentofcomputerscienceandtechnologycentral method scraping top news headline text web universityofpunjabbathindabathindaindia pagesiegooglenewsfeedwebsiteswhicharepresentin jinstengindiaserb two different languages hindi english referring rst created headline link extractor sameevent use extracted textto classify news parses selected news websites searched ten us pairs content avoiding irrelevant based news site home pages months use informationonthearticlesbymeasuringasimilarityscore parser extract k news site max news pairs based method called cosine similarity imum number articles second author uses cal jaccard similarity euclidean similarity culation cosine similarity quantify similarity research distinguish similar news articles well news also provide techniques work differentonesthepurposeofthispaperisalsotodiscover assistinanalyzingarchivednewswebpagesbyintroducing bilingual news articles comparable corpus tools parsing select html news sites hero particular study dealing representation headlinestoriesusingcssselectorsauthorsstudiesover news measurement similarity among new months shown overall similarity decreased articles experiment uses similarly named entities asthenumberofarticlesincreasedstudiesfromtheauthor include representative features news indicate would set synchronous stories toassessthesimilaritybetweenarticlesofthesamenews given day besides relevant national events approach research proposing new method focused canbeusedtofurtherexaminetheoccasionalelectionsthat knowledge base framework aims provide human held information value category named entities katarzyna baraniak marcin sydow work tools within news comparable corpus news would support detection analysis hindi english compared approach tradi information bias author uses methods auto tional one obtains better results similarity also matically identify articles reporting sub distance measures calculate similarity two docu ject event entity use comparative mentsorsentencesintoasinglenumericalvalueandbrings analysisortoconstructatestortrainingcollectionwithin degree semantic similarity distance paper author explains representations doc oneanotherseveralsimilaritymeasureshavebeenusedby ument text method similarity measures text researchers much work done clustering include tests cosine similarity similarity newspapers study aims compare euclideandistancejaccardcoefcientpearsoncoefcient semanticsimilaritybetweentwoarticlesofthesamenews correlation averaged kullbackleibler diver present two different languages hindi english gence author also applies machine learning optimize human understanding basic concept approach recognize similar article develop measuring news similarities identify feature articles machine learning model detects similar articles auto vectors thereafter measure difference maticallyidentifyingfragments oftext concerningsimilar features low distance features eventsandidentifyingbiasinthemisexpectedtheauthor implies high level similarity value large dis - also working expand research study lan - tance features implies low level guages ( e . g ., polish , english ). similarity value [ 6 ]. euclidean distance , cosine distance , maake benard magara et al ., suggest system use jaccardcoefﬁcientmetricsaresomeofthedistancemetrics 220 artiﬁcial intelligent research paper written 8 artiﬁ - used document similarity computation . study cial intelligence experts [ 8 ]. work uses recursive explores twoseparatemethods ofgeneratingfeatures partitioning , random forest , improved machine texts : ( 1 ) tf - idf vectors , ( 2 ) bag words also learning algorithms average accuracy implements two methods calculating textual similarity timing efﬁciency 80 . 73 2 . 354628 . seconds , news articles : ( 1 ) cosine similarity jaccard algorithm typically performed quite well compared similarity tf - idf vectors ( 2 ) euclidean distance boosted even random forest algorithms . using bag words . sophisticated models used future studies much like latent semanticanalysis ( lsa ), sincedocuments canbeidentiﬁedasbelongingtothesameclassevenifthey literature rereview similar words phrases . vikas thada dr . vivek jaglan authors used cosine similarity , dice literature , similarity measures used coefﬁcient , jaccard similarity algorithms [ 9 ]. work various purposes . section , proposals completedontheﬁrst10pagesofthegooglesearchresult reviewed . expanded 30 – 35 pages reliable efﬁ - atkins et al . [ 1 ] describe technique assess top ciency estimate future study . cosine similarity news headline story selected set us - based news eventually concluded best ﬁtness compared websites , calculate correlations across . others dataset . summary , initial 123 j . inst . eng . indiaser . b ﬁndings promising , still long way go achieve greatest crawling efﬁciency possible . sys - tematicmethodproposedbynasabetal .[ 10 ] thefollowing points determine similarities . ( 1 ) article texts divided three sections headings , abstracts keywords . ( 2 ) abstract , keywords , based linkto title article weighing . ( 3 ) weighted mean esti - mated based description , abstract , keyword use pearson ’ correlation method ﬁnd similarity person machine scores . 87 % accuracy proposed technique . use specialized fig . 1 aframeworkforcomparativeanalysis wordnetitcanalsoconcentrateonarticlesimilarities . proposedframeworkcanbeusedforothertextsthatrequire wordnet language , texts persian languages . . snover et al ., explore new way usingmonolingualtargetdatatoenhancetheefﬁciencyofa statisticalorpredictivemachinetranslationfornewsstories jaccard similarity measures . ﬁnal step frame - [ 11 ]. thismethodemployscomparabletextvarioustextsin work compare analyze produced results . target language explore equivalent explain steps detail . stories mentioned source language document . dataset used paper known ‘ google large monolingual datasetfor source documenttobe news ’, publicly available [ 13 ]. google news : translated target language , searched google offering special experience google news documents may similar source documents . combines news items one . provides experimental results paper generated constant , personalized ﬂow newspapers thousands thedifferenceofthelanguageandtranslationmodelsshow ofpublishersandmagazinesgroupedaround . googlenews vital improvements baseline framework . combination global events , local news news qian et al . [ 12 ] using comparable corpus , bilingual stories ’ reading . turn dependency mapping model bilingual lexicon building headlines show top news world . english chinese . model considers additional sections allow delve various dependent words relationships measuring topics sports , business technology . similarity bilingual words thus offers greatest value service delivered news 35 precise less noisy representation . author ’ also languages using google news experiment extracts illustrated bilingual dependency mappings news articles hindi english languages . created optimized automatically without human input , contributing medium - sized set dependency map - headline link extractor pings impacts bilingual lexicon con - struction ( blc ) fully exploited weight basic python library searching downloading live learningusingasimplebuteffectiveperceptronalgorithm , news articles google news feeds googlenews making approach quickly adaptable several gnewsclient [ 14 ]. using , one pick top language pairs . headlines running google ’ news websites check top headlines particular subject ( keyword ). experiment use , extract links hindi methodology english news related event . major steps methodology given . article scraping figure 1 presents framework work . textual news data ﬁrst pre - processed rep - ‘ newspaper ’ isapythonmoduleusedtoextractnewspaper resented structural format . two represen - articles parse . newspapers using special - tationmethodsofgeneratingfeaturesfromthetextthatare ized web scrapping algorithms extract valuable investigatedinthisstudyaretf - idf , andbagofword . textfromawebsite . thisworksextremelywellonwebsites represented three representation methods , online newspapers . experiment extracted represented method compared three similarity links hindi english news , also measures shown fig . 1 . e . cosine , euclidean extract text using newspaper module . 123 j . inst . eng . indiaser . b translator idf vectors used feature vectors measure similarity articles news - results . using package , google offers language translation package python ; words taken similarities measures hindi news articles translated different languages ( english language ). either hindi corpus translated similarityfunctionisareal - valuedfunctionthatcalculates intoenglishorenglishcorpuscanbetranslatedintohindi . similarity two items . calculation sim - translated hindi corpus english . ilarity achieved mapping distances similarities translation performed level sentences . withinthevectorspace . thisexperimentprovidestwotests translation also generates map words various lan - ofsimilarity : cosinesimilarity , similaritywithjaccard , guages , english . research used bilingual dic - euclidean distance . tionaries ranging hindi english . ( 1 ) cosine similarity cosine angle n - di - mensionalspace , betweentwon - dimensionalvectors . pre - processing data cleaning isthedotproductofthetwovectors , dividedby - productof two vectors ’ lengths ( magnitudes ) [ 16 ]. simi - pre - processingstepssuchastheeliminationofstop - words , larity cosine measured using following lemmatization , parsing letters , punctuation marks , formula : numbers completed . words lemma - : b pn ( cid : 4 ) b tized wordnetlemmatizer nltk library took similarityða ; bþ¼jjajj ( cid : 4 ) jjbjj¼pﬃpﬃﬃﬃﬃﬃnﬃﬃﬃiﬃ¼ﬃﬃaﬃ1ﬃﬃ2ﬃﬃpi ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ english stop - words [ 15 ]. i¼1 i¼1 ð2þ vector space model asshowninfig . 2 , supposethere istwopoint ’ sp1and p2 , distance within points increases amathematicalmodelisalsocalledthetermvectormodel , similarity points decreases vice versa . whichdescribestextdocumentsasidentiﬁervariables , 1 ( cid : 5 ) cosinesimilarity¼cosinedistance terms tokens . course , term depends comparisons , usually , words , keywords sen - theresultoftheanglewillshow theresult . ifthe angle tences compared . is0betweenthedocumentvectorsthenthecosinefunction 1 documents . angel feature vectors value cosine function less 1 . angle reach - 1 documents artiﬁcial intelligence feature vector n - dimen - completely different ? thus way calculating sional vector computational features describe cosine angle vectors p1 p2 decides entity . really important method calculating vectors pointing direction . semanticsimilarityamongtexts . methodswereusedduring ( 2 ) jaccard similarity jaccard similarity calculates experiment measure function vectors tf - idf similaritiesamongsets . ’ sdeﬁnedastheintersectionsize ( term frequency - inverse document frequency ) sim - ple algorithm transforming text meaningful representation numbers . tf - idf weight measure factwhichevaluatestheimportanceofaspeciﬁcwordina text . mathematics , ( cid : 2 ) ( cid : 3 ) x n tfidf weight¼ tf ( cid : 3 ) log ð1þ ; df i2d document , tf number occurrences , theithterm , df isthenumber ofdocumentswhichcontain ithterm ; nis total number documents . thesklearn - vectorized function wasused toconstructatf - idffunction . thiswholemodelwasconstructedbyusingthedocuments , andagroupofsuchtf - idfvectors wasgeneratedconsisting tf idf weight term documents . tf - fig . 2 cosinesimilarity 123 j . inst . eng . indiaser . b divided bythe unionsize oftwo sets . jaccard similitude ac2 ¼ab2þbc2 ð6þ determined using formula [ 16 ] . pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ac ¼ ab2þbc2 ð7þ b b jða ; bþ¼ ¼ ð3þ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ b ‘ jajþjbj ( cid : 5 ) ja bj ac ¼ ðx2 ( cid : 5 ) x1þ2þðy2 ( cid : 5 ) y1þ2 ð8þ \\\\ representsintersectionand [ representstheunion . sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ inthispaper , aandbarebagsofwordsthatcontainnews jx ! yj¼ xðx ( cid : 5 ) yþ2 ð9þ articles . i¼1 • jaccard ( , ) = 1 table 1 shows comparative analysis methods • jaccard ( , b ) = 0 b = 0 based relative pros cons . table also • b ’ size describes application areas selected • always assign number 0 1 . techniques used . jaccard distance instead similarity measures similarity score dissimilarity found subtracting jaccard similarity coefﬁcient 1 : similarityscoremeansthattwodatasetsarehowsimilarto jdða ; bþ¼1 ( cid : 5 ) jdða ; bþ ð4þ one another . data collection include two separate ja [ bj ( cid : 5 ) ja [ bj textsasinthiscase . thesimilaritybetweenthetwotextsis jdða ; bþ¼ ð5þ ja [ bj evaluated according scoring system . euclidean dis - tance ﬁnd similarity texts , ( 3 ) euclidean distance another similarity measure ﬁndsthemetric , thedistancebetweenbothtexts [ 18 ]; vector space model euclidean distance l2 different ways calculate similarity : distance , euclidean norm . similarity measure 1 differentiates similarity measurements similarityða ; bþ¼ ð10þ 1þdistanceða ; bþ vector space model judging angle like rest rather direct distance vector noun phrase extraction inputs . asshowninfig . 3 , iftherearetwopointslike ( x1 , y1 ) noun phrase extraction technique text analysis , ( x2 , y2 ) let us consider dimension point consistingoftheautomatedextractionofnounsinatext . one wants ﬁnd distance ( x1 , y1 ) helps summarize contents text identify ( x2 , y2 ) basically use particular parameter like key topics discussed . paper concludes euclideandistancetocheckthatifthisparticularpointsare extraction frequency noun phrases fre - nearertoeachotherthanitwillconsiderthatthistwo - point quencyofthenextwordofthenounfromnewsarticlescan similar . euclidean distance calcu - considerably improve similarity measures . textblob latedbasedonthepythagorastheorem . letdrepresentthe python module used extract noun [ 19 ]. measureofdistancesbetween ( x1 , y1 ) ( x2 , y2 ). hence distance c expressed : proposed method paper introduces two methods calculating similaritybetweentwoarticlesofthesamenews , whichare present two different languages ( hindi english ), based methods calculating feature vector similarity measures . cosine similarity jaccard similarity tf - idf vectors thepre - processednewsarticleswereturnedintovectorsof tf - idf using vectorized model tf - idf . vectors obtainedwereasparse - matrixcontainingtf - idfweightsfor news article word dimensions [ number fig . 3 euclideandistance 123 j . inst . eng . indiaser . b table1 comparisonoftheprosandconsofdifferentmeasuresandtheirapplicationarea si . similarity pros cons applicationarea measures 1 cosine bothcontinuousandcategoricalvariablesmaybe ’ tworkeffectively textmining , documentsimilarity similarity used withnominaldata [ 17 ] 2 jaccard bothcontinuousandcategoricalvariablesmaybe ’ tworkeffectively documentclassiﬁcation coefﬁcient used withnominaldata 3 euclidean easytocomputeandworkwellwithadatasetwith doesnotworkwithimage applicationinvolvingintervaldata , dna distance compactorisolatedclusters [ 17 ] dataefﬁciently analysis , k - meanalgorithm news articles * number features ( distinct words )] [ 16 ]. bag words euclidean distance tf - idf weight matrix used featureforeverytext , andsimilarityamongnewsarticlesis pre - processed documents described calculated using cosine similarity jaccard similarity . vector frequency word compare sklearn ’ sbuilt - incosineandjaccardsimilaritymodulewas similar comparing bag vector words . used measure similarity . experiment uses bag - - words model computer processes vectors much faster vast ﬁle table2 samplepairofcompletelysimilarnews table3 samplepairofdifferentnewsstoriesaboutthesametopic 123 j . inst . eng . indiaser . b table4 samplepairofcompletelydissimilarnews performedonpairsofnewsheadlineobtainedfromgoogle news [ 14 ]. thechosennewsarticlesarelistedintables 2 , 3and4 . thenewsarticlesweregiventoahumanexpertto judge similarity dissimilarity . result , human expert determined 6 pairs ( pair 1 – 6 ) com - pletely similar news 5 pairs ( pair 7 – 11 ) different news topic 5 pairs ( pair 12 – 16 ) completely dissimilar news . expert judg - ment used benchmark evaluate automatic similarity calculation news articles . cosine similarity , jaccard coefﬁcient , euclidean distance applied . result three measures shown tables 5 , 6 7 . toprovideabetterunderstandingofthethreecompared measures , theresults areshownonabargraph asdepicted fig . 4 . figure 5 shows similarity measures bar graph different news stories topic . figure 6 shows similarity measures bar graph completely dissimilar news . performance measures used experiment accuracy , precision , recallandf - measures . thesemeasures calculated determining number news articles correctlyidentiﬁedassimilarordissimilarcomparedtothe decisionsbyhumanexperts [ 21 ]. inotherwords , usingthe human decisions benchmark number true pos - itive ( tp ) equivalent actual similar news cor - rectly identiﬁed similar , true negative ( tn ) equivalent actual dissimilar news correctly identiﬁed dissimilar , falsepositive ( fp ) whichisequivalenttoactual similar news incorrectly identiﬁed dissimilar , false - text lot data [ 20 ]. paper load news negative ( fn ) whichisequivalenttoactualdissimilarnews articles list called corpus calculate feature incorrectly identiﬁed similar determined . , vectors documents ﬁnally compute accuracy calculated ( tp ? tn )/ data , precision euclideandistanceandthentocheckhowsimilartheyare . tp /( tp ? fp ), recallistp /( tp ? fn ) andthef - measures greaterthedistance , lesssimilartheyare . thispaperusesa harmonic mean precision recall , module library called sklearn machine equal 2tp /( 2tp ? fp ? fn ) [ 21 ]. results pre - learning library . sented next section . results discussion result analysis figure 7 presents graph similarity measurements proposed algorithms implemented using python sample pair news articles using euclidean , jaccard 3 . 7 . 3 ( 64 - bit ). fortheexperiment , around1000newsstories cosine similarity measures representation wererandomlypickedfromthedataset . thealgorithmruns schemes . e . tf - idf , bag word representation . dataset , measures compares various learned fig . 7 , cosine performs similar similarity score . every news article ’ similarity benchmarkfornewswiththesamemeaning ( pair1 – 6 ) calculated every article . different news topic ( pair 7 – 11 ) - everforcompletelydissimilarnews ( pair12 – 16 ) jaccard ’ comparative analysis euclidean score similar human benchmark . toproveourpointfurther , wecalculatedthecorrelation analyze performance representation method scores similarity measures human different similarity measures , experiment benchmark shown table 8 . 123 j . inst . eng . indiaser . b table5 similaritymeasuresofcompletelysamenews cosine jaccard euclidean 1 . . cosinesimilarity jaccardsimilarity euclideansimilarity 0 . 8 1 0 . 8931 0 . 38075 0 . 04679 0 . 6 2 0 . 856 0 . 2134 0 . 02764 3 0 . 8476 0 . 3589 0 . 04861 0 . 4 4 0 . 7434 0 . 3289 0 . 04610 0 . 2 5 0 . 8034 0 . 2086 0 . 08609 0 6 0 . 7899 0 . 2756 0 . 02440 news 1 news 2 news 3 news 4 news 5 fig . 5 comparison similarity coefﬁcients different news articlesaboutthesametopic table6 similarity measures different news stories cosine jaccard euclidean sametopic 1 . . cosinesimilarity jaccardsimilarity euclideansimilarity 0 . 8 7 0 . 7063 0 . 1168 0 . 02311 0 . 6 8 0 . 5301 0 . 047 0 . 01377 0 . 4 9 0 . 5459 0 . 1516 0 . 04511 0 . 2 10 0 . 6316 0 . 1196 0 . 03771 0 11 0 . 7182 0 . 132 0 . 02990 news 1 news 2 news 3 news 4 news 4 fig . 6 comparison similarity coefﬁcients completely dissim - ilarnews table7 similaritymeasuresofcompletelydissimilarnews . . cosinesimilarity jaccardsimilarity euclideansimilarity human cosine jaccard euclidean 12 0 . 3447 0 . 066 0 . 0434 1 13 0 . 4032 0 . 0705 0 . 00804 es 0 . 8 ur 14 0 . 4843 0 . 0996 0 . 0334 ea 0 . 6 15 0 . 5490 0 . 1003 0 . 0298 0 . 4 16 0 . 3466 0 . 08503 0 . 02949 arit mil 0 . 2 si 0 1 2 3 4 5 6 7 8 9 10111213141516 cosine jaccard euclidean news hadlines 1 fig . 7 similarityscoregraph 0 . 8 0 . 6 0 . 4 ( tables 9 , 10 , 11 ) ﬁnd accuracy , precision , recallandf - measuresasexplainedintheprevioussection . 0 . 2 table 12 gives clear picture performance 0 similarity measure . analyzing results see news 1 news 2 news 3 news 4 news 5 news 6 theprecisionvalueofjaccardmeasuresis1 . 0or100 % fig . 4 comparison similarity coefﬁcients articles less 50 % euclidean distance . however , euclidean news gives high value recall compared precision . cosine measuregives good accuracy level andf1 score , correlation score table 8 , per - difference recall value precision ceived cosine jaccard similarity cor - high . , among three methods cosine similarity related benchmark scores . analyze usingtf - idfshowedgreateraccuracy , recallandf - measure produced result calculating confusion matrix [ 3 ] scores 81 . 25 %, 100 % 76 . 92 %, respectively . 123 j . inst . eng . indiaser . b conclusion table8 correlationofthesimilarityscorestothebenchmark method correlation ongoing research conducted comparison three cosineandbenchmark 0 . 919847 different methods estimate semantic similarity jaccardandbenchmark 0 . 816131 among two news articles ( nearly ) topic / event euclideanandbenchmark 0 . 422671 measure similarity two different languages ( hindi english ). experiment tested using googlenews data sets . three methodologies arethesimilarityofcosinewithtf - idfvectors , similarityof jaccard tf - idf vectors , bag words euclidean dis - table9 confusionmatrixforcosinesimilarity tance . allthreeofthesemethodsshowedpromisingresults , among three methods , cosine similarity using tf - 16news predicted : predicted : yes idf showed greater accuracy , recall f - measure scores total : 8 8 81 . 25 %, 100 % 76 . 92 %, respectively . accuracy actual : tn = 8 fp = 3 two methods may improved actual : yes fn = 0 tp = 5 doc2vec model [ 6 ], takes text corpus input thresholdvalue : 0 . 788 generates document vectors output . experiment totalreﬁnednews : 8 also looking expand work languages . table10 confusionmatrixforjaccardsimilarity references 16news predicted : predicted : yes 1 . g . atkins , . weigle , andm . nelson , measuringnewssimilarity acrosstenu . . newssites , arxivpreprintarxiv , pp . 1 – 11 , 2018 total : 12 4 2 . j . gibson , b . wellner , . lubar , identiﬁcation duplicate actual : tn = 8 fp = 0 news stories web pages , mitre corporation 202 actual : yes fn = 4 tp = 4 burlington rd . bedford 01730 usa , 202 burlington rd . thresholdvalue : 0 . 245 bedfordma01730usa , 2008 3 . . singh , . . kumar , . v . goyal , review techniques totalreﬁnednews : 4 extraction bilingual lexicon comparable corpora . int . j . eng . technol . 7 ( 2 ), 16 – 20 ( 2018 ) 4 . . montalvo , r . mart ´ ınez , . casilla , bilingualnewsclustering using named entities fuzzy similarity ( springer , heidel - table11 confusionmatrixforeuclideansimilarity berg , 2007 ), pp . 108 – 114 5 . . mohd saad . . kamarudin , comparative analysis news predicted : predicted : yes similarity measuresforsentence levelsemanticmeasurement total : 8 8 text . ieee international conference control system , computingandengineering , pp . 90 – 94 , 2013 actual : tn = 8 fp = 7 6 . p . sitikhu , comparison semantic similarity methods actual : yes fn = 0 tp = 1 maximum human interpretability , 2019 . arxiv : 1910 . 09129v2 thresholdvalue : 0 . 0529 [ cs . ir ] 7 . k . baraniak . sydow , news articles similarity auto - totalreﬁnednews : 8 maticmediabiasdetectioninpolishnewsportals , inproceedings federated conference computer science infor - mationsystems , 2018 8 . . b . magara . zuva , comparative analysis text similarity measures algorithms research paper recom - mendersystems , inconferenceoninformationcommunications table12 accuracylevelofeachsimilaritymeasures technologyandsociety ( ictas ), 2018 similaritymeasures performancemeasures 9 . v . thada , . v . jaglan , comparison jaccard , dice , cosine similarity coefﬁcient ﬁnd best ﬁtness value web retrieved accuracy precision recall f - measure documents using genetic algorithm . int . j . innov . eng . technol . ( ijiet ) 2 ( 4 ), 202 – 204 ( 2013 ) cosine 0 . 8125 62 . 5 1 . 0 0 . 7692 10 . . . nasab , anewapproachforﬁndingsemanticsimilarscien - jaccard 0 . 750 1 . 0 0 . 50 0 . 666 tiﬁc articles . j . adv . comput . sci . technol . ( jacst ) 4 , 563 - 59 euclidean 0 . 5625 0 . 125 1 . 0 0 . 222 ( 2015 ) 11 . . snover , b . dorr , andr . schwartz , languageandtranslation thehighestvalueisshowninbold model adaptation using comparable corpora , proceedings 123 j . inst . eng . indiaser . b the2008conferenceonempiricalmethodsinnaturallanguage https :// dataaspirant . com / 2015 / 04 / 11 / ﬁve - - popular - processing , honolulu , 2008 similarity - measures - implementation - - python / 12 . q . longhuaandw . hongling , bilinguallexiconconstruction 17 . . goswami , . babu , b . purkayastha , comparative analysis fromcomparable corpora viadependencymapping , inproceed - similarity measures . int . j . manag . technol . eng . 8 ( xi ), ingsofcoling , 2012 786 – 797 ( 2018 ) 13 . . ydanideahl @ danideahl , googlenewsisgettinganoverhaul 18 . . ali , textualsimilarity , issn2011 - 19 , 2011 andcustomizednewsfeeds , theverge , 8may2018 .[ online ]. 19 . textblob : simpliﬁed text processing , [ online ]. available : available : https :// textblob . readthedocs . io / en / dev / https :// www . theverge . com / 2018 / 5 / 8 / 17329074 / google - news - 20 . bag words euclidean distance , [ online ]. available : update - new - features - newsstand - io - 2018 https :// pythonprogramminglanguage . com / bag - - words - 14 . h . hu , ‘‘ googlenews . pypi ,’’ pypi . org , mar 13 , 2020 . [ online ]. euclidean - distance / available : https :// pypi . org / project / googlenews / 21 . . . kamaruddi , graph - based representation sentence simi - 15 . j . brownlee , howtocleantextformachinelearningwithpython , larity measure : comparative analysis . int . j . eng . technol . machine learning mastery , october 18 , 2017 . [ online ]. avail - 7 ( 2 . 4 ), 32 – 35 ( 2018 ) able : https :// machinelearningmastery . com / clean - text - machine - publisher ’ note springer nature remains neutral regard learning - python / jurisdictionalclaimsinpublishedmapsandinstitutionalafﬁliations . 16 . . polamuri , five popular similarity measures implemen - 123']\n",
      "Duration: 0:00:00.091793\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = datetime.now()\n",
    "norm_corpus = u.preprocess_corpus(corpus)\n",
    "print(norm_corpus)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de27de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a6b02070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3727 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4272 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fd332ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3053 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3534 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus2)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "520697f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 2, ..., 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dense representation \n",
    "# warning might give a memory error if data is too big\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9e9b7982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aandbarebagsofwordsthatcontainnews</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbc</th>\n",
       "      <th>abdelrahman</th>\n",
       "      <th>abhibhav</th>\n",
       "      <th>able</th>\n",
       "      <th>abs</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstractdeep</th>\n",
       "      <th>ac</th>\n",
       "      <th>...</th>\n",
       "      <th>yuchi</th>\n",
       "      <th>yun</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zhao</th>\n",
       "      <th>zhi</th>\n",
       "      <th>zhihuacui</th>\n",
       "      <th>zhishuai</th>\n",
       "      <th>zhou</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3053 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aandbarebagsofwordsthatcontainnews  ab  abbc  abdelrahman  abhibhav  able  \\\n",
       "0                                   0   2     0            1         1     1   \n",
       "1                                   0   0     0            0         0     0   \n",
       "2                                   1   0     2            0         0     1   \n",
       "\n",
       "   abs  abstract  abstractdeep  ac  ...  yuchi  yun  zhang  zhao  zhi  \\\n",
       "0    5         0             1   3  ...      1    0      3     2    2   \n",
       "1    0         1             0   0  ...      0    1      0     0    0   \n",
       "2    0         4             0   3  ...      0    0      0     0    0   \n",
       "\n",
       "   zhihuacui  zhishuai  zhou  zone  zuva  \n",
       "0          1         1     1     0     0  \n",
       "1          0         0     0     1     0  \n",
       "2          0         0     0     0     1  \n",
       "\n",
       "[3 rows x 3053 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "08e40b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24, 25,  3, ...,  7,  2,  0],\n",
       "       [ 2,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  1,  1,  1]], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a7cb806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_matrix[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4c3954d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xrange'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-a3d7c09142c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xrange'"
     ]
    }
   ],
   "source": [
    "import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c40d415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_OUT_columns = [0, 2]\n",
    "idx_IN_columns = [i for i in range(np.shape(cv_matrix)[1]) if i not in idx_OUT_columns]\n",
    "extractedData = cv_matrix[:,idx_IN_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b0f42aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  0,  1, ...,  7,  2,  0],\n",
       "       [ 0,  1,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  1,  1,  1]], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> A\n",
    "array([[1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]])\n",
    "\n",
    ">>> A[:,2] # returns the third columm\n",
    "array([3, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1bea0215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsNumber(value):\n",
    "    if True in [char.isdigit() for char in value]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8d5af592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containsNumber(\"hola3 a\")\n",
    "containsNumber(\"holaq a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "061ea5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 16,\n",
       " 18,\n",
       " 23,\n",
       " 35,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 70,\n",
       " 77,\n",
       " 80,\n",
       " 86,\n",
       " 87,\n",
       " 90,\n",
       " 98,\n",
       " 103,\n",
       " 105,\n",
       " 107,\n",
       " 112,\n",
       " 113,\n",
       " 115,\n",
       " 116,\n",
       " 118,\n",
       " 122,\n",
       " 127,\n",
       " 128,\n",
       " 133,\n",
       " 135,\n",
       " 136,\n",
       " 139,\n",
       " 143,\n",
       " 144,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 154,\n",
       " 156,\n",
       " 164,\n",
       " 168,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 182,\n",
       " 189,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 203,\n",
       " 206,\n",
       " 208,\n",
       " 213,\n",
       " 216,\n",
       " 219,\n",
       " 220,\n",
       " 225,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 234,\n",
       " 239,\n",
       " 242,\n",
       " 245,\n",
       " 246,\n",
       " 252,\n",
       " 254,\n",
       " 255,\n",
       " 267,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 291,\n",
       " 295,\n",
       " 298,\n",
       " 301,\n",
       " 306,\n",
       " 309,\n",
       " 313,\n",
       " 318,\n",
       " 325,\n",
       " 326,\n",
       " 336,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 366,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 377,\n",
       " 393,\n",
       " 395,\n",
       " 397,\n",
       " 398,\n",
       " 401,\n",
       " 419,\n",
       " 435,\n",
       " 451,\n",
       " 471,\n",
       " 472,\n",
       " 501,\n",
       " 508,\n",
       " 511,\n",
       " 517,\n",
       " 522,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 529,\n",
       " 535,\n",
       " 540,\n",
       " 545,\n",
       " 547,\n",
       " 553,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 578,\n",
       " 610,\n",
       " 612,\n",
       " 616,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 627,\n",
       " 633,\n",
       " 664,\n",
       " 668,\n",
       " 669,\n",
       " 691,\n",
       " 702,\n",
       " 706,\n",
       " 722,\n",
       " 736,\n",
       " 738,\n",
       " 742,\n",
       " 743,\n",
       " 746,\n",
       " 756,\n",
       " 763,\n",
       " 766,\n",
       " 768,\n",
       " 769,\n",
       " 787,\n",
       " 807,\n",
       " 812,\n",
       " 814,\n",
       " 815,\n",
       " 822,\n",
       " 824,\n",
       " 825,\n",
       " 829,\n",
       " 830,\n",
       " 843,\n",
       " 852,\n",
       " 857,\n",
       " 858,\n",
       " 862,\n",
       " 863,\n",
       " 880,\n",
       " 882,\n",
       " 891,\n",
       " 898,\n",
       " 905,\n",
       " 910,\n",
       " 924,\n",
       " 928,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 948,\n",
       " 950,\n",
       " 952,\n",
       " 954,\n",
       " 955,\n",
       " 963,\n",
       " 964,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 1008,\n",
       " 1015,\n",
       " 1020,\n",
       " 1028,\n",
       " 1034,\n",
       " 1035,\n",
       " 1036,\n",
       " 1037,\n",
       " 1064,\n",
       " 1083,\n",
       " 1091,\n",
       " 1095,\n",
       " 1096,\n",
       " 1099,\n",
       " 1100,\n",
       " 1101,\n",
       " 1133,\n",
       " 1152,\n",
       " 1153,\n",
       " 1167,\n",
       " 1168,\n",
       " 1169,\n",
       " 1175,\n",
       " 1187,\n",
       " 1216,\n",
       " 1224,\n",
       " 1226,\n",
       " 1248,\n",
       " 1271,\n",
       " 1273,\n",
       " 1277,\n",
       " 1278,\n",
       " 1279,\n",
       " 1280,\n",
       " 1283,\n",
       " 1285,\n",
       " 1286,\n",
       " 1287,\n",
       " 1288,\n",
       " 1305,\n",
       " 1316,\n",
       " 1321,\n",
       " 1322,\n",
       " 1329,\n",
       " 1334,\n",
       " 1335,\n",
       " 1346,\n",
       " 1350,\n",
       " 1351,\n",
       " 1352,\n",
       " 1359,\n",
       " 1366,\n",
       " 1368,\n",
       " 1369,\n",
       " 1370,\n",
       " 1371,\n",
       " 1382,\n",
       " 1391,\n",
       " 1395,\n",
       " 1398,\n",
       " 1399,\n",
       " 1400,\n",
       " 1402,\n",
       " 1409,\n",
       " 1416,\n",
       " 1417,\n",
       " 1418,\n",
       " 1422,\n",
       " 1423,\n",
       " 1425,\n",
       " 1426,\n",
       " 1427,\n",
       " 1433,\n",
       " 1434,\n",
       " 1436,\n",
       " 1437,\n",
       " 1443,\n",
       " 1444,\n",
       " 1445,\n",
       " 1447,\n",
       " 1456,\n",
       " 1469,\n",
       " 1472,\n",
       " 1491,\n",
       " 1507,\n",
       " 1529,\n",
       " 1542,\n",
       " 1549,\n",
       " 1553,\n",
       " 1560,\n",
       " 1561,\n",
       " 1562,\n",
       " 1564,\n",
       " 1565,\n",
       " 1576,\n",
       " 1579,\n",
       " 1582,\n",
       " 1584,\n",
       " 1585,\n",
       " 1596,\n",
       " 1631,\n",
       " 1632,\n",
       " 1642,\n",
       " 1645,\n",
       " 1647,\n",
       " 1673,\n",
       " 1682,\n",
       " 1700,\n",
       " 1701,\n",
       " 1703,\n",
       " 1704,\n",
       " 1718,\n",
       " 1720,\n",
       " 1723,\n",
       " 1724,\n",
       " 1742,\n",
       " 1760,\n",
       " 1761,\n",
       " 1763,\n",
       " 1769,\n",
       " 1787,\n",
       " 1795,\n",
       " 1799,\n",
       " 1804,\n",
       " 1809,\n",
       " 1812,\n",
       " 1818,\n",
       " 1828,\n",
       " 1830,\n",
       " 1848,\n",
       " 1867,\n",
       " 1868,\n",
       " 1870,\n",
       " 1876,\n",
       " 1877,\n",
       " 1878,\n",
       " 1879,\n",
       " 1883,\n",
       " 1884,\n",
       " 1888,\n",
       " 1889,\n",
       " 1890,\n",
       " 1892,\n",
       " 1895,\n",
       " 1896,\n",
       " 1902,\n",
       " 1906,\n",
       " 1907,\n",
       " 1911,\n",
       " 1922,\n",
       " 1924,\n",
       " 1928,\n",
       " 1929,\n",
       " 1940,\n",
       " 1958,\n",
       " 1959,\n",
       " 1961,\n",
       " 1968,\n",
       " 1974,\n",
       " 1978,\n",
       " 1981,\n",
       " 1985,\n",
       " 1987,\n",
       " 1989,\n",
       " 1999,\n",
       " 2010,\n",
       " 2036,\n",
       " 2062,\n",
       " 2065,\n",
       " 2072,\n",
       " 2075,\n",
       " 2085,\n",
       " 2087,\n",
       " 2088,\n",
       " 2091,\n",
       " 2095,\n",
       " 2096,\n",
       " 2110,\n",
       " 2121,\n",
       " 2127,\n",
       " 2130,\n",
       " 2131,\n",
       " 2135,\n",
       " 2139,\n",
       " 2153,\n",
       " 2162,\n",
       " 2175,\n",
       " 2176,\n",
       " 2177,\n",
       " 2179,\n",
       " 2206,\n",
       " 2234,\n",
       " 2241,\n",
       " 2269,\n",
       " 2271,\n",
       " 2272,\n",
       " 2285,\n",
       " 2287,\n",
       " 2298,\n",
       " 2299,\n",
       " 2300,\n",
       " 2302,\n",
       " 2317,\n",
       " 2327,\n",
       " 2343,\n",
       " 2346,\n",
       " 2347,\n",
       " 2348,\n",
       " 2349,\n",
       " 2351,\n",
       " 2362,\n",
       " 2363,\n",
       " 2364,\n",
       " 2365,\n",
       " 2367,\n",
       " 2371,\n",
       " 2372,\n",
       " 2374,\n",
       " 2375,\n",
       " 2376,\n",
       " 2386,\n",
       " 2400,\n",
       " 2401,\n",
       " 2412,\n",
       " 2418,\n",
       " 2420,\n",
       " 2433,\n",
       " 2436,\n",
       " 2437,\n",
       " 2438,\n",
       " 2439,\n",
       " 2440,\n",
       " 2441,\n",
       " 2443,\n",
       " 2444,\n",
       " 2445,\n",
       " 2447,\n",
       " 2450,\n",
       " 2466,\n",
       " 2469,\n",
       " 2476,\n",
       " 2477,\n",
       " 2478,\n",
       " 2479,\n",
       " 2480,\n",
       " 2491,\n",
       " 2502,\n",
       " 2508,\n",
       " 2509,\n",
       " 2511,\n",
       " 2521,\n",
       " 2524,\n",
       " 2538,\n",
       " 2545,\n",
       " 2553,\n",
       " 2565,\n",
       " 2569,\n",
       " 2570,\n",
       " 2571,\n",
       " 2576,\n",
       " 2581,\n",
       " 2588,\n",
       " 2590,\n",
       " 2595,\n",
       " 2596,\n",
       " 2599,\n",
       " 2609,\n",
       " 2615,\n",
       " 2617,\n",
       " 2619,\n",
       " 2623,\n",
       " 2634,\n",
       " 2636,\n",
       " 2639,\n",
       " 2645,\n",
       " 2646,\n",
       " 2648,\n",
       " 2650,\n",
       " 2651,\n",
       " 2652,\n",
       " 2653,\n",
       " 2654,\n",
       " 2656,\n",
       " 2657,\n",
       " 2658,\n",
       " 2659,\n",
       " 2660,\n",
       " 2662,\n",
       " 2663,\n",
       " 2666,\n",
       " 2667,\n",
       " 2668,\n",
       " 2669,\n",
       " 2670,\n",
       " 2672,\n",
       " 2673,\n",
       " 2674,\n",
       " 2675,\n",
       " 2676,\n",
       " 2677,\n",
       " 2679,\n",
       " 2680,\n",
       " 2681,\n",
       " 2682,\n",
       " 2683,\n",
       " 2684,\n",
       " 2686,\n",
       " 2688,\n",
       " 2689,\n",
       " 2691,\n",
       " 2693,\n",
       " 2694,\n",
       " 2695,\n",
       " 2697,\n",
       " 2698,\n",
       " 2700,\n",
       " 2701,\n",
       " 2704,\n",
       " 2705,\n",
       " 2707,\n",
       " 2708,\n",
       " 2709,\n",
       " 2710,\n",
       " 2711,\n",
       " 2713,\n",
       " 2714,\n",
       " 2715,\n",
       " 2719,\n",
       " 2721,\n",
       " 2738,\n",
       " 2740,\n",
       " 2742,\n",
       " 2745,\n",
       " 2747,\n",
       " 2753,\n",
       " 2754,\n",
       " 2758,\n",
       " 2759,\n",
       " 2760,\n",
       " 2761,\n",
       " 2763,\n",
       " 2774,\n",
       " 2776,\n",
       " 2783,\n",
       " 2789,\n",
       " 2798,\n",
       " 2804,\n",
       " 2822,\n",
       " 2824,\n",
       " 2826,\n",
       " 2843,\n",
       " 2844,\n",
       " 2846,\n",
       " 2848,\n",
       " 2876,\n",
       " 2879,\n",
       " 2888,\n",
       " 2909,\n",
       " 2912,\n",
       " 2915,\n",
       " 2916,\n",
       " 2918,\n",
       " 2922,\n",
       " 2923,\n",
       " 2924,\n",
       " 2926,\n",
       " 2927,\n",
       " 2928,\n",
       " 2929,\n",
       " 2936,\n",
       " 2940,\n",
       " 2941,\n",
       " 2942,\n",
       " 2943,\n",
       " 2944,\n",
       " 2949,\n",
       " 2950,\n",
       " 2953,\n",
       " 2955,\n",
       " 2957,\n",
       " 2960,\n",
       " 2961,\n",
       " 2963,\n",
       " 2964,\n",
       " 2965,\n",
       " 2966,\n",
       " 2967,\n",
       " 2968,\n",
       " 2969,\n",
       " 2970,\n",
       " 2971,\n",
       " 2972,\n",
       " 2973,\n",
       " 2974,\n",
       " 2982,\n",
       " 2985,\n",
       " 2987,\n",
       " 2988,\n",
       " 2990,\n",
       " 2991,\n",
       " 2994,\n",
       " 2995,\n",
       " 3002]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result =[vocab.index(c) for c in vocab if (c[0].isnumeric()or containsNumber(c)or len(c)>14) ]\n",
    "print(len(result))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1d352e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_IN_columns = [i for i in range(np.shape(vocab)[0]) if i not in result]\n",
    "extractedWords =  [vocab[i] for i in idx_IN_columns]\n",
    "idx_IN_columns = [i for i in range(np.shape(cv_matrix)[1]) if i not in result]\n",
    "extractedData = cv_matrix[:,idx_IN_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098cc413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "371614b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a1'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "869f182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_IN_columns = [i for i in range(np.shape(cv_matrix)[1]) if i not in result]\n",
    "extractedData = cv_matrix[:,idx_IN_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "072ea378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3289)\n",
      "(3289,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>aandbarebagsofwordsthatcontainnews</th>\n",
       "      <th>ab11</th>\n",
       "      <th>ab22</th>\n",
       "      <th>ab2þbc2</th>\n",
       "      <th>abdel</th>\n",
       "      <th>abhibhav</th>\n",
       "      <th>able</th>\n",
       "      <th>abs</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬁndsthemetric</th>\n",
       "      <th>ﬁne</th>\n",
       "      <th>ﬁner</th>\n",
       "      <th>ﬁnest</th>\n",
       "      <th>ﬁnite</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬁtness</th>\n",
       "      <th>ﬁve</th>\n",
       "      <th>ﬂow</th>\n",
       "      <th>ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a1  a2  aandbarebagsofwordsthatcontainnews  ab11  ab22  ab2þbc2  abdel  \\\n",
       "0   2   2                                   0     1     1        0      1   \n",
       "1   0   0                                   0     0     0        0      0   \n",
       "2   0   0                                   1     0     0        1      0   \n",
       "\n",
       "   abhibhav  able  abs  ...  ﬁndsthemetric  ﬁne  ﬁner  ﬁnest  ﬁnite  ﬁrst  \\\n",
       "0         1     1    5  ...              0    3     2      1      1     6   \n",
       "1         0     0    0  ...              0    0     0      0      0     1   \n",
       "2         0     1    0  ...              1    0     0      0      0     1   \n",
       "\n",
       "   ﬁtness  ﬁve  ﬂow  ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ  \n",
       "0       0    7    2                       0  \n",
       "1       0    0    0                       0  \n",
       "2       2    1    1                       1  \n",
       "\n",
       "[3 rows x 3289 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(extractedData))\n",
    "print(np.shape(extractedWords))\n",
    "pd.DataFrame(extractedData, columns=extractedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7035d424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3727"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)\n",
    "\n",
    "## Eliminar columnas donde empieza con numeros \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ced08bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>0000000005</th>\n",
       "      <th>0000110088</th>\n",
       "      <th>00010</th>\n",
       "      <th>00015</th>\n",
       "      <th>00020</th>\n",
       "      <th>00025</th>\n",
       "      <th>0003</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬁndsthemetric</th>\n",
       "      <th>ﬁne</th>\n",
       "      <th>ﬁner</th>\n",
       "      <th>ﬁnest</th>\n",
       "      <th>ﬁnite</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬁtness</th>\n",
       "      <th>ﬁve</th>\n",
       "      <th>ﬂow</th>\n",
       "      <th>ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     00  000  0000  0000000005  0000110088  00010  00015  00020  00025  0003  \\\n",
       "0  0.08  0.1  0.01        0.00         0.0   0.00   0.00   0.00   0.00   0.0   \n",
       "1  0.02  0.0  0.00        0.01         0.0   0.01   0.01   0.01   0.01   0.0   \n",
       "2  0.00  0.0  0.00        0.00         0.0   0.00   0.00   0.00   0.00   0.0   \n",
       "\n",
       "   ...  ﬁndsthemetric   ﬁne  ﬁner  ﬁnest  ﬁnite  ﬁrst  ﬁtness   ﬁve   ﬂow  \\\n",
       "0  ...           0.00  0.01  0.01    0.0    0.0  0.01    0.00  0.02  0.01   \n",
       "1  ...           0.00  0.00  0.00    0.0    0.0  0.01    0.00  0.00  0.00   \n",
       "2  ...           0.01  0.00  0.00    0.0    0.0  0.00    0.01  0.00  0.00   \n",
       "\n",
       "   ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ  \n",
       "0                    0.00  \n",
       "1                    0.00  \n",
       "2                    0.01  \n",
       "\n",
       "[3 rows x 3727 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tt = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "tt_matrix = tt.fit_transform(cv_matrix)\n",
    "\n",
    "tt_matrix = tt_matrix.toarray()\n",
    "vocab = cv.get_feature_names()\n",
    "pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "80394821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aandbarebagsofwordsthatcontainnews</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbc</th>\n",
       "      <th>abdelrahman</th>\n",
       "      <th>abhibhav</th>\n",
       "      <th>able</th>\n",
       "      <th>abs</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstractdeep</th>\n",
       "      <th>ac</th>\n",
       "      <th>...</th>\n",
       "      <th>yuchi</th>\n",
       "      <th>yun</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zhao</th>\n",
       "      <th>zhi</th>\n",
       "      <th>zhihuacui</th>\n",
       "      <th>zhishuai</th>\n",
       "      <th>zhou</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3053 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aandbarebagsofwordsthatcontainnews    ab  abbc  abdelrahman  abhibhav  \\\n",
       "0                                0.00  0.01  0.00          0.0       0.0   \n",
       "1                                0.00  0.00  0.00          0.0       0.0   \n",
       "2                                0.01  0.00  0.01          0.0       0.0   \n",
       "\n",
       "   able   abs  abstract  abstractdeep    ac  ...  yuchi   yun  zhang  zhao  \\\n",
       "0   0.0  0.02      0.00           0.0  0.01  ...    0.0  0.00   0.01  0.01   \n",
       "1   0.0  0.00      0.01           0.0  0.00  ...    0.0  0.01   0.00  0.00   \n",
       "2   0.0  0.00      0.02           0.0  0.01  ...    0.0  0.00   0.00  0.00   \n",
       "\n",
       "    zhi  zhihuacui  zhishuai  zhou  zone  zuva  \n",
       "0  0.01        0.0       0.0   0.0  0.00  0.00  \n",
       "1  0.00        0.0       0.0   0.0  0.01  0.00  \n",
       "2  0.00        0.0       0.0   0.0  0.00  0.01  \n",
       "\n",
       "[3 rows x 3053 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2',\n",
    "                     use_idf=True, smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus2)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "920c7c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>0000000005</th>\n",
       "      <th>0000110088</th>\n",
       "      <th>00010</th>\n",
       "      <th>00015</th>\n",
       "      <th>00020</th>\n",
       "      <th>00025</th>\n",
       "      <th>0003</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬁndsthemetric</th>\n",
       "      <th>ﬁne</th>\n",
       "      <th>ﬁner</th>\n",
       "      <th>ﬁnest</th>\n",
       "      <th>ﬁnite</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬁtness</th>\n",
       "      <th>ﬁve</th>\n",
       "      <th>ﬂow</th>\n",
       "      <th>ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     00  000  0000  0000000005  0000110088  00010  00015  00020  00025  0003  \\\n",
       "0  0.08  0.1  0.01        0.00         0.0   0.00   0.00   0.00   0.00   0.0   \n",
       "1  0.02  0.0  0.00        0.01         0.0   0.01   0.01   0.01   0.01   0.0   \n",
       "2  0.00  0.0  0.00        0.00         0.0   0.00   0.00   0.00   0.00   0.0   \n",
       "\n",
       "   ...  ﬁndsthemetric   ﬁne  ﬁner  ﬁnest  ﬁnite  ﬁrst  ﬁtness   ﬁve   ﬂow  \\\n",
       "0  ...           0.00  0.01  0.01    0.0    0.0  0.01    0.00  0.02  0.01   \n",
       "1  ...           0.00  0.00  0.00    0.0    0.0  0.01    0.00  0.00  0.00   \n",
       "2  ...           0.01  0.00  0.00    0.0    0.0  0.00    0.01  0.00  0.00   \n",
       "\n",
       "   ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ  \n",
       "0                    0.00  \n",
       "1                    0.00  \n",
       "2                    0.01  \n",
       "\n",
       "[3 rows x 3727 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without number at beginning of the word\n",
    "# \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2',\n",
    "                     use_idf=True, smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a20bfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_IN_columns = [i for i in range(np.shape(vocab)[0]) if i not in result]\n",
    "extractedWords =  [vocab[i] for i in idx_IN_columns]\n",
    "idx_IN_columns = [i for i in range(np.shape(cv_matrix)[1]) if i not in result]\n",
    "extractedData = tv_matrix[:,idx_IN_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0cbd79c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abbc</th>\n",
       "      <th>abdelrahman</th>\n",
       "      <th>abhibhav</th>\n",
       "      <th>able</th>\n",
       "      <th>abs</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstractdeep</th>\n",
       "      <th>ac</th>\n",
       "      <th>accident</th>\n",
       "      <th>...</th>\n",
       "      <th>yuchi</th>\n",
       "      <th>yun</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zhao</th>\n",
       "      <th>zhi</th>\n",
       "      <th>zhihuacui</th>\n",
       "      <th>zhishuai</th>\n",
       "      <th>zhou</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2462 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ab  abbc  abdelrahman  abhibhav  able   abs  abstract  abstractdeep  \\\n",
       "0  0.01  0.00          0.0       0.0   0.0  0.02      0.00           0.0   \n",
       "1  0.00  0.00          0.0       0.0   0.0  0.00      0.01           0.0   \n",
       "2  0.00  0.01          0.0       0.0   0.0  0.00      0.02           0.0   \n",
       "\n",
       "     ac  accident  ...  yuchi   yun  zhang  zhao   zhi  zhihuacui  zhishuai  \\\n",
       "0  0.01       0.0  ...    0.0  0.00   0.01  0.01  0.01        0.0       0.0   \n",
       "1  0.00       0.0  ...    0.0  0.01   0.00  0.00  0.00        0.0       0.0   \n",
       "2  0.01       0.0  ...    0.0  0.00   0.00  0.00  0.00        0.0       0.0   \n",
       "\n",
       "   zhou  zone  zuva  \n",
       "0   0.0  0.00  0.00  \n",
       "1   0.0  0.01  0.00  \n",
       "2   0.0  0.00  0.01  \n",
       "\n",
       "[3 rows x 2462 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.round(extractedData, 2), columns=extractedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9b5d2b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2462)\n",
      "2462\n"
     ]
    }
   ],
   "source": [
    "print(extractedData.shape)\n",
    "print(len(extractedWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a5d56efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.064704</td>\n",
       "      <td>0.081611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.064704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.084636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081611</td>\n",
       "      <td>0.084636</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.064704  0.081611\n",
       "1  0.064704  1.000000  0.084636\n",
       "2  0.081611  0.084636  1.000000"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7a5c2048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058311</td>\n",
       "      <td>0.069289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069289</td>\n",
       "      <td>0.074676</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.058311  0.069289\n",
       "1  0.058311  1.000000  0.074676\n",
       "2  0.069289  0.074676  1.000000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "similarity_matrix = cosine_similarity(extractedData)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a5e64c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058475</td>\n",
       "      <td>0.069273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069273</td>\n",
       "      <td>0.074837</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.058475  0.069273\n",
       "1  0.058475  1.000000  0.074837\n",
       "2  0.069273  0.074837  1.000000"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "similarity_matrix = cosine_similarity(extractedData)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1d91fe57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058972</td>\n",
       "      <td>0.069769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069769</td>\n",
       "      <td>0.075574</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.058972  0.069769\n",
       "1  0.058972  1.000000  0.075574\n",
       "2  0.069769  0.075574  1.000000"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "similarity_matrix = cosine_similarity(extractedData)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core', tag=True, entity=True)\n",
    "#text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27913aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "  \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c16462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lem(text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "14d87fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b2ecb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lem(text):\n",
    "    # Tokenize: Split the sentence into words\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    #print(word_list)\n",
    "    #> ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
    "\n",
    "    # Lemmatize list of words and join\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    #print(lemmatized_output)\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "67a619e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The striped bat are hanging on their foot for best'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem( \"The striped bats are hanging on their feet for best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a01716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "#> ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "caafa5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=reload(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81220307",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e967323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = u.get_text_from_pdf('../references/ref1.pdf')\n",
    "text2 = u.get_text_from_pdf('../references/ref2.pdf')\n",
    "text3 = u.get_text_from_pdf('../references/ref11.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9d7520c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = u.corpus_to_df([text1,text2,text3])\n",
    "corpus = np.array([text1,text2,text3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7e1bd128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)\\n2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)\\nGuiding Deep Learning System Testing Using\\nSurprise Adequacy\\nJinhan Kim∗, Robert Feldt†‡, Shin Yoo∗\\n∗ † ‡\\nSchool of Computing Dept. of Computer Science and Engineering Dept. of Software Engineering\\nKAIST Chalmers University Blekinge Inst. of Technology\\nDaejeon, Republic of Korea Gothenburg, Sweden Karlskrona, Sweden\\n{jinhankim,shin.yoo}@kaist.ac.kr robert.feldt@chalmers.se robert.feldt@bth.se\\nAbstract—Deep Learning (DL) systems are rapidly being anothervehicletoyieldinoneoftherarercircumstances,and\\nadopted in safety and security critical domains, urgently calling crashed into the other vehicle when the expectation proved\\nfor ways to test their correctness and robustness. Testing of\\nincorrect [3]. There is an urgent need to verify and validate\\nDL systems has traditionally relied on manual collection and\\nbehaviours of DL systems. However, a signiﬁcant part of\\nlabelling of data. Recently, a number of coverage criteria based\\non neuron activation values have been proposed. These criteria existing software testing technique is not directly applicable\\nessentiallycountthenumberofneuronswhoseactivationduring to DL systems. Most notably, traditional white-box testing\\nthe execution of a DL system satisﬁed certain properties, such techniques that aim to increase structural coverage [4] is not\\nas being above predeﬁned thresholds. However, existing cover-\\nveryusefulforDLsystems,astheirbehaviourisnotexplicitly\\nage criteria are not sufﬁciently ﬁne grained to capture subtle\\nencoded in their control ﬂow structures.\\nbehavioursexhibitedbyDLsystems.Moreover,evaluationshave\\nfocused on showing correlation between adversarial examples A number of novel approaches towards testing and veri-\\nand proposed criteria rather than evaluating and guiding their ﬁcation of DL systems have been recently proposed to ﬁll\\nuse for actual testing of DL systems. We propose a novel test in the gap [19], [27], [34], [40]. Most of these techniques\\nadequacy criterion for testing of DL systems, called Surprise\\nshare two assumptions. The ﬁrst assumption is essentially a\\nAdequacy for Deep Learning Systems (SADL), which is based\\ngeneralisation of the essence of metamorphic testing [11]: if\\non the behaviour of DL systems with respect to their training\\ndata. We measure the surprise of an input as the difference two inputs to a DL system are similar with respect to some\\nin DL system’s behaviour between the input and the training humansense,theoutputsshouldalsobesimilar.Forexample,\\ndata (i.e., what was learnt during training), and subsequently DeepTest [40] checks whether an autonomous driving system\\ndevelopthisasanadequacycriterion:agoodtestinputshouldbe\\nbehavesinthesamewaywhentheinputimageistransformed\\nsufﬁcientlybutnotovertlysurprisingcomparedtotrainingdata.\\nasifthesamesceneisunderadifferentweathercondition.The\\nEmpirical evaluation using a range of DL systems from simple\\nimage classiﬁers to autonomous driving car platforms shows second assumption, also based in more traditional software\\nthat systematic sampling of inputs based on their surprise can testing results [15], is that the more diverse a set of input is,\\nimproveclassiﬁcationaccuracyofDLsystemsagainstadversarial the more effective testing of a DL system one can perform.\\nexamples by up to 77.5% via retraining.\\nForexample,DeepXplore[34]presentedtheNeuronCoverage\\nIndex Terms—Test Adequacy, Deep Learning Systems\\n(the ratio of neurons whose activation values were above a\\npredeﬁned threshold) as the measure of diversity of neuron\\nI. INTRODUCTION\\nbehaviour, and subsequently showed that inputs violating the\\nDeepLearning(DL)[24]systemshaveachievedsigniﬁcant ﬁrst assumption will also increase the neuron coverage.\\nprogress in many domains including image recognition [13], While the recently introduced techniques have made sig-\\n[22], [38], speech recognition [17], and machine transla- niﬁcant advances over manual ad hoc testing of DL systems,\\ntion [20], [37]. Based on their capability to match or even there is a major limitation. The coverage criteria proposed so\\nsurpass human performance, DL systems are increasingly fararenotsufﬁcientlyﬁnegrained,inasensethatallofthem\\nbeing adopted as part of larger systems in both safety and simply count neurons whose activation values satisfy certain\\nsecuritycriticaldomainssuchasautonomousdriving[6],[10], conditions. While this aggregation by counting does allow\\nand malware detection [12]. the tester to quantify the test effectiveness of a given input\\nSuch adoption of DL systems calls for new challenges, as set,itconveyslittleinformationaboutindividualinputs.For\\nit is critically important that these larger systems are both example,itisnotimmediatelyclearwhenaninputwithhigher\\ncorrectandpredictable. Despitetheirimpressive experimental NC should be considered better than another with lower NC,\\nperformances, DL systems are known to exhibit unexpected and why: certain inputs may naturally activate more neurons\\nbehaviours under certain circumstances. For example, in a above the threshold than others, and vice versa. Another\\nreported incident, an autonomous driving vehicle expected example is the k-Multisection Neuron Coverage [27], which\\n11555588--11222255//1199//$$3311..0000  ©©22001199  IIEEEEEE 11003399\\nDDOOII  1100..11110099//IICCSSEE..22001199..0000110088\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\npartitionstherangesofactivationvaluesofneurons,observed ing additional adversarial examples, sampling additional\\nduring training, into k buckets, and count the number of total inputs with broader SA values can improve the accuracy\\nbucketscoveredbyasetofinputs.Whenmeasuredforasingle after retraining by up to 77.5%.\\n1\\ninput,thecoveragewillbeeither iftheinputactivateseach • Weundertakeallourexperimentsusingpubliclyavailable\\nk\\nneuronwithavaluefromoneofthekbuckets,orsmallerthan DLsystemsrangingfromsmallbenchmarks(MNISTand\\nthatifsomeneuronsactivateoutsidetherangeobservedduring CIFAR-10) to a large system for autonomous driving\\ntraining.Again,theinformationabouthowfarsuchactivations vehicles (Dave-2 [6] and Chauffeur [1]). All implemen-\\ngo beyond observed range is lost during aggregation, making tations are available online.2\\nit hard to evaluate the relative value of each input. For a test The remaining of this paper is organised as follows. Sec-\\nadequacy criterion to be practically useful, it should be able tion II introduces Surprise Adequacy for DL systems, SADL:\\ntoguidetheselectionofindividualinputs,eventuallyresulting two variants of SADL are presented along with algorithms\\nin improvements of the accuracy of the DL system under that measure them. Section III sets out the research questions\\ninvestigation. and Section IV describes the experimental set-up of the\\nTo overcome these limitations, we propose a new test empirical evaluations. Section V presents the results from\\nadequacy for DL systems, called Surprise Adequacy for DL empiricalevaluations.SectionVIaddressesthreatstovalidity.\\nsystems (SADL). Intuitively, a good test input set for a SectionVIIpresentsrelatedwork,andSectionVIIIconcludes.\\nDL system should be systematically diversiﬁed to include\\ninputs ranging from those similar to training data to those II. SURPRISEADEQUACYFORDEEPLEARNINGSYSTEMS\\nsigniﬁcantly different and adversarial.1 At individual input All existing test adequacy criteria for DL systems aim\\ngranularity, SADL measures how surprising the input is to to measure the diversity of an input set. Neuron Coverage\\na DL system with respect to the data the system was trained (NC) [34] posits that the higher the number of neurons that\\nwith:theactualmeasureofsurprisecanbeeitherbasedonthe are activated above a predeﬁned threshold, the more diverse\\nlikelihood of the system having seen a similar input during inputtheDLsystemhasbeenexecutedwith.DeepGauge[27]\\ntraining (here with respect to probability density distributions proposed a range of ﬁner grained adequacy criteria including\\nextrapolated from the training process using kernel density k-Multisection Neuron Coverage, which measures the ratio\\nestimation [41]), or the distance between vectors representing of activation value buckets that have been covered across all\\ntheneuronactivationtracesofthegiveninputandthetraining neurons,andNeuronBoundaryCoverage,whichmeasuresthe\\ndata(heresimplyusingEuclideandistance).Subsequently,the ratioofneuronsthatareactivatedbeyondtherangesobserved\\nSurpriseAdequacy(SA)ofasetoftestinputsismeasuredby during training.\\ntherangeofindividualsurprisevaluesthesetcovers.Weshow We argue that diversity in testing of DL systems is more\\nthat SADL is sufﬁciently ﬁne grained by training adversarial meaningful when it is measured with respect to the training\\nexample classiﬁers based on SADL values that can produce data,asDLsystemsarelikelytobemoreerrorproneforinputs\\nhigher accuracy compared to the state of the art. We also that are unfamiliar, i.e., diverse. Furthermore, while neuron\\nshow that sampling inputs according to SADL for retraining activation above thresholds, or beyond observed ranges, may\\nDL systems can result in higher accuracy, thus showing that be closely related to diversity of the given input, they do not\\nSADLisanindependentvariablethatcanpositivelyaffectthe measure to what degree the activations of the network for\\neffectiveness of DL system testing. one input differs from the activations for another input. They\\nThe technical contributions of this paper are as follows: are fundamentally discretisations and do not utilize the fact\\n• We propose SADL, a ﬁne grained test adequacy metric that neuron activations are continuous quantities. In contrast,\\nthatmeasuresthesurpriseofaninput,i.e.,thedifference our aim is to deﬁne an adequacy criterion that quantitatively\\nin the behaviour of a DL system between a given input measures behavioural differences observed in a given set of\\nand the training data. Two concrete instances of SADL inputs, relative to the training data.\\nareproposedbasedondifferentwaystoquantifysurprise.\\nA. Activation Trace and Surprise Adequacy\\nBoth are shown to be correlated with existing coverage\\ncriteria for DL systems. Let N = {n1,n2,...} be a set of neurons that constitutes\\n• We show that SADL is sufﬁciently ﬁne grained in cap- a DL system D, and let X ={x1,x2,...} be a set of inputs.\\nturing the behaviour of DL systems by training a highly We denote the activation value of a single neuron n with\\naccurate adversarial example classiﬁer. Our adversarial respect to an input x as αn(x). For an ordered (sub)set of\\nexample classiﬁer shows as much as 100% and 94.53% neurons, let N ⊆ N, αN(x) denote a vector of activation\\nROC-AUC score when applied to MNIST [25] and values,eachelementcorrespondingtoanindividualneuronin\\nCIFAR-10 [21] dataset, respectively. N: the cardinality of αN(x) is equal to |N|. We call αN(x)\\n• We show that SADL metrics can be used to sample the Activation Trace (AT) of x over neurons in N. Similarly,\\neffective test input sets. When retraining DL systems us- letAN(X)beasetofactivationtraces,observedoverneurons\\ninN,forasetofinputsX:AN(X)={αN(x)|x∈X}.We\\n1Experimentsshowbeneﬁtsofdiversityforgeneraltesting[15]andbeneﬁts\\nofa‘scaleofdistances’oftestinputsforrobustnesstestingintroducedin[35]. 2Pleaserefertohttps://github.com/coinse/sadl.\\n11004400\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\nnote that the activation trace is trivially available after each Since we want to measure the surprise of the input x, we\\nexecution of the network for a given input. needametricthatincreaseswhenprobabilitydensitydecreases\\nSince behaviours of DL systems are driven along the data- (i.e., the input is rarer compared to the training data), and\\nﬂow and not control-ﬂow, we assume that activation traces vice versa (i.e., the input is similar to the training data).\\nobservedoverallNwithrespecttoX,AN(X),fullycaptures Adopting common approach of converting probability density\\nthe behaviours of the DL system under investigation when to a measure of rareness [26], [39], we deﬁne LSA to be the\\nexecuted using X.3 negative of the log of density:\\nSurprise Adequacy (SA) aims to measure the relative nov-\\nelty (i.e., surprise) of a given new input with respect to the LSA(x)=−log(fˆ(x)) (2)\\ninputs used for training. Given a training set T, we ﬁrst\\ncomputeAN(T)byrecordingactivationvaluesofallneurons Note that extra information about input types can be used\\ntomakeLSAmoreprecise.Forexample,givenaDLclassiﬁer\\nusingeveryinputinthetrainingdataset.Subsequently,givena\\nD, we expect inputs that share the same class label will have\\nnewinputx,wemeasurehowsurprisingxiswhencompared\\nto T by comparing the activation trace of x to AN(T). This sreimplialcairnAgTTs.Wwiethca{nxex∈plToit|thDis(xb)y=comc}puftoinrgclLaSssAcp.eWrcelaussse,\\nquantitative similarity measure is called Surprise Adequacy\\nper-class LSA for DL classiﬁers in our empirical evaluation.\\n(SA). We introduce two variants of SA, each with different\\nway of measuring the similarity between x and AN(T).4\\nNote that certain types of DL tasks allow us to focus on x1b x2b\\npartsofthetrainingsetTtogetmorepreciseandmeaningful\\nmeasurement of SA. For example, suppose we are testing a C2 Boundary \\nclassiﬁer with a new input x, which is classiﬁed by the DL x1 b1 b2 Learnt by DL\\nsystem under investigation as the class c. In this case, the C1 a1 x2\\nsurpriseofxismoremeaningfullymeasuredagainstAN(Tc), x1a a2\\nin which Tc is the subset of T where members are classiﬁed x2a\\nas c. Basically, the input might be surprising as an example Fig.1:AnexampleofDistance-basedSA.Blackdotsrepresent\\nof class c even if not surprising in relation to the full set of ATs of training data inputs, whereas grey dots represent ATs\\ntraining examples. ofnewinputs,x1andx2.Comparedtodistancesfromx1aand\\nB. Likelihood-based Surprise Adequacy x2a toclassc2,ATofx1 isfartheroutfromclassc1 thanthat\\nKernel Density Estimation (KDE) [41] is a way of esti- ofx2,i.e., ab11 > ab22 (seeEquations3,4,and5).Consequently,\\nwe decide that x1 is more surprising than x2 w.r.t. class c1.\\nmating the probability density function of a given random\\nvariable. The resulting density function allows the estimation\\nofrelativelikelihoodofaspeciﬁcvalueoftherandomvariable. C. Distance-based Surprise Adequacy\\nLikelihood-based SA (LSA) uses KDE to estimate the proba- AnalternativetoLSAissimplytousethedistancebetween\\nbility density of each activation value in AN(T), and obtains ATs as the measure of surprise. Here, we deﬁne Distance-\\nthe surprise of a new input with respect to the estimated basedSurprise Adequacy (DSA) usingtheEuclideandistance\\ndensity. This is an extension of existing work that uses KDE between the AT of a new input x and ATs observed during\\nto detect adversarial examples [14]. To reduce dimensionality training. Being a distance metric, DSA is ideally suited to\\nand computational cost, we only consider the neurons in a exploit the boundaries between inputs, as can be seen in the\\nselected layer NL ⊆ N, which yields a set of activation classiﬁcationexampleinFigure1.Bycomparingthedistances\\ntraces, ANL(X). To further reduce the computational cost, a1 and a2 (i.e., distance between the AT of a new input and\\nwe ﬁlter out neurons whose activation values show variance thereferencepoint,whichisthenearestAToftrainingdatain\\nlowerthanapre-deﬁnedthreshold,t,astheseneuronswillnot c1) to distances b1 and b2 (i.e., distance to c2 measured from\\ncontribute much information to KDE. The cardinality of each the reference point), we get a sense of how close to the class\\ntracewillbe|NL|.GivenabandwidthmatrixH andGaussian boundary the new inputs are. We posit that, for classiﬁcation\\nkernelfunctionK,theactivationtraceofthenewinputx,and problems, inputs that are closer to class boundaries are more\\nxi ∈T, KDE produces density function fˆas follows: surprising and valuable in terms of test input diversity. On\\nthe other hand, for tasks without any boundaries between\\n(cid:2)\\n1 inputs, such as prediction of appropriate steering angle for\\nfˆ(x)= |ANL(T)|xi∈TKH(αNL(x)−αNL(xi)) (1) aWuittohnonmoocluasssdbriovuinngdacriaers,,DanSAATmoafyannoetwbienpeuatsibleyinagppfalircfarbolme.\\n3For the sake of simplicity, we assume that it is possible to get the that of another training input does not guarantee that the new\\ncompleteactivationtracesfromalltheneuronsinaDLsystem.Fornetwork input is surprising, as the AT may still be located in crowded\\narchitectures with loops, such as Recurrent Neural Nets (RNNs) [18], it is parts of the AT space. Consequently, we only apply DSA for\\npossibletounrolltheloopsuptoapredeﬁnedbound[40].\\nclassiﬁcation tasks, for which it can be more effective than\\n4However,themainideaisgeneralandother,speciﬁcvariantswouldresult\\nifusingothersimilarityfunctions. LSA (see Section V-A and V-B for more details).\\n11004411\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\nLet us assume that a DL system D, which consists of a surprising. However, an input with arbitrarily high SA value\\nset of neurons N, is trained for a classiﬁcation task with a may simply be irrelevant, or at least less interesting, to the\\nset of classes C, using a training dataset T. Given the set of problem domain (e.g., an image of a trafﬁc sign will be\\nactivationtracesAN(T),anewinputx,andapredictedclass irrelevant to the testing of animal photo classiﬁers). As such,\\nof the new input cx ∈C, we deﬁne the reference point xa to SC can only be measured with respect to pre-deﬁned upper\\nbe the closest neighbour of x that shares the same class. The bound,inthesamewaythetheoreticallyinﬁnitepathcoverage\\ndistance between x and xa follows from the deﬁnition: is bounded by a parameter [44]. Second, SC does not render\\nitselftoacombinatorialsetcoverproblem,whichthetestsuite\\nxa = argmin (cid:4)αN(x)−αN(xi)(cid:4), minimisation is often formulated into [43]. This is because a\\nD(xi)=cx (3) single input yields only a single SA value and cannot belong\\ndista =(cid:4)αN(x)−αN(xa)(cid:4) tomultipleSAbuckets.Thesenseofredundancywithrespect\\nto SC as a coverage criteria is weaker than that of structural\\nSubsequently,fromxa,weﬁndtheclosestneighbourofxa coverage, for which a single input can cover multiple targets.\\ninaclassotherthancx,xb,andthedistancedistb,asfollows: While we aim to show that SA can guide the better selection\\nof inputs, rigorous study of optimisation of test suites for DL\\nxb = argmin (cid:4)αN(xa)−αN(xi)(cid:4), systemsremainsafuturework.However,asweshowwithour\\nD(xi)∈C\\\\{cx} (4) empirical studies, SC can still guide test input selection.\\ndistb =(cid:4)αN(xa)−αN(xb)(cid:4)\\nIII. RESEARCHQUESTIONS\\nIntuitively, DSA aims to compare the distance from the AT\\nof a new input x to known ATs belonging to its own class, Our empirical evaluation is designed to answer the follow-\\ncx, to the known distance between ATs in class cx and ATs ing research questions.\\nin other classes in C\\\\{cx}. If the former is relatively larger\\nRQ1. Surprise: Is SADL capable of capturing the relative\\nthan the latter, x would be a surprising input for class cx to\\nsurprise of an input of a DL system?\\nthe classifying DL system D. While there are multiple ways\\nto formalise this we select a simple one and calculate DSA We provide answers to RQ1 from different angles. First,\\nas the ratio between dista and distb. Investigation of more wecomputetheSAofeachtestinputincludedintheoriginal\\ncomplicated formulations is left as future work. dataset, and see if a DL classiﬁer ﬁnds inputs with higher\\nsurprise more difﬁcult to correctly classify. We expect more\\nDSA(x)= dista (5) surprising input to be harder to correctly classify. Second, we\\ndistb evaluate whether it is possible to detect adversarial examples\\nD. Surprise Coverage based on SA values, as we expect adversarial examples to\\nbe more surprising as well as to cause different behaviours\\nGivenaset ofinputs,wecanalsomeasuretherangeofSA\\nof DL systems. Using different techniques, multiple sets of\\nvalues the set covers, called Surprise Coverage (SC). Since\\nadversarial examples are generated and compared by their\\nboth LSA and DSA are deﬁned in continuous spaces, we\\nSA values. Finally, we train adversarial example classiﬁers\\nuse bucketing to discretise the space of surprise and deﬁne\\nusing logistic regression on SA values. For each adversarial\\nbothLikelihood-basedSurpriseCoverage(LSC)andDistance-\\nattackstrategy,wegenerate10,000adversarialexamplesusing\\nbased Surprise Coverage (DSC). Given an upper bound of U,\\nand buckets B ={b1,b2,...,bn} that divide (0,U] into n SA 10,000 original test images provided by MNIST and CIFAR-\\n10. Using 1,000 original test images and 1,000 adversarial\\nsegments, SC for a set of inputs X is deﬁned as follows:\\nexamples,allchosenrandomly,wetrainthelogisticregression\\nclassiﬁers.Finally,weevaluatethetrainedclassiﬁersusingthe\\nSC(X)= |{bi |∃x∈X :SA(x)∈(U · i−n1,U · ni]}| (6) remaining 9,000 original test images and 9,000 adversarial\\nn examples.IfSAvaluescorrectlycapture thebehaviourofDL\\nA set of inputs with high SC is a diverse set of inputs systems, we expect the SA based classiﬁers to successfully\\nranging from similar to those seen during training (i.e., low detect adversarial examples. We use Area Under Curve of\\nSA) to very different from what was seen during training Receiver Operator Characteristics (ROC-AUC) for evaluation\\n(i.e., high SA). We argue that an input set for a DL system as it captures both true and false positive rates [8].\\nshould not only be diversiﬁed, but systematically diversiﬁed\\nRQ2. Layer Sensitivity: Does the selection of layers of\\nconsidering SA. Recent results also validate this notion by\\nneurons used for SA computation have any impact on how\\nshowingthatmoredistanttestinputsweremorelikelytolead\\naccurately SA reﬂects the behaviour of DL systems?\\nto exceptions but might not be as relevant for testing [35].\\nWhileweusethetermcoverandcoverage,theimplications Bengio et al. suggest that deeper layers represent higher\\nofSAbasedcoverageisdifferentfromthetraditionalstructural levelfeaturesoftheinput[5]:subsequentworkthatintroduced\\ncoverage.First,unlikemostofthestructuralcoveragecriteria, KDE based adversarial example detection technique [14]\\nthere is no ﬁnite set of targets to cover, as in statement or assumes the deepest (i.e., the last hidden) layer to contain\\nbranchcoverage:aninputcan,atleastintheory,bearbitrarily the most information helpful for detection. We evaluate this\\n11004422\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\nTABLE I: List of datasets and models used in the study.\\nDataset Description DNNModel #ofNeuron SyntheticInputs Performance\\nMNIST Handwritten digit images composed of AﬁvelayerConvNetwith 320 FGSM,BIM-A,BIM- 99.31% (Ac-\\n50,000imagesfortrainingand10,000im- max-pooling and dropout B,JSMA,C&W. curacy)\\nagesfortest. layers.\\nCIFAR-10 Object recognition dataset in ten different A 12 layer ConvNet with 2,208 FGSM,BIM-A,BIM- 82.27% (Ac-\\nclasses composed of 50,000 images for max-pooling and dropout B,JSMA,C&W. curacy)\\ntrainingand10,000imagesfortest. layers.\\nUdacity Self-drivingcardatasetthatcontains Dave-2 [6] architecture 1,560 DeepXplore’s test 0.09(MSE)\\nSelf-drivingCar cameraimagesfromthevehicle,composed fromNvidia. input generation via\\nChallenge of101,396imagesfortrainingand5,614 jointoptimization.\\nimagesfortest.Thegoalofthechallenge\\nChauffeur[1]architecture 1,940 DeepTest’s combined 0.10(MSE)\\nistopredictsteeringwheelangle.\\nwithCNNandLSTM. transformation.\\nassumption in the context of SA by calculating LSA and U as the upper bound used in RQ3 to compute the SC, we\\nDSA of all individual layers, and subsequently by comparing divide the range of SA [0,U] into four overlapping subsets:\\nadversarialexampleclassiﬁerstrainedonSAfromeachlayer. the ﬁrst subset including the low 25% SA values ([0,U]), the\\n4\\n2U\\nRQ3. Correlation: Is SC correlated to existing coverage second including the lower half ([0, 4 ]), the third including\\n3U\\ncriteria for DL systems? the lower 75% ([0, 4 ]), and ﬁnally the entire range, [0,U].\\nThesefoursubsetsareexpectedtorepresentincreasinglymore\\nIn addition to capturing input surprise, we want SC to be\\ndiversesetsofinputs.WesettherangeRtooneofthesefour,\\nconsistent with existing coverage criteria based on counting\\nrandomly sample 100 images from each R, and train existing\\naggregation.Ifnot,thereisariskthatSCisinfactmeasuring\\nmodels for ﬁve additional epochs. Finally, we measure each\\nsomething other than input diversity. For this, we check\\nmodel’s performance (accuracy for MNIST and CIFAR-10,\\nwhether SC is correlated with other criteria. We control the\\nMSE for Dave-2) against the entire adversarial and synthetic\\ninput diversity by cumulatively adding inputs generated by\\ninputs, respectively. We expect retraining with more diverse\\ndifferentmethod(i.e.,differentadversarialexamplegeneration\\nsubset will result in higher performance.\\ntechniques or input synthesis techniques), execute the studied\\nDL systems with these input, and compare the observed IV. EXPERIMENTALSETUP\\nchanges of various coverage criteria including SC and four\\nWe evaluate SADL on four different DL systems using (a)\\nexisting ones: DeepXplore’s Neuron Coverage (NC) [40] and\\nthe original test sets, (b) adversarial examples generated by\\nthree Neuron-level Coverages (NLCs) introduced by Deep-\\nﬁve attack strategies, and (c) synthetic inputs generated by\\nGauge [27]: k-Multisection Neuron Coverage (KMNC), Neu-\\nDeepXplore [34] and DeepTest [40]. This section describes\\nronBoundaryCoverage(NBC),andStrongNeuronActivation\\nthe studied DL systems and the input generation methods.\\nCoverage (SNAC).\\nFor MNIST and CIFAR-10, we start from the original test A. Datasets and DL Systems\\ndata provided by the dataset (10,000 images), and add 1,000 TableIliststhesubjectdatasetsandmodelsofDLsystems.\\nadversarial examples, generated by FGSM, BIM-A, BIM-B, MNIST [25] and CIFAR-10 [21] are widely used datasets\\nJSMA,andC&W,ateachstep.ForDave-2,westartfromthe for machine learning research, each of which is a collection\\noriginaltestdata(5,614images)andadd700syntheticimages of images in ten different classes. For MNIST, we adopt\\ngenerated by DeepXplore at each step. For Chauffeur, each the widely studied ﬁve layer Convolutional Neural Network\\nstepadds1,000syntheticimages(Set1toSet3),eachproduced (ConvNet)withmax-poolinganddropoutlayersandtrainitto\\nby applying random number of DeepTest transformations. achieve 99.31% accuracy on the provided test set. Similarly,\\nthe adopted model for CIFAR is a 12-layer ConvNet with\\nRQ4. Guidance: Can SA guide retraining of DL systems\\nmax-pooling and dropout layers, trained to achieve 82.27%\\nto improve their accuracy against adversarial examples and\\naccuracy on the provided test set.\\nsynthetic test inputs generated by DeepXplore?\\nFor evaluation of SADL for DL systems in safety criti-\\nTo evaluate whether SADL can guide additional training cal domains, we use the Udacity self-driving car challenge\\nof existing DL systems with the aim of improved accuracy dataset [2], which contains a collection of camera images\\nagainstadversarialexamples,weaskwhetherSAcanguidethe from the driving car. As its aim is to predict steering wheel\\nselectionofinputforadditionaltraining.Fromtheadversarial angle, the model accuracy is measured using Mean Squared\\nexamplesandsynthesisedinputsforthesemodels5,wechoose Error (MSE) between actual and predicted steering angles.\\nfour sets of 100 images from four different SA ranges. Given\\nWe use a pre-trained Dave-2 model [6], which is a public\\nartefactprovidedbyDeepXplore6,andapre-trainedChauffeur\\n5We could not resume training of Chauffeur model for additional ﬁve\\nepochs,whichiswhyitisabsentfromRQ4. 6DeepXploreisavailablefrom:https://github.com/peikexin9/deepxplore.\\n11004433\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\nmodel[1],madepubliclyavailablebytheUdacityself-driving on all neurons is computationally infeasible due to precision\\ncar challenge. Dave-2 consists of nine layers including ﬁve loss. For RQ2, we set the activation variance threshold for\\nconvolutional layers, and achieves 0.09 in MSE. Chauffeur layers activation 7 and activation 8 of CIFAR-10 to 10−4,\\nconsists of both a ConvNet and an LSTM sub-model, and which reduces the number of neurons used for the computa-\\nachieves 0.10 in MSE. tion of LSA and, consequently, the computational cost. For\\ncomputation of other coverage criteria in RQ3, we use the\\nB. Adversarial Examples and Synthetic Inputs\\nconﬁgurations in Table II. The threshold of NC is set to 0.5.\\nSADL is evaluated using both adversarial examples and ForNLCs,weallsetthenumberofsections(k)to1,000.For\\nsynthetic test inputs. Adversarial examples are crafted by ap- LSC and DSC, we manually choose the layer, the number\\nplying,totheoriginalinput,smallperturbationsimperceptible of buckets (n), and the upper bound (ub). For RQ4, the\\nto humans, until the DL system under investigation behaves layers chosen for MNIST and CIFAR-10 are activation 3 and\\nincorrectly[16].Werelyonadversarialattackstogeneratein- activation 5respectively.Weperform20runsofretrainingfor\\nputimagesforMNISTandCIFAR-10:thesegeneratedimages each subject and report the statistics.\\nare more likely to reveal robustness issues in the DL systems Allexperimentswereperformedonmachinesequippedwith\\nthan the test inputs provided by the original datasets. We use Inteli7-8700CPU,32GBRAM,runningUbuntu16.04.4LTS.\\nﬁve widely studied attack strategies to evaluate SADL: Fast MNIST and CIFAR-10 are implemented using Keras v.2.2.0.\\nGradient Sign Method (FGSM) [16], Basic Iterative Method\\n(BIM-A, BIM-B) [23], Jacobian-based Saliency Map Attack V. RESULT\\n(JSMA) [33], and Carlini&Wagner (C&W) [9]. Our imple- Duetothespacelimit,wecannotincludeallplotsandtables\\nmentation of these strategies is based on cleverhans [32] and make them available online: https://coinse.github.io/sadl.\\nand a framework of Ma et al. [30].\\nA. Input Surprise (RQ1)\\nFor Dave-2 and Chauffeur, we use the state-of-the-art\\nsynthetic input generation algorithms, DeepXplore [34] and Figure 2 shows how the classiﬁcation accuracy changes\\nDeepTest[40].Bothalgorithmsaredesignedtosynthesisenew when we classify sets of images of growing sizes from the\\ntest input from existing ones with the aim of detecting erro- test inputs included in the MNIST and CIFAR-10 dataset.\\nneous behaviours in autonomous driving vehicle. For Dave-2, The sets of images corresponding to the red dots (Ascending\\nwe use DeepXplore’s input generation via joint optimization SA) start with images with the lowest SA, and increasingly\\nalgorithm, whose aim is to generate inputs that lead multiple include images with higher SA in the ascending order of SA;\\nDLsystemstrainedindependently,butusingthesametraining the sets of images corresponding to the blue dots grow in\\ndata, to disagree with each other. Using Dave-2 and its two the opposite direction (i.e., from images with the highest SA\\nvariants, Dave-dropout and Dave-norminit, we collect syn- to lower SA). As a reference, the green dots show the mean\\ntheticinputsgeneratedbylightingeffect(Light),occlusionby accuracy of randomly growing sets across 20 repetitions. It\\nasingleblackrectangle(SingleOcc),andocclusionbymultiple is clear that including images with higher LSA values, i.e.,\\nblack rectangles (MultiOcc). For Chauffeur, we synthesise more surprising images, leads to lower accuracy. For visual\\nnew inputs by iteratively applying random transformations conﬁrmation on another dataset, we also chose sets of inputs\\nprovided by DeepTest to original input images: translation, synthesised for Chauffeur by DeepTest, from three distinct\\nscale, shear, rotation, contrast, brightness, and blur.7 levelsofLSAvalues:Figure3showsthatthehighertheLSA\\nvalues are, the harder it is to recognise images visually. Both\\nTABLE II: Conﬁgurations for RQ3. quantitatively and visually, the observed trend supports our\\nclaim that SADL captures input surprise: even for unseen\\nDNN NC NLCs LSC DSC\\nModel th k layer n ub n ub inputs, SA can measure how surprising the given input is,\\nMNIST 0.5 1,000 activation 3 1,000 2,000 1,000 2.0 whichisdirectlyrelatedtotheperformanceoftheDLsystem.\\nCIFAR-10 0.5 1,000 activation 3 1,000 100 1,000 2.0 Figure4showsplotsofsortedDSAvaluesof10,000adver-\\nDave-2 0.5 1,000 block1 conv2 1,000 150 N/A\\nChauffeur 0.5 1,000 convolution2d 11 1,000 5 N/A sarial examples, generated by each of the ﬁve techniques, as\\nwellastheoriginaltestinputs.Figure5containssimilarplots\\nbased on LSA values of 2,000 randomly selected adversarial\\nC. Conﬁgurations\\nexamples and the original test set, from different layers of\\nFor all research questions, the default activation variance MNIST and CIFAR-10. For both MNIST and CIFAR-10, the\\nthreshold for LSA is set to 10−5, and the bandwidth for test inputs provided with the datasets (represented in blue\\nKDE is set using Scott’s Rule [36]. The remaining of this colour) tend to be the least surprising, whereas the majority\\nSection details RQ speciﬁc conﬁgurations. For RQ1, we use of adversarial examples are clearly separated from the test\\ntheactivation 2layerforMNIST,andactivation 6forCIFAR- inputsbytheirhigherSAvalues.Thissupportsourclaimthat\\n10, when computing LSA values. Computation of LSA based SADL can capture the differences in DL system’s behaviours\\nfor adversarial examples.\\n7Atthetimeofourexperiments,thepubliclyavailableversionofDeepTest\\nFinally, Table III shows the ROC-AUC results of DSA-\\ndidnotinternallysupportrealisticimagetransformationssuchasfogandrain\\neffects. based classiﬁcation using all neurons in MNIST and CIFAR-\\n11004444\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\n(a)LowLSA\\n(a)SelectedtestinputsbasedonLSAinMNIST\\n(b)MediumLSA\\n(c)HighLSA\\n(b)SelectedtestinputsbasedonDSAinMNIST\\nFig. 3: Synthetic images for Chauffeur model generated by\\nDeepTest. Images with higher LSA values tend to be harder\\nto recognise and interpret visually.\\n(c)SelectedtestinputsbasedonLSAinCIFAR-10\\n(d)SelectedtestinputsbasedonDSAinCIFAR-10\\nFig. 2: Accuracy of test inputs in MNIST and CIFAR-10\\ndataset, selected from the input with the lowest SA, increas- Fig.4:SortedDSAvaluesofadversarialexamplesforMNIST\\ningly including inputs with higher SA, and vice versa (i.e., and CIFAR-10.\\nfromtheinputwiththehighestSAtoinputswithlowerSA).\\nSADL can capture the relative surprise of inputs. Inputs\\n10.8 TheresultsshowthatthegapinDSAvaluesobservedin with higher SA are harder to correctly classify; adversarial\\nFigure4canbeusedtoclassifyadversarialexampleswithhigh examples show higher SA values and can be classiﬁed based\\naccuracy. For the relatively simpler MNIST model, the DSA- on SA accordingly.\\nbased classiﬁer can detect adversarial examples with ROC-\\nAUC ranging from 96.97% to 99.38%. The DSA-based clas- B. Impact of Layer Selection (RQ2)\\nsiﬁcation for the more complicated CIFAR-10 model shows\\nTable IV shows the ROC-AUC of classiﬁcation of adver-\\nlowerROC-AUCvalues,butanswerstoRQ2suggestthatDSA\\nsarial examples, resulting in each row corresponding to a\\nfromspeciﬁclayerscanproducesigniﬁcantlyhigheraccuracy\\nclassiﬁer trained on LSA and DSA from a speciﬁc layer in\\n(see Section V-B).\\nMNIST, respectively. Rows are ordered by their depth, i.e.,\\nBasedonthreedifferentanalyses,theanswertoRQ1isthat\\nactivation 3isthedeepestandthelasthiddenlayerinMNIST.\\nThe highest ROC-AUC values for each attack strategy are\\n8LSA-based classiﬁcation is only possible for subsets of neurons due to\\ntypeset in bold. For MNIST, there is no clear evidence that\\nthecomputationalcostofKDE;henceweintroducetheresultsofLSA-based\\nclassiﬁcationwhenansweringtheimpactoflayerselectionforRQ2. the deepest layer is the most effective.\\n11004455\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\nFig. 5: Sorted LSA of randomly selected 2,000 adversarial examples for MNIST and CIFAR-10 from different layers\\nTABLE III: ROC-AUC of DSA-based classiﬁcation of adver-\\nthemostaccurateclassiﬁerforBIM-A.Moreimportantly,per-\\nsarial examples for MNIST and CIFAR-10\\nlayer DSA values produce much more accurate classiﬁcation\\nDataset FGSM BIM-A BIM-B JSMA C&W results than all neuron DSA values, as can be seen in the\\ncomparison between Table III and Table IV & V. Identical\\nMNIST 98.34% 99.38% 96.97% 97.10% 99.04%\\nCIFAR-10 76.81% 72.93% 71.66% 88.96% 92.84% models have been used to produce results in Tables above.\\nTABLEV:ROC-AUCresultsofSAperlayersonCIFAR-10.\\nTABLE IV: ROC-AUC results of SA per layers on MNIST.\\nSA Layer FGSM BIM-A BIM-B JSMA C&W\\nSA Layer FGSM BIM-A BIM-B JSMA C&W\\nactivation 1 72.91% 61.59% 63.30% 76.85% 74.01%\\nactivation 1 100.00% 99.94% 100.00% 98.17% 99.48% activation 2 89.59% 62.17% 73.20% 80.33% 75.98%\\nLSA activation 2 100.00% 99.46% 100.00% 94.42% 99.23% pool 1 93.31% 61.79% 78.89% 82.64% 73.48%\\npool 1 100.00% 99.73% 100.00% 99.08% 99.61% activation 3 86.75% 62.69% 76.93% 80.33% 79.02%\\nactivation 3 93.29% 81.70% 86.73% 94.45% 37.96%\\nactivation 4 83.31% 62.73% 86.15% 80.86% 80.42%\\nactivation 1 100.00% 99.85% 100.00% 97.79% 99.39% LSA pool 2 82.82% 61.16% 89.69% 80.61% 73.85%\\nactivation 2 100.00% 99.39% 99.99% 97.59% 99.69% activation 5 83.80% 60.64% 96.31% 79.56% 64.60%\\nDSA\\npool 1 100.00% 99.32% 99.99% 98.21% 99.69% activation 6 63.85% 51.90% 99.74% 66.99% 60.40%\\nactivation 3 98.45% 99.43% 97.40% 97.07% 99.10% pool 3 63.46% 51.86% 99.77% 67.62% 56.21%\\nactivation 7 67.96% 61.09% 92.18% 83.02% 76.85%\\nactivation 8 59.28% 52.66% 99.60% 73.26% 62.15%\\nactivation 1 65.00% 62.25% 61.57% 73.85% 79.09%\\nThe cases for which ROC-AUC is 100% can be explained activation 2 77.63% 64.73% 67.95% 78.16% 81.59%\\nby Figure 5: LSA values from activation 1 of MNIST, for pool 1 80.22% 64.89% 70.94% 78.96% 82.03%\\nactivation 3 83.25% 68.48% 73.49% 79.89% 84.16%\\nexample, show a clear separation between the original test\\nactivation 4 81.77% 68.94% 77.94% 80.55% 84.62%\\ninputs and FGSM, BIM-A, or BIM-B: by choosing an appro- DSA pool 2 82.51% 69.28% 81.43% 80.92% 84.81%\\nactivation 5 81.45% 70.29% 83.28% 82.15% 85.15%\\npriatethreshold,itispossibletocompletelyseparatetestinputs\\nactivation 6 71.71% 70.92% 71.15% 84.05% 85.42%\\nfrom adversarial examples. Similarly, the plot of LSA from pool 3 71.75% 70.35% 74.65% 83.57% 85.17%\\nactivation 3 of MNIST shows that C&W LSA line crossing activation 7 71.04% 71.44% 81.46% 89.94% 92.98%\\nactivation 8 70.35% 70.65% 90.47% 90.46% 94.53%\\nwith that of the original test data (i.e., C&W adversarial\\nexamples are less surprising than the original test data): this Based on these results, we answer RQ2 that DSA is\\nresults in the low ROC-AUC value of 37.96%. sensitive to the selection of layers it is computed from,\\nTable V contains the ROC-AUC values of LSA- and DSA- and beneﬁts from choosing the deeper layer. However,\\nbasedclassiﬁers,trainedoneachlayeroftheCIFAR-10model: for LSA, there is no clear evidence supporting the deeper\\nfor each attack strategy, the highest ROC-AUC values are layer assumption. The layer sensitivity varies across different\\ntypeset in bold. Interestingly, LSA and DSA show different adversarial example generation strategies.\\ntrendswithCIFAR-10.WithLSA,thereisnostrongevidence\\nC. Correlation between SC and Other Criteria (RQ3)\\nthat the deepest layer produces the most accurate classiﬁers.\\nHowever, with DSA, the deepest layer produces the most Table VI shows how different coverage criteria respond to\\naccurateclassiﬁersforthreeoutofﬁveattackstrategies(BIM- increasing diversity levels9. Columns represent steps, at each\\nB,JSMA,andC&W),whiletheseconddeepestlayerproduces\\n9Seehttps://coinse.github.io/sadlforplots.\\n11004466\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\nof which more inputs are added to the original test set. If the the highest accuracy for MNIST and CIFAR-10, the lowest\\nincreaseincoverageatastepislessthan0.1percentagepoint MSE for Dave-2). The best performance is typeset in bold.\\n4\\nwhen compared to the previous step, the value is underlined. Thefullrange, ,producesthebestretrainingperformance\\n4\\n2 3\\nThe threshold of 0.1 percentage point is based on the ﬁnest for 13 conﬁgurations, followed by (5 conﬁgurations), (3\\n4 4\\n1\\nstep change possible for LSC, DSC, as well as KMNC, as all conﬁgurations), and (3 conﬁgurations). Note that for the\\n4\\n2\\nthreeusebucketingwithk=1,000.Weacknowledgethatthe conﬁguration of CIFAR-10 and BIM-B, both ranges and\\n4\\n3\\nthreshold is arbitrary, and provide it only as a supporting aid. produces the same and the best retraining performance.\\n4\\nNotethatDSCcannotbecomputedforthesetwoDLsystems, The largest improvement is observed when retraining MNIST\\n4\\nas they are not classiﬁers (see Section II-C). againstFGSMusingDSA:theaccuracyofthe rangeshows\\n4\\nOverall, most of the studied criteria increase as additional 77.5%increasefromthatof 1 (i.e.,from15.60%to28.69%).\\n4\\ninputs are added at each step. The notable exception is NC, While retraining MNIST against BIM-B using DSA shows\\nwhichplateausagainstmanysteps.Thisisinlinewithresults evengreaterimprovement(from9.40%to40.94%),wesuspect\\nin existing work [27]. There exists an interplay between this is an outlier as the accuracy for ranges 1 and 2 are\\n4 4\\nthe type of added inputs and how different criteria respond: signiﬁcantly smaller when compared to other conﬁgurations.\\nSNAC, KMNC, and NBC show signiﬁcant increases with the While our observations are limited to the DL systems and\\naddition of BIM-B examples to CIFAR-10, but change little inputgenerationtechniquesstudiedhere,weanswerRQ4that\\nwhenC&Winputsareadded.However,onlySNACandNBC SA can provide guidance for more effective retraining\\nexhibit a similar increase with the addition of input Set 1 for against adversarial examples based on our interpretation\\nChauffeur,whileKMNCincreasesmoresteadily.Overall,with of the observed trend.\\nthe exception of NC, we answer RQ3 that SC is correlated\\nDNN SA R FGSM BIM-A BIM-B JSMA C&W\\nwith other coverage criteria introduced so far. Model μ σ μ σ μ σ μ σ μ σ\\n∅ 11.65 - 9.38 - 9.38 - 18.88 - 8.92 -\\nDNN Criteria Test Step1 Step2 Step3 Step4 Step5 1/4 25.81 1.95 95.14 0.69 41.00 0.01 72.67 3.09 92.51 0.51\\nLSC 29.50 +F3G4S.9M0 +BI3M7.-1A0 +B5IM6.3-B0 +J6S1M.9A0 +C62&.0W0 MNIST LSA 234///444 222893...467560 234...969138 999555...789170 000...497189 444000...999873 000...111208 777557...043387 221...667805 999222...545516 010...607737\\nDSC 46.00 56.10 65.00 67.20 70.90 72.30\\nNC 42.73 42.73 43.03 43.03 43.03 45.45 1/4 15.60 2.12 93.67 3.42 9.90 1.05 74.56 2.62 12.80 0.96\\nMNIST KMNC 68.42 70.96 72.24 75.82 77.31 77.37 DSA 23//44 1296..6377 46..3125 9955..7387 00..7903 490..4801 00..0252 7768..1061 21..6897 1122..4367 11..0104\\nNBC 6.52 14.55 16.36 36.06 38.03 43.48 4/4 27.69 5.59 95.31 0.98 40.94 0.04 76.60 2.38 13.61 1.19\\nSNAC 10.91 19.39 19.39 53.33 57.27 57.27\\n∅ 6.13 - 0.00 - 0.00 - 2.68 - 0.31 -\\nLSC 46.20 54.70 55.8 57.70 61.10 63.20\\nDSC 66.20 70.10 70.6 80.90 83.40 84.10 1/4 11.07 1.20 32.34 1.70 0.59 1.76 32.80 2.05 34.38 2.83\\nCIFAR-10 NKNSNCMBACNCC 2216862....17555768 2216973....23278061 226971...3253.8108 23246437....20919861 2326444...7330.3112 23247447....04871140 CIFAR-10 LSA 2341////4444 11112224....97586936 2212....11118796 33322225....61798494 2222....04297099 0000....88609901 2210....11700060 33333554....88894132 2222....58502141 44442554....95729841 2222....72008342\\nDNN Criteria Test +SingleOcc +MultiOcc +Light DSA 23//44 1143..6841 11..9855 2391..5993 32..5727 00..0011 00..0000 3345..4691 12..8490 4446..7196 22..3425\\n4/4 13.12 1.41 32.17 2.36 0.60 1.76 37.32 1.58 46.21 2.72\\nLSC 30.00 42.00 42.00 76.00\\nNC 79.55 80.26 80.45 83.14 (a)MNISTandCIFAR-10\\nDave-2 KMNC 33.53 35.15 35.91 37.94\\nNBC 0.51 5.29 5.32 6.60\\nSNAC 1.03 10.58 10.64 13.21 DNN SA R SingleOcc MultiOcc Light\\nDNN Criteria Test +Set1 +Set2 +Set3 Model μ σ μ σ μ σ\\nLSC 48.90 53.50 56.10 58.40 ∅ 0.4212 - 0.0964 - 0.3822 -\\nNC 22.14 22.65 22.70 22.83 1/4 0.0586 0.0142 0.0539 0.0003 0.0573 0.0057\\nChauffeur KMNC 48.08 50.79 52.20 53.21 Dave-2 2/4 0.0540 0.0012 0.0562 0.0060 0.0560 0.0042\\nNBC 3.05 16.88 17.96 19.13 LSA\\n3/4 0.0554 0.0041 0.0544 0.0009 0.0570 0.0133\\nSNAC 3.93 18.37 19.41 20.93\\n4/4 0.0553 0.0028 0.0561 0.0042 0.0601 0.0111\\nTABLE VI: Changes in various coverage criteria against\\n(b)Dave-2\\nincreasing input diversity. We put additional inputs into the\\nTABLE VII: Retraining guided by SA: we sample 100 inputs\\noriginal test inputs and observe changes in coverage values.\\nU 2U\\nfrom four increasingly wider ranges of SA: [0, ], [0, ],\\n4 4\\n3U\\n[0, ],and[0,U],andretrainforﬁveadditionalepochsusing\\n4\\nD. Retraining Guidance (RQ4)\\nthe samples as the training data, and measure the accuracy\\nTable VII shows the impact of SA-based guidance for and MSE against the entire adversarial and synthetic inputs.\\nretraining of MNIST, CIFAR-10, and Dave-2 models. The Samplingfromwiderrangesimprovestheretrainingaccuracy.\\n1 4\\ncolumn R from to represents the increasingly wider\\n4 4\\nranges of SA from which the inputs for additional training\\nare sampled; rows with R = ∅ show performance of the VI. THREATSTOVALIDITY\\nDL system before retraining. Overall, there are 23 retraining The primary threat to internal validity of this study is the\\nconﬁgurations (2 SA types × 2 DL systems × 5 adversarial correctness of implementation of the studied DL systems, as\\nattackstrategies,and1SAtype×1DLsystem×threeinput well as the computation of SA values. We have used publicly\\nsynthesismethods),eachofwhichisevaluatedagainstfourSA available architectures and pre-trained models as our subjects\\nrangeswith20repetitions.Columnsμandσcontainthemean to avoid incorrect implementation. SA computation depends\\nand standard deviation of observed performance metric (i.e., on a widely used computation library, SciPy, which has\\n11004477\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\nstood the public scrutiny. Threats to external validity mostly of DL systems. Ma et al. proposed DeepCT, which views\\nconcerns the number of the models and input generation ranges of neuron activation values as parameter choices and\\ntechniques we study here. It is possible that SADL is less appliesCombinatorialInteractionTesting(CIT)tomeasurein-\\neffectiveagainstotherDLsystems.Whilewebelievethecore teractioncoverage[29].SCisdifferentfromDeepCTasSADL\\nprincipleofmeasuringinputsurpriseisuniversallyapplicable, aimsto quantify the amountofsurprise, ratherthan simply to\\nonly further experimentations can reduce this particular risk. detectsurpriseviaincreaseincoverage.DeepMutationapplies\\nFinally, threats to construct validity asks whether we are the principle of mutation testing to DL systems by mutating\\nmeasuring the correct factors to draw our conclusion. For all training data, test data, as well as the DL system itself, based\\nstudied DL systems, activation traces are immediate artefacts on source and model level mutation operators [28].\\noftheirexecutionsandthemeaningofoutputaccuracyiswell\\nVIII. CONCLUSION\\nestablished, minimising the risk of this threat.\\nWe propose SADL, a surprise adequacy framework for DL\\nVII. RELATEDWORK systems that can quantitatively measure relative surprise of\\nAdversarial examples pose signiﬁcant threats to the perfor- each input with respect to the training data, which we call\\nmance of DL systems [7]. There are existing work in the SurpriseAdequacy(SA).UsingSA,wealsodevelopSurprise\\nmachine learning community on detection of such inputs. Coverage (SC), which measures the coverage of discretised\\nFeinman et al. [14] ﬁrst introduced the KDE as a means of input surprise ranges, rather than the count of neurons with\\nsimilarity measurement, with the aim of detecting adversarial speciﬁc activation traits. Our empirical evaluation shows that\\nexamples. SADL improves upon the existing work by a SA and SC can capture the surprise of inputs accurately and\\nnumber of different ways. First, we generalise the concept aregoodindicatorsofhowDLsystemswillreacttounknown\\nofSurpriseAdequacy(SA)andintroduceDistance-basedSA. inputs. SA is correlated with how difﬁcult a DL system ﬁnds\\nSecond,ourevaluationisinthecontextofDLsystemtesting. an input, and can be used to accurately classify adversarial\\nThird,ourevaluationofSADLincludesmorecomplicatedand examples.SCcanbeusedtoguideselectionofinputsformore\\npractical DL systems, as well as testing techniques such as effectiveretrainingofDLsystemsforadversarialexamplesas\\nDeepXplore and DeepTest. Finally, we show that the choice well as inputs synthesised by DeepXplore.\\nof neurons has limited impact on LSA.\\nACKNOWLEDGEMENT\\nArangeoftechniqueshasbeenrecentlyproposedtotestand\\nThis work was supported by the Engineering Research\\nverify DL systems. The existing techniques are largely based\\nCenter Program through the National Research Foundation\\non two assumptions. The ﬁrst assumption is a variation of\\nof Korea funded by the Korean Government (MSIT) (NRF-\\nmetamorphictesting[11],[31],[42].SupposeaDLsystemN\\n2018R1A5A1059921), Institute for Information & commu-\\nproducesanoutputowhengiveniastheinput,i.e.,N(i)=o.\\nThen we expect N(i(cid:3)) (cid:7) o when i(cid:3) (cid:7) i . Huang et al. [19] nications Technology Promotion grant funded by the Ko-\\nrean government (MSIT) (No.1711073912), and the Next-\\nproposed a veriﬁcation technique that can automatically gen-\\nGeneration Information Computing Development Program\\nerate counter-examples that violate this assumption. Pei et\\nthrough the National Research Foundation of Korea funded\\nal. introduced DeepXplore [34], a white-box technique that\\nby the Korean government (MSIT) (2017M3C4A7068179).\\ngenerates test inputs that cause disagreement among a set of\\nDL systems, i.e., Nm(i) (cid:8)= Nn(i) for independently trained Robert Feldt acknowledges the projects TOCSYC (Swedish\\nKnowledge Foundation, KKS, num. 20130085) and BaseIT\\nDL systems Nm and Nn. Tian et al. presented DeepTest,\\n(SwedishScienceCouncil,VR,num.2015-04913)forfunding\\nwhose metamorphic relations include both simple geometric\\nparts of the work of this paper.\\nperturbations as well as realistic weather effects [40]. The\\nsecondassumptionisthatthemorediverseasetofinputis,the REFERENCES\\nmoreeffectiveitwillbefortestingandvalidatingDLsystems.\\n[1] Autonomous driving model: Chauffeur. https://github.com/udacity/\\nPei et al. proposed Neuron Coverage (NC), which measures self-driving-car/tree/master/steering-models/community-models/\\nthe ratio of neurons whose activation values are above a chauffeur.\\npredeﬁned threshold [34]. It has been shown that adding test [2] The udacity open source self-driving car project. https://github.com/\\nudacity/self-driving-car.\\ninputs that violate the ﬁrst assumption increases the diversity [3] Google accident 2016: A google self-driving car caused a crash\\nmeasured through NC. Similarly, DeepGauge introduced a for the ﬁrst time http://www.theverge.com/2016/2/29/11134344/\\nset of multi-granularity coverage criteria that are thought to google-self-driving-car-crash-report,2016.\\n[4] Paul Ammann and Jeff Offutt. Introduction to Software Testing.\\nreﬂect behaviours of DL systems in ﬁner granularity [27]. CambridgeUniversityPress,2016.\\nWhile these criteria capture input diversity, all of them are [5] YoshuaBengio,Gre´goireMesnil,YannDauphin,andSalahRifai.Better\\nessentially count of neurons unlike SA, and therefore cannot mixingviadeeprepresentations. CoRR,abs/1207.4404,2012.\\n[6] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard\\nbedirectlylinkedtobehavioursofDLsystems.Weshowthat Firner,BeatFlepp,PrasoonGoyal,LawrenceDJackel,MathewMon-\\nSA is closely related to the behaviours by training accurate fort,UrsMuller,JiakaiZhang,etal.Endtoendlearningforself-driving\\nadversarial example classiﬁers based on SA. cars. arXivpreprintarXiv:1604.07316,2016.\\n[7] Nicholas Carlini and David Wagner. Adversarial examples are not\\nApart from coverage criteria, other concepts in traditional easily detected. Proceedings of the 10th ACM Workshop on Artiﬁcial\\nsoftwaretestinghavebeenreformulatedandappliedtotesting IntelligenceandSecurity-AISec’17,2017.\\n11004488\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\n[8] Nicholas Carlini and David Wagner. Adversarial examples are not [29] Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu, Jianjun Zhao,\\neasily detected: Bypassing ten detection methods. In Proceedings of and Yadong Wang. Combinatorial testing for deep learning systems.\\nthe 10th ACM Workshop on Artiﬁcial Intelligence and Security, pages arXivpreprintarXiv:1806.07723,2018.\\n3–14.ACM,2017. [30] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wi-\\n[9] NicholasCarliniandDavidA.Wagner. Towardsevaluatingtherobust- jewickrema, Michael E Houle, Grant Schoenebeck, Dawn Song, and\\nnessofneuralnetworks. CoRR,abs/1608.04644,2016. JamesBailey. Characterizingadversarialsubspacesusinglocalintrinsic\\n[10] ChenyiChen,AriSeff,AlainKornhauser,andJianxiongXiao.Deepdriv- dimensionality. arXivpreprintarXiv:1801.02613,2018.\\ning:Learningaffordancefordirectperceptioninautonomousdriving.In [31] Christian Murphy, Kuang Shen, and Gail Kaiser. Automatic system\\nProceedingsoftheIEEEInternationalConferenceonComputerVision, testing of programs without test oracles. In Proceedings of the 18th\\npages2722–2730,2015. InternationalSymposiumonSoftwareTestingandAnalysis,ISSTA2009,\\n[11] T. Y. Chen, F.-C. Kuo, T. H. Tse, and Zhi Quan Zhou. Metamorphic pages189–200.ACMPress,2009.\\ntesting and beyond. In Proceedings of the International Workshop on [32] Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow,\\nSoftwareTechnologyandEngineeringPractice(STEP2003),pages94– Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom\\n100,September2004. Brown,AurkoRoy,AlexanderMatyasko,VahidBehzadan,KarenHam-\\n[12] ZhihuaCui,FeiXue,XingjuanCai,YangCao,Gai-geWang,andJinjun bardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley,\\nChen. Detection of malicious code variants based on deep learning. Abhibhav Garg, Jonathan Uesato, Willi Gierke, Yinpeng Dong, David\\nIEEETransactionsonIndustrialInformatics,14(7):3187–3196,2018. Berthelot, Paul Hendricks, Jonas Rauber, and Rujun Long. Technical\\n[13] ClementFarabet,CamilleCouprie,LaurentNajman,andYannLeCun. report on the cleverhans v2.1.0 adversarial examples library. arXiv\\nLearninghierarchicalfeaturesforscenelabeling. IEEEtransactionson preprintarXiv:1610.00768,2018.\\npatternanalysisandmachineintelligence,35(8):1915–1929,2013. [33] NicolasPapernot,PatrickD.McDaniel,SomeshJha,MattFredrikson,\\nZ. Berkay Celik, and Ananthram Swami. The limitations of deep\\n[14] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B\\nlearninginadversarialsettings. CoRR,abs/1511.07528,2015.\\nGardner. Detecting adversarial samples from artifacts. arXiv preprint\\n[34] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore:\\narXiv:1703.00410,2017.\\nAutomatedwhiteboxtestingofdeeplearningsystems.InProceedingsof\\n[15] Robert Feldt, Simon Poulding, David Clark, and Shin Yoo. Test set\\nthe26thSymposiumonOperatingSystemsPrinciples,SOSP’17,pages\\ndiameter:Quantifyingthediversityofsetsoftestcases. InProceedings\\n1–18,NewYork,NY,USA,2017.ACM.\\noftheIEEEInternationalConferenceonSoftwareTesting,Veriﬁcation,\\n[35] SimonPouldingandRobertFeldt. Generatingcontrollablyinvalidand\\nandValidation,ICST2016,pages223–233,2016.\\natypicalinputsforrobustnesstesting. InSoftwareTesting,Veriﬁcation\\n[16] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining\\nand Validation Workshops (ICSTW), 2017 IEEE International Confer-\\nand harnessing adversarial examples. In International Conference on\\nenceon,pages81–84.IEEE,2017.\\nLearningRepresentations,2015.\\n[36] David W Scott. Multivariate density estimation: theory, practice, and\\n[17] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman\\nvisualization. JohnWiley&Sons,2015.\\nMohamed,NavdeepJaitly,AndrewSenior,VincentVanhoucke,Patrick\\n[37] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence\\nNguyen, Tara N Sainath, et al. Deep neural networks for acoustic\\nlearning with neural networks. In Advances in neural information\\nmodeling in speech recognition: The shared views of four research\\nprocessingsystems,pages3104–3112,2014.\\ngroups. IEEESignalprocessingmagazine,29(6):82–97,2012.\\n[38] ChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet,ScottReed,\\n[18] Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. DragomirAnguelov,DumitruErhan,VincentVanhoucke,andAndrew\\nNeuralComputation,9(8):1735–1780,1997. Rabinovich. Going deeper with convolutions. In Proceedings of the\\n[19] XiaoweiHuang,MartaKwiatkowska,SenWang,andMinWu. Safety IEEEconferenceoncomputervisionandpatternrecognition,pages1–\\nveriﬁcation of deep neural networks. In Rupak Majumdar and Viktor 9,2015.\\nKuncˇak,editors,ComputerAidedVeriﬁcation,pages3–29,Cham,2017. [39] L. Tarassenko. BioSign™ : multi-parameter monitoring for early\\nSpringerInternationalPublishing. warning of patient deterioration. IET Conference Proceedings, pages\\n[20] Se´bastienJean,KyunghyunCho,RolandMemisevic,andYoshuaBen- 71–76(5),January2005.\\ngio.Onusingverylargetargetvocabularyforneuralmachinetranslation. [40] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. Deeptest:\\nIn Proceedings of the 53rd Annual Meeting of the Association for Automatedtestingofdeep-neural-network-drivenautonomouscars. In\\nComputational Linguistics and the 7th International Joint Conference Proceedingsofthe40thInternationalConferenceonSoftwareEngineer-\\nonNaturalLanguageProcessing(Volume1:LongPapers),volume1, ing,pages303–314.ACM,2018.\\npages1–10,2015. [41] Matt P Wand and M Chris Jones. Kernel smoothing. Chapman and\\n[21] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 Hall/CRC,1994.\\ndataset. online:http://www.cs.toronto.edu/kriz/cifar.html,2014. [42] Shin Yoo. Metamorphic testing of stochastic optimisation. In Pro-\\n[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet ceedingsofthe3rdInternationalWorkshoponSearch-BasedSoftware\\nclassiﬁcation with deep convolutional neural networks. In Advances Testing,SBST2010,pages192–201,2010.\\ninneuralinformationprocessingsystems,pages1097–1105,2012. [43] ShinYooandMarkHarman.Regressiontestingminimisation,selection\\n[23] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial andprioritisation:Asurvey. SoftwareTesting,Veriﬁcation,andRelia-\\nexamplesinthephysicalworld. CoRR,abs/1607.02533,2016. bility,22(2):67–120,March2012.\\n[24] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. [44] HongZhu,PatrickA.V.Hall,andJohnH.R.May. Softwareunittest\\nNature,521(7553):436,2015. coverageandadequacy.ACMComput.Surv.,29(4):366–427,December\\n[25] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten 1997.\\ndigit database. AT&T Labs [Online]. Available: http://yann. lecun.\\ncom/exdb/mnist,2,2010.\\n[26] Stijn Luca, Peter Karsmakers, Kris Cuppens, Tom Croonenborghs,\\nAnoukVandeVel,BertenCeulemans,LievenLagae,SabineVanHuffel,\\nandBartVanrumste.Detectingrareeventsusingextremevaluestatistics\\napplied to epileptic convulsions in children. Artiﬁcial Intelligence in\\nMedicine,60(2):89–96,2014.\\n[27] LeiMa,FelixJuefei-Xu,JiyuanSun,ChunyangChen,TingSu,Fuyuan\\nZhang, Minhui Xue, Bo Li, Li Li, Yang Liu, Jianjun Zhao, and\\nYadongWang.Deepgauge:Comprehensiveandmulti-granularitytesting\\ncriteria for gauging the robustness of deep learning systems. CoRR,\\nabs/1803.07519,2018.\\n[28] LeiMa,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,FelixJuefei-Xu,\\nChaoXie,LiLi,YangLiu,JianjunZhao,etal.Deepmutation:Mutation\\ntesting of deep learning systems. arXiv preprint arXiv:1805.05206,\\n2018.\\n11004499\\nAuthorized licensed use limited to: Universidad Nacional Autonoma De Mexico (UNAM). Downloaded on March 08,2021 at 20:33:46 UTC from IEEE Xplore.  Restrictions apply. \\n',\n",
       "       'Comparative analysis of the government plans of\\nthe Peruvian presidential candidates, SDO(UN)\\nand State Policies of the National Agreement\\nbased on NLP\\nHonorio Apaza Alanoca1, Josimar Chire2 and Jimy Oblitas3\\n1\\n2 1 Data Science Research Group\\n0\\n, National University of Moquegua, Ilo, Moquegua, Peru\\n2\\n2 Institute of Mathematics and Computer Science (ICMC),\\n \\nr University of Sa˜o Paulo (USP), Sa˜o Carlos, SP, Brazil\\np\\n3 Facultad de Ingenier´ıa,\\nA\\nUniversidad Privada del Norte, Cajamarca, Peru´\\n5  hapazaa@unam.edu.pe, jecs89@usp.br, jimy.oblitas@upn.edu.pe\\n \\n \\n]\\nY\\nAbstract. The analysis of government proposal during elections from\\nC\\npolitical parties is vital to choose the next authorities in any city or\\n.\\ns country. In this paper, we use a text mining approach to analyze the\\nc documentsandprovideaneasyvisualizationtosupportaneasyanalysis.\\n[\\n  Besides, a comparison with a national plan based on sustainable devel-\\n \\n1 opmentobjectivesofUN(UnitedNations)from2030Agendaisperfomed\\nv using Natural Language techniques.\\n5\\n6 Keywords: Natural Language Processing, Text Mining, Data Science,\\n7 System Recommender, Elections, Politics, Peru, South America\\n1\\n0\\n.\\n4 1 Introduction\\n0\\n1\\nElection of authorities is an important event, because citizens will choose the\\n2\\npeoplewhowillrepresentthemandpurposeprojectstoimprovethenational,re-\\n:\\nv gional context. Traditionally, political parties promote their candidates through\\ni\\nX mass media, i.e. radio, television, social networks and more. Candidates travel\\nto visit cities and gain more electors.\\nr\\na In Peru, to participate in president elections is a requirement to send a gov-\\nernment proposal or plan to Jurado Nacional de Elecciones (National Elections\\nJury). This document summarizes the proposal of the candidates, considering\\nthemostimportantproblemsforthepartyandsolutionsthattheypurpose.Usu-\\nally, these documents have dozens of pages and these are not read for citizens\\nto choose the next authority. Besides, United Nations (UN) purposed an 2030\\nAgenda to summarize the most important issues which need special attention\\nfor governments related to poverty, communication, discrimination and more.\\nIn 2015, the United Nations (UN) adopted a new international develop-\\nment agenda: the 2030 Agenda that includes the 17 Sustainable Development\\n2 Honorio Apaza Alanoca, Josimar Chire and Jimy Oblitas\\nGoals and 169 targets. This agenda speciﬁes the need for actions to strengthen\\nsustainable economic growth, decent employment and industrialization in all\\ncountries[Caribbean, 2017].\\nThe2030Agendaconsidersacomplexcombinationoffairlydetailedthematic\\ntargets, through a comprehensive approach that requires addressing sustainable\\ndevelopment as a necessary integration of the social, economic and environmen-\\ntal axes [Nieto, 2017]. Although it is recognized that each country has its own\\npriorities, this agenda is a reference for government plans seeking an adequate\\nsustainabledevelopmentofPeru.Therefore,measuringthealignmentorpossible\\nevolution of government plans of presidential candidates is a necessary task.\\nIn this context, the use of software tools, such as text mining, emerges as a\\nquick and interesting proposal to measure trends. In addition to the fact that,\\nin the Peruvian context, such tools are not used yet, this contrasts with global\\ntrends in the use of software tools that are already established, as in the cam-\\npaignsofTrumpandBolsonaro,intheUnitedStates(USA)andBrazil,whichil-\\nlustratepolicyfactsthathavebeenfavoredbyICTs[Garcia-Nunes et al., 2020].\\nNatural language processing has shown potential as a promising tool to ex-\\nploit urban data sources. Authors, such as [Cai, 2021], suggest that the use of\\nurban big data sources is still starting and the most studied areas are: urban\\ngovernance and management, public health, land use and functional zones, mo-\\nbility and urban design, having been very useful in expanding study scales and\\nreducing research costs.\\nText Mining area uses a well-know Data Mining approach, from Data Col-\\nlection, Exploration, Analysis to Visualization. Text Mining focuses in Text\\nAnalysis, uses Natural Language Techniques (NLP). Many studies were per-\\nformed to analyze diﬀerent problems from diﬀerent areas, i.e. epidemiology\\n[Chire Saire and Oblitas Cruz, 2020],politics[Sharma and Shekhar, 2020],mar-\\nketing, etc.\\nApplicationsofTextMininginPoliticsandElections,i.e.AnticipatingPolit-\\nical Behaviour [Sangar et al., 2013], Study Voting Patterns [Bagui et al., 2007],\\nFraudIdentiﬁcation[Poloni and Formolo, 2015],SentimentalAnalysisofcitizens\\n[Sharma and Ghose, 2020],ElectionResultPrediction[Ramteke et al., 2016]and\\nmore.\\nThe objective of this paper is analyze the government proposal of Peruvian\\ncandidates to president elections using a Text Mining Approach to support an\\neasyunderstandingofthedocuments.Besides,performamatchingprocesswith\\nnational plan adapted from 2030 Agenda, to check how important are these\\nobjective for political parties.\\nSectionIincludesthereviewofthebibliography,SectionIIdevelopsthework\\nproposal, Section III discloses the results of the research and in Section IV gives\\nconclusions, last section presents future work.\\nAnalysis government plans Peruvian presidential candidates 3\\n2 Proposal\\nNatural language processing is a process transformation the text information\\nin numeric data [Di Giuda et al., 2020]. This work is based on the following\\nresearch process:\\nData Collection Data Analysis Reporting\\nSelect and retrieve data \\nComparative analysis \\n(Government plan of  Report research results \\nWith algorithm Jaro \\nCandidates for the  and findings.\\nWinkler.\\nPresidency of Peru).\\nFig.1: Research process, this process is planed and used for [Kim et al., 2017]\\n2.1 Data collection\\nFor the present work, 18 government plans of the candidates for the presidency\\nof the Republic of Peru have been collected. Also the sustainable development\\ngoals and policies of the state of the national agreement, the sustainable devel-\\nopment goals (SDGs) promoted by the United Nations, whose predecessor are\\nthe Millennium Development Goals, constitute an inclusive global agenda with\\ngoals for 2030[secretaria ejecutivo del acuerdo nacional, 2017].\\n2.2 Data analysis\\nJaroWinkleristhemainalgorithmtoperformcomparativetextanalysisofdoc-\\numents(governmentplansofthecandidates)withtheSustainableDevelopment\\nGoals (SDGs) promoted by the United Nations.\\n(cid:40)\\n0 if m=0\\nSim (s ,s )= (1)\\nj 1 2 1(m + m + m−t)\\n3 s1 s2 m\\nThe objective is to calculate the distance of the strings of texts that are written\\nin the plans of the government of the candidates and the objectives and policies\\nof sustainable development of the state of the national agreement. In this ﬁrst\\npreliminary test of the research we are interested in knowing what results are\\nobtained with Jaro Winkler.\\n2.3 Reporting\\nFinally,thelaststageoftheresearchistomakeareportontheresultsobtained,\\nin this case the results are the Jaro Winkler distance between the plans of the\\ncandidates’governmentandtheobjectivesandsustainabledevelopmentpolicies\\nof the state of the national agreement.\\n4 Honorio Apaza Alanoca, Josimar Chire and Jimy Oblitas\\n3 Results\\nThis section shows the result frequency of terms in a word cloud, it can be\\nseen that each candidate highlights a particular topic, such as: System, Health,\\nProgram, etc. This result is due to the fact that currently the nation and the\\nworld are suﬀering from a global pandemic, therefore, the plans of the candi-\\ndates’ government propose proposals to solve problems related to health. This\\nalso shows that other important issues such as education, economics, etc. have\\nbeenneglected.Especiallyissuesrelatedtosustainabledevelopmentgoals(SDG)\\npromoted by the United Nations.\\nAccion Popular Partido Morado Avanza Pais Alianza para el Progreso APRA\\nDemocracia Directa Frente Amplio Frente Esperanza Fuerza Popular Juntos por el Peru\\nPartido Nacionalista Peru Libre Patria Segura Podemos Peru PPC\\nRenovavion Popular Somos Peru Union por el Peru Victoria Nacional\\nFig.2: Cloud of words of plans of the government of the candidates\\nAmong the candidates’ plans, the one that stands out the most is the gov-\\nernment plan of the political party Avanza Pais on the economic issue, It can\\nalso be seen that the Accion Popular political party has a uniform distribution\\nin its government plan on the issues of economy, health, education and politics.\\nCan be seen in Figure 3.\\nInthiscasewecanvarytheissueswewanttomeasure,thiscanbeaccording\\ntothecontextofthemomentanddiﬀerentsectorsofsociety,theyhavediﬀerent\\nproblems and needs, so it is important to analyze from other points of view,\\nsocial classes and thoughts.\\nBelow we present a graphical (Figure 4) representation of how similar are the\\ngovernment plans of the candidates for the presidency of Peru, in Figure 4 it\\ncan be seen that they are not so identical, but if you can see the degree of\\nsimilarity they have, but This is due to the fact that government plans clearly\\naddress very similar issues that translate into social problems (health, economy,\\nprograms, etc.) and government (judiciary, corruption, congress, etc.).\\nIn the experiment, the diﬀerences by proliﬁc class were also denoted, in some\\ncases the distance is very noticeable between the political parties considered to\\nAnalysis government plans Peruvian presidential candidates 5\\n0.00035\\n0.00030\\ngobierno\\n0.00025\\npolítica\\neducación 0.00020\\nsalud\\n0.00015\\neconomía\\nreligión 0.00010\\nAccion Popular Partido Morado Avanza Pais Alianza para el Progreso APRA Democracia Directa Frente Amplio Frente Esperanza Fuerza Popular Juntos por el Peru Partido Nacionalista Peru Libre Patria Segura Podemos Peru PPC Renovavion Popular Somos Peru Union por el Peru Victoria Nacional 00..0000000005\\nFig.3: Important areas in the documents\\nbeontheleftwiththoseontheright.Whichcanbesimilarinthedailyexercise,\\nwhich obviously have very diﬀerent thoughts, therefore very diﬀerent proposals\\nbetween these two sides of Peruvian politics.\\n1.0\\n0.0\\n2.5\\n0.9\\n5.0\\n7.5 0.8\\n10.0\\n0.7\\n12.5\\n15.0\\n0.6\\n17.5\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nFig.4: Documents similarity\\n6 Honorio Apaza Alanoca, Josimar Chire and Jimy Oblitas\\nIn this section we are going to analyze the distance between chains of texts\\nwritteninthegovernmentplansofthecandidatesandtheobjectivesandpolicies\\nof sustainable development of the state of the national agreement, we try to\\ndiﬀerentiatethesimilaritiesbetweenthesetwodocuments,whenachainoftexts\\nissimilartoanothermeansthatthedocumentcontainstextssimilartotheother.\\nSo, we could say that a government plan addresses one or many sustainable\\ndevelopment goals and policies of the state of the national agreement.\\nFIN DE LA POBREZA\\nHAMBRE CERO 0.66\\nSALUD Y BIENESTAR\\nEDUCACION DE CALIDAD 0.64\\nIGUALDAD DE GENERO\\nAGUA LIMPIA Y SANEAMIENTO 0.62\\nENERGIA ASEQUIBLE Y NO CONTAMINANTE\\nTRABAJO DECENTE Y CRECIMIENTO ECONOMICO 0.60\\nINDUSTRIA INNOVACION E INFREESTRUCTURA\\nREDUCCION DE LAS DESIGUALDADES\\n0.58\\nCIUDADES Y COMUNIDADES SOSTENIBLES\\nPRODUCCION Y CONSUMO RESPONSABLES\\n0.56\\nACCION POR EL CLIMA\\nVIDA SUBMARINA\\n0.54\\nVIDA DE ECOSISTEMAS TERRESTRES\\nPAZ JUSTICIA E INSTITUCIONES SOLIDAS\\nALIANZAS PARA LOGRAR LOS OBJETIVOS 0.52\\nAccion Popular Partido Morado Avanza Pais Alianza para el Progreso APRA Democracia Directa Frente Amplio Frente Esperanza Fuerza Popular Juntos por el Peru Partido Nacionalista Peru Libre Patria Segura Podemos Peru PPC Renovavion Popular Somos Peru Union por el Peru Victoria Nacional\\nFig.5: Documents similarity Plan\\nIn the graph above, it can be seen that the government plan of the political\\npartyAvanzaPaisaddressesmuchmorethanothersthegoalofpeace,justiceand\\nsolid institutions (paz justicia e instituciones solidas), followed by the political\\nparty Renovacion Ppular. However, little is addressed the objectives such as:\\nunderwater life(Vida submarina), health and well-being(salud y bienestar), end\\nof poverty(ﬁn de la probreza), etc.\\n4 Conclusions\\nThe algorithm Jaro Winkler based on measuring the distance of text chains\\nshows us that we are very interesting preliminary results, it shows us some\\ndiﬀerences between the government plans of the candidates for the presidency\\nof Peru, as well as the objectives of the Sustainable Development Goals and the\\nState Policies of the National Agreement. However, these results can be further\\nreﬁned with the most advanced artiﬁcial intelligence methods or algorithms.\\nAnalysis government plans Peruvian presidential candidates 7\\nIn the present we want to highlight the way in which the diﬀerences between\\nthe government plan documents can be graphically demonstrated, this way of\\nshowing the document diﬀerences is very important for the electorate, because\\nwithouthavingtoreadallthegovernmentplans,theycanobtainamoregeneral\\nvision graphically.\\n5 Future work\\nOneofthefuturejobsistoexperimentwithhighlyadvancedartiﬁcialintelligence\\ntechniques in the discipline of natural language processing and text mining.\\nIt would be very interesting to study and experience how coherent the argu-\\nments of the candidates are in the debate with their government plan. Because\\nthere must be coherence of ideas between the proposals that are written in the\\ngovernment plan with what the candidate expresses in the debate, interviews in\\nthe press, etc.\\nReferences\\nBagui et al., 2007. Bagui, S., Mink, D., and Cash, P. (2007). Data mining techniques\\nto study voting patterns in the US. Data Science Journal, 6(0):46–63.\\nCai, 2021. Cai,M.(2021). Naturallanguageprocessingforurbanresearch:Asystem-\\natic review. Heliyon, 7(3):e06322.\\nCaribbean, 2017. Caribbean, E. C. f. L. A. a. t. (2017). 2030 agenda for sustainable\\ndevelopment. Last Modiﬁed: 2017-06-28T13:23-04:00 Publisher: CEPAL.\\nChire Saire and Oblitas Cruz, 2020. ChireSaire,J.andOblitasCruz,J.(2020).Study\\nofCoronavirusImpactonParisianPopulationfromApriltoJuneusingTwitterand\\nText Mining Approach. pages 242–246.\\nDi Giuda et al., 2020. Di Giuda, G. M., Locatelli, M., Schievano, M., Pellegrini, L.,\\nPattini, G., Giana, P. E., and Seghezzi, E. (2020). Natural Language Processing for\\nInformationandProjectManagement,pages95–102. SpringerInternationalPublish-\\ning, Cham.\\nGarcia-Nunes et al., 2020. Garcia-Nunes, P. I., Rodrigues, P. A., Oliveira, K. G., and\\nda Silva, A. E. A. (2020). A computational tool for weak signals classiﬁcation –\\nDetectingthreatsandopportunitiesonpoliticsinthecasesoftheUnitedStatesand\\nBrazilian presidential elections. Futures, 123:102607.\\nKim et al., 2017. Kim,K.,joungPark,O.,Yun,S.,andYun,H.(2017). Whatmakes\\ntouristsfeelnegativelyabouttourismdestinations?applicationofhybridtextmining\\nmethodologytosmartdestinationmanagement. TechnologicalForecastingandSocial\\nChange, 123:362–369.\\nNieto, 2017. Nieto, A. T. (2017). CRECIMIENTO ECONO´MICO E INDUSTRIAL-\\nIZACIO´N EN LA AGENDA 2030: PERSPECTIVAS PARA ME´XICO. Problemas\\ndel Desarrollo, 48(188):83–111.\\nPoloni and Formolo, 2015. Poloni,Y.T.andFormolo,D.(2015).Dataminingtoiden-\\ntify fraud suspected on electronic elections. In 2015 Ninth International Conference\\non Complex, Intelligent, and Software Intensive Systems, pages 19–23.\\nRamteke et al., 2016. Ramteke,J.,Shah,S.,Godhia,D.,andShaikh,A.(2016). Elec-\\ntion result prediction using twitter sentiment analysis. In 2016 International Con-\\nference on Inventive Computation Technologies (ICICT), volume 1, pages 1–5.\\n8 Honorio Apaza Alanoca, Josimar Chire and Jimy Oblitas\\nSangar et al., 2013. Sangar, A. B., Khaze, S. R., and Ebrahimi, L. (2013). Participa-\\ntion anticipating in elections using data mining methods.\\nsecretaria ejecutivo del acuerdo nacional, 2017. secretaria ejecutivo del acuerdo na-\\ncional (2017). Objetivos de desarrollo dostenible y politicas del estado del acuerdo\\nnacional.\\nSharma and Ghose, 2020. Sharma, A. and Ghose, U. (2020). Sentimental analysis of\\ntwitter data with respect to general elections in india. Procedia Computer Science,\\n173:325–334. International Conference on Smart Sustainable Intelligent Computing\\nand Applications under ICITETM2020.\\nSharma and Shekhar, 2020. Sharma, A. and Shekhar, H. (2020). Intelligent Learning\\nbasedOpinionMiningModelforGovernmentalDecisionMaking. ProcediaComputer\\nScience, 173:216–224.\\n',\n",
       "       'J.Inst.Eng.IndiaSer.B\\nhttps://doi.org/10.1007/s40031-020-00501-5\\nORIGINAL CONTRIBUTION\\nText Similarity Measures in News Articles by Vector Space Model\\nUsing NLP\\nRitika Singh1 • Satwinder Singh1\\nReceived:4June2020/Accepted:7October2020\\n(cid:2)TheInstitutionofEngineers(India)2020\\nAbstract The present global size of online news websites Keywords Bilingual news article similarity (cid:2)\\nis more than 200 million. According to MarketingProfs, Cosine similarity (cid:2) Jaccard similarity (cid:2) Euclidean distance\\nmorethan2millionarticlesarepublishedeverydayonthe\\nweb, but Online News websites have also circulated edi-\\ntorialcontent overthe internetthat speciﬁes which articles Introduction\\nto display on their website’s home pages and what articles\\nto highlight, e.g., broad text size for main news articles. A huge increase in the number of online newspaper pub-\\nMany of the articles posted on a news website are very lishing is only because of the digital technology innova-\\nsimilar to many other news websites. The selective tions. When in the modern world so much information\\nreporting of top news headlines and also the similarity appears at a tremendous speed, readers need to ﬁnd out if\\namong news across various news associations is well- they are reading true news or false news. False news and\\nidentiﬁedbutnotverywellcalculated.Thispaperidentiﬁes information can endanger and confuse not only a person’s\\nthe top news items on the news sites and measures the life,butalsoanentiresociety,soitisveryimportanttoﬁnd\\nsimilarity between two same news items in two languages out the source of information and compare it with other\\n(Hindi and English) referring to the same event. To news. So this study has an interest in extracting online\\naccomplish this, a highlighted headline and link extractor news platforms, speciﬁcally to measure the similarity of\\nhas been created to extract top news for both Hindi and news articles across various sites. This article provides\\nEnglishfromGoogle’snewsfeed.First,translatetheHindi detailsaboutwhatnewsisbeingconsidered,howitisbeing\\nnews article into English by using Google translator and presented and, and highlighted on a website [1]. News\\nthen compare it with English news articles. Second, we articles which are published on the website usually appear\\nused the cosine similarity, Jaccard similarity, Euclidean in similar or rectiﬁed form on several different websites.\\ndistance measure to calculate news similarity score. The Similar and almost identical news is confusing for users.\\nfrequency of nouns and the next word of nouns from the Similarity slows down the process of discovering new\\nnews articles are also extracted. Our methodology clearly information about a topic, and potentially leads to missing\\nshowsthatwecanefﬁcientlyidentifytopnewsarticlesand information,iftheusermistakenlyrecognizestwonewsas\\nmeasure the similarity between news reports. similarwheninfactonecontainsnewdata.Itismuchmore\\ndifﬁcult to locate similar news items in websites. This is\\nbecause of the large amount of miscellaneous content or\\nmaterial on these articles. Although the main news article\\n& RitikaSingh text can be similar on two different web pages, the extra-\\nritikasingh2397@outlook.com\\nneous material on the pages may not be the same. There-\\nSatwinderSingh fore, traditional approaches to equivalent news\\nsatwinder.singh@cup.edu.in\\ndetermination would fail [2]. First, this paper developed a\\n1 DepartmentofComputerScienceandTechnology,Central method for scraping top news headline text from web\\nUniversityofPunjab-Bathinda,Bathinda,India pages,i.e.Googlenewsfeedwebsiteswhicharepresentin\\n123\\nJ.Inst.Eng.IndiaSer.B\\ntwo different languages (Hindi and English), referring to do this, they ﬁrst created a headline and link extractor that\\nthe sameevent then use the extracted textto classify news parses selected news websites and then searched ten US-\\npairs with the same content, avoiding any irrelevant based news site home pages for 3 months. They use a\\ninformationonthearticles.Bymeasuringasimilarityscore parser to extract k = 1, 3, 10 for each news site, the max-\\nfor news pairs based on a method called Cosine similarity imum number of articles. Second, the author uses the cal-\\nand Jaccard similarity and Euclidean Similarity, this culation of cosine similarity to quantify the similarity of\\nresearch can distinguish similar news articles, as well as news. They also provide techniques during this work to\\ndifferentones.Thepurposeofthispaperisalsotodiscover assistinanalyzingarchivednewswebpagesbyintroducing\\nbilingual news articles in a comparable corpus [3]. In tools for parsing select HTML news sites for Hero and\\nparticular, the study is dealing with the representation of headlinestoriesusingCSSselectors.Author’sstudiesover\\nnews and the measurement of the similarity among new 3 months have shown that the overall similarity decreased\\narticles. This experiment uses the similarly named entities asthenumberofarticlesincreased.Studiesfromtheauthor\\nwhich they include as representative features of the news. indicate that they would set up synchronous stories for a\\nToassessthesimilaritybetweenarticlesofthesamenews, given day besides relevant national events. This approach\\nthis research proposing a new method focused on a canbeusedtofurtherexaminetheoccasionalelectionsthat\\nknowledge base framework that aims to provide human are being held.\\ninformation on the value of the category of named entities Katarzyna Baraniak and Marcin Sydow work on tools\\nwithin the news [4]. In a comparable corpus with news in that would support the detection and analysis of the\\nHindi and English, we compared our approach to a tradi- information bias [7]. The author uses methods to auto-\\ntional one which obtains better results. Similarity and also matically identify the articles reporting on the same sub-\\ndistance measures calculate the similarity of two docu- ject, event, or entity to use them more in comparative\\nmentsorsentencesintoasinglenumericalvalueandbrings analysisortoconstructatestortrainingcollection.Within\\nout the degree of semantic similarity [5] or distance from the paper, the author explains representations of the doc-\\noneanother.Severalsimilaritymeasureshavebeenusedby ument text and the method of similarity measures for text\\nthe researchers, but not much work has been done on the clustering. Which include tests such as cosine similarity,\\nsimilarity of newspapers. This study aims to compare the Euclideandistance,Jaccardcoefﬁcient,Pearsoncoefﬁcient\\nsemanticsimilaritybetweentwoarticlesofthesamenews, of correlation, and Averaged Kullback–Leibler Diver-\\npresent in two different languages (Hindi and English), to gence. The author also applies a machine learning\\noptimize human understanding. The basic concept for approach to recognize a similar article and develop a\\nmeasuring news similarities is to identify Feature articles machine learning model that detects similar articles auto-\\nvectors, and thereafter measure the difference between matically.Identifyingfragments oftext concerningsimilar\\nthose features. Low distance between those features eventsandidentifyingbiasinthemisexpected.Theauthor\\nimplies a high level of similarity value, while a large dis- is also working to expand the research study to other lan-\\ntance in between those features implies a low level of guages (e.g., Polish, English).\\nsimilarity value [6]. Euclidean distance, Cosine distance, Maake Benard Magara et al., suggest a system to use\\nJaccardcoefﬁcientmetricsaresomeofthedistancemetrics 220 artiﬁcial intelligent research paper written by 8 artiﬁ-\\nused in document similarity computation. This study cial intelligence experts [8]. This work uses Recursive\\nexplores twoseparatemethods ofgeneratingfeatures from Partitioning, Random Forest, and improved machine\\nthe texts: (1) the Tf-idf vectors, (2) bag of words also learning algorithms by having an average accuracy and\\nimplements two methods for calculating textual similarity timing efﬁciency of 80.73 and 2.354628. Seconds, this\\nbetween news articles: (1) cosine similarity and Jaccard algorithm typically performed quite well compared to the\\nsimilarity with Tf-idf vectors and (2) Euclidean distance Boosted and even the Random Forest algorithms. More\\nusing a bag of words. sophisticated models can be used in future studies much\\nlike the Latent SemanticAnalysis (LSA),sincedocuments\\ncanbeidentiﬁedasbelongingtothesameclassevenifthey\\nLiterature Rereview have no similar words and phrases. Vikas Thada and Dr.\\nVivek Jaglan authors used the cosine similarity, dice\\nIn the literature, similarity measures have been used for coefﬁcient, Jaccard similarity algorithms [9]. The work is\\nvarious purposes. In this section, some proposals are completedontheﬁrst10pagesoftheGooglesearchresult\\nreviewed. and will be expanded to 30–35 pages for a reliable efﬁ-\\nAtkins et al. [1] describe a technique to assess the top ciency estimate in future study. The cosine similarity\\nnews headline story from a selected set of US-based news eventually concluded was the best ﬁtness compared with\\nwebsites, and then calculate correlations across them. To others for this dataset. In summary, while the initial\\n123\\nJ.Inst.Eng.IndiaSer.B\\nﬁndings are promising, there is still a long way to go to\\nachieve the greatest crawling efﬁciency possible. A sys-\\ntematicmethodproposedbyNasabetal.[10]thefollowing\\npoints determine the similarities. (1) Article texts are\\ndivided into three sections as headings, abstracts and\\nkeywords. (2) Abstract,keywords, based on the linkto the\\ntitle of article weighing. (3) The weighted mean is esti-\\nmated based on the description, abstract, and keyword and\\nuse Pearson’s correlation method to ﬁnd the similarity\\nbetween person and machine scores. They have 87%\\naccuracy in this proposed technique. Use a specialized\\nFig.1 Aframeworkforcomparativeanalysis\\nWordNetitcanalsoconcentrateonarticlesimilarities.The\\nproposedframeworkcanbeusedforothertextsthatrequire\\na WordNet of that language, such as texts in Persian and\\nother languages. M. Snover et al., explore a new way of\\nusingmonolingualtargetdatatoenhancetheefﬁciencyofa\\nstatisticalorpredictivemachinetranslationfornewsstories Jaccard similarity measures. The ﬁnal step in the frame-\\n[11].Thismethodemployscomparabletextvarioustextsin work is to compare and analyze the produced results. We\\nthe target language which explore the same or equivalent further explain each of the steps in detail.\\nstories as mentioned in the source language document. A The dataset used in this paper is known as ‘Google\\nlarge monolingual datasetfor each source documenttobe News’, and is publicly available [13]. Google News:\\ntranslated in the target language, which is searched for Google is offering a special experience to Google News\\ndocuments that may be similar to the source documents. which combines all its news items into one. It provides a\\nThe experimental results of this paper generated through constant, personalized ﬂow of newspapers from thousands\\nthedifferenceofthelanguageandtranslationmodelsshow ofpublishersandmagazinesgroupedaround.GoogleNews\\nvital improvements over the baseline framework. is a combination of global events, local news and news\\nQian et al. [12] using a comparable corpus, a bilingual stories that you’ve been reading. Then you can turn to\\ndependency mapping model for bilingual lexicon building Headlines to show top news from all over the world.\\nfrom English to Chinese. This model considers both Additional sections here allow you to delve into various\\ndependent words and their relationships when measuring topics such as sports, business and technology. And its\\nthe similarity between bilingual words and thus offers a greatest value is that this service delivered the news in 35\\nmore precise and less noisy representation. Author’s also languages so using Google news this experiment extracts\\nillustrated that bilingual dependency mappings can be the news articles in both Hindi and English languages.\\ncreated and optimized automatically without human input,\\ncontributing to a medium-sized set of dependency map- Headline and Link Extractor\\npings and that their impacts on Bilingual Lexicon Con-\\nstruction (BLC) can be fully exploited through weight A basic python library for searching and downloading live\\nlearningusingasimplebuteffectiveperceptronalgorithm, news articles from Google News feeds is GoogleNews or\\nmaking their approach quickly adaptable to several other gnewsclient [14]. Using this, one can pick up the top\\nlanguage pairs. headlines running on Google’s news websites or check for\\ntop headlines on a particular subject (or keyword). So this\\nexperiment can use this, to extract links from both Hindi\\nMethodology and English news that related to the same event.\\nThe major steps of the methodology are given below. Article Scraping\\nFigure 1 presents the framework of this work. The\\ntextual news data are ﬁrst pre-processed before it is rep- ‘Newspaper’isaPythonmoduleusedtoextractnewspaper\\nresented into a more structural format. The two represen- articles and to parse them. Newspapers are using special-\\ntationmethodsofgeneratingfeaturesfromthetextthatare ized Web scrapping algorithms to extract all the valuable\\ninvestigatedinthisstudyaretf-idf,andBagofWord.Once textfromawebsite.Thisworksextremelywellonwebsites\\nrepresented into these three representation methods, each of the online newspapers. This experiment has extracted\\nrepresented method is compared with three similarity links from both Hindi and English news, so now also\\nmeasures as shown in Fig. 1 i.e. Cosine, Euclidean and extract their text using the Newspaper module.\\n123\\nJ.Inst.Eng.IndiaSer.B\\nTranslator idf vectors have now been used as feature vectors to\\nmeasure the similarity between articles in news-results.\\nThrough using this package, Google offers a language\\ntranslation package for Python; words are taken from the Similarities Measures\\nHindi news articles and translated into different languages\\n(English language). Either Hindi corpus can be translated Similarityfunctionisareal-valuedfunctionthatcalculates\\nintoEnglishorEnglishcorpuscanbetranslatedintoHindi. the similarity between two items. The calculation of sim-\\nHere we have translated Hindi corpus into English. The ilarity is achieved by mapping distances to similarities\\ntranslation is performed at a level of the sentences. This withinthevectorspace.Thisexperimentprovidestwotests\\ntranslation also generates a map of words in various lan- ofsimilarity:cosinesimilarity,similaritywithJaccard,and\\nguages, from English. This research used bilingual dic- Euclidean distance.\\ntionaries ranging from Hindi to English. (1) Cosine Similarity It is a cosine angle in an n-di-\\nmensionalspace,betweentwon-dimensionalvectors.This\\nPre-processing and Data Cleaning isthedotproductofthetwovectors,dividedby-productof\\nthe two vectors’ lengths (or magnitudes) [16]. The simi-\\nPre-processingstepssuchastheeliminationofstop-words, larity of the cosine is measured by using the following\\nlemmatization, and parsing letters, punctuation marks, and formula:\\nnumbers have been completed. The words were lemma- A:B Pn A (cid:4)B\\ntized by WordNetLemmatizer and NLTK library took the similarityðA;BÞ¼jjAjj(cid:4)jjBjj¼pﬃPﬃﬃﬃﬃﬃnﬃﬃﬃiﬃ¼ﬃﬃAﬃ1ﬃﬃ2ﬃﬃpi ﬃPﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃBﬃﬃﬃ2ﬃﬃ\\nEnglish stop-words [15]. i¼1 i i¼1 i\\nð2Þ\\nVector Space Model\\nAsshowninFig. 2,supposethere istwopoint’sp1and\\np2, as the distance within these points increases the\\nAmathematicalmodelisalsocalledthetermvectormodel,\\nsimilarity between these points decreases and vice versa.\\nwhichdescribestextdocumentsasidentiﬁervariables,such\\n1(cid:5)CosineSimilarity¼CosineDistance\\nas terms or tokens. Of course, the term depends on the\\ncomparisons, but usually, only words, keywords or sen- Theresultoftheanglewillshow theresult.Ifthe angle\\ntences are compared. is0betweenthedocumentvectorsthenthecosinefunction\\nis 1 and both documents are the same. If the angel is any\\nFeature Vectors other value then the cosine function will be less than 1.\\nDoes the angle reach - 1 then the documents are\\nIn the Artiﬁcial Intelligence feature vector is an n-dimen- completely different? Thus this way by calculating the\\nsional vector of computational features that describe some cosine angle between the vectors of P1 and P2 decides if\\nentity. That is a really important method of calculating the vectors are pointing in the same direction or not.\\nsemanticsimilarityamongtexts.Methodswereusedduring (2) Jaccard Similarity Jaccard similarity calculates\\nthis experiment to measure the function vectors is TF-IDF similaritiesamongsets.It’sdeﬁnedastheintersectionsize\\n(Term Frequency-Inverse Document Frequency) is a sim-\\nple algorithm for transforming a text into a meaningful\\nrepresentation of numbers. Tf-idf weight is a measure of\\nfactwhichevaluatestheimportanceofaspeciﬁcwordina\\ntext. In mathematics,\\n(cid:2) (cid:3)\\nX N\\ntfidf weight¼ tf (cid:3)log ð1Þ\\ni;d df\\ni2d i\\nwhere in document d, tf is the number of occurrences of\\ni,d\\ntheithterm,df isthenumber ofdocumentswhichcontain\\ni\\nithterm; Nis the total number of documents.Thesklearn-\\nvectorized function wasused toconstructatf-idffunction.\\nThiswholemodelwasconstructedbyusingthedocuments,\\nandagroupofsuchtf-idfvectors wasgeneratedconsisting\\nof the tf idf weight of and term in the documents. Such tf-\\nFig.2 Cosinesimilarity\\n123\\nJ.Inst.Eng.IndiaSer.B\\ndivided bythe unionsize oftwo sets.Jaccard similitude is AC2 ¼AB2þBC2 ð6Þ\\ndetermined using the formula [16] below.\\npﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\\nT T AC ¼ AB2þBC2 ð7Þ\\nA B A B\\nJðA;BÞ¼ S ¼ T ð3Þ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\\nA B ‘jAjþjBj(cid:5)jA Bj AC ¼ ðx2(cid:5)x1Þ2þðy2(cid:5)y1Þ2 ð8Þ\\nwhere\\\\representsintersectionand[representstheunion. sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\\nm\\nInthispaper,AandBarebagsofwordsthatcontainnews jX !Yj¼ XðX (cid:5)YÞ2 ð9Þ\\ni i\\narticles.\\ni¼1\\n• Jaccard(A,A) = 1 Table 1 shows a comparative analysis of the methods\\nT\\n• Jaccard(A,B) = 0 if A B = 0 based on their relative pros and cons. The table also\\n• A and B don’t have to be the same size describes the application areas where the selected\\n• Always assign a number between 0 and 1. techniques can be used.\\nJaccard distance which instead of similarity measures\\nSimilarity Score\\ndissimilarity between can be found by subtracting Jaccard\\nsimilarity coefﬁcient from 1:\\nSimilarityscoremeansthattwodatasetsarehowsimilarto\\nJDðA;BÞ¼1(cid:5)JDðA;BÞ ð4Þ\\none another. The data collection will include two separate\\njA[Bj(cid:5)jA[Bj textsasinthiscase.Thesimilaritybetweenthetwotextsis\\nor JDðA;BÞ¼ ð5Þ\\njA[Bj evaluated according to the scoring system. Euclidean dis-\\ntance does not ﬁnd the similarity between the texts, but\\n(3) Euclidean Distance Another similarity measure in\\nﬁndsthemetric,thedistancebetweenbothtexts[18];there\\nthe vector space model is Euclidean distance or L2\\nare different ways to calculate similarity:\\ndistance, or Euclidean norm. This similarity measure\\n1\\ndifferentiates similarity measurements from the other SimilarityðA;BÞ¼ ð10Þ\\n1þDistanceðA;BÞ\\nvector space model by not judging from the angle like\\nthe rest but rather the direct distance between the vector\\nNoun Phrase Extraction\\ninputs.\\nAsshowninFig. 3,iftherearetwopointslike(X1,Y1)\\nNoun Phrase Extraction is a technique of text analysis,\\nand (X2, Y2) and let us consider any dimension point so if\\nconsistingoftheautomatedextractionofnounsinatext.It\\none wants to ﬁnd out the distance between (X1, Y1) and\\nhelps to summarize the contents of a text and identify the\\n(X2, Y2) then basically use this particular parameter like\\nkey topics being discussed. This paper concludes that the\\nEuclideandistancetocheckthatifthisparticularpointsare\\nextraction of the frequency of noun phrases and the fre-\\nnearertoeachotherthanitwillconsiderthatthistwo-point\\nquencyofthenextwordofthenounfromnewsarticlescan\\nare similar with each other. Euclidean distance is calcu-\\nconsiderably improve similarity measures. TextBlob is a\\nlatedbasedonthePythagorastheorem.LetDrepresentthe\\nPython module that is used to extract a noun [19].\\nmeasureofdistancesbetween(X1,Y1)and(X2,Y2).Hence\\nthe distance from A to C can be expressed as:\\nProposed Method\\nThis paper introduces two methods for calculating the\\nsimilaritybetweentwoarticlesofthesamenews,whichare\\npresent in two different languages (Hindi and English),\\nbased on methods for calculating the feature vector and\\nsimilarity measures.\\nCosine Similarity and Jaccard Similarity with TF-\\nIDF Vectors\\nThepre-processednewsarticleswereturnedintovectorsof\\ntf-idf by using a vectorized model of tf-idf. The vectors\\nobtainedwereasparse-matrixcontainingtf-idfweightsfor\\nnews article word having the dimensions of [number of\\nFig.3 Euclideandistance\\n123\\nJ.Inst.Eng.IndiaSer.B\\nTable1 Comparisonoftheprosandconsofdifferentmeasuresandtheirapplicationarea\\nSI. Similarity Pros Cons Applicationarea\\nmeasures\\n1 Cosine Bothcontinuousandcategoricalvariablesmaybe Doesn’tworkeffectively Textmining,documentsimilarity\\nsimilarity used withnominaldata[17]\\n2 Jaccard Bothcontinuousandcategoricalvariablesmaybe Doesn’tworkeffectively Documentclassiﬁcation\\ncoefﬁcient used withnominaldata\\n3 Euclidean EasytoComputeandworkwellwithadatasetwith Doesnotworkwithimage Applicationinvolvingintervaldata,DNA\\ndistance compactorisolatedclusters[17] dataefﬁciently analysis,K-meanalgorithm\\nnews articles * number of features (distinct words)] [16]. Bag of Words Euclidean Distance\\nThat tf-idf weight from the matrix was now used as a\\nfeatureforeverytext,andsimilarityamongnewsarticlesis The pre-processed documents have been described as a\\ncalculated using cosine similarity and Jaccard similarity. vector with the frequency of each word and compare how\\nSklearn’sbuilt-incosineandJaccardsimilaritymodulewas similar they are by comparing their bag of vector words.\\nused to measure the similarity. This experiment uses the bag-of-words model because the\\ncomputer processes vectors much faster than a vast ﬁle of\\nTable2 Samplepairofcompletelysimilarnews\\nTable3 Samplepairofdifferentnewsstoriesaboutthesametopic\\n123\\nJ.Inst.Eng.IndiaSer.B\\nTable4 Samplepairofcompletelydissimilarnews performedonpairsofnewsheadlineobtainedfromGoogle\\nNews[14].ThechosennewsarticlesarelistedinTables 2,\\n3and4.Thenewsarticlesweregiventoahumanexpertto\\njudge the similarity and dissimilarity. As a result, the\\nhuman expert has determined 6 pairs (pair 1–6) are com-\\npletely similar news and 5 pairs (pair 7–11) are different\\nnews about the same topic and the other 5 pairs (pair\\n12–16) are completely dissimilar news. The expert judg-\\nment is used as a benchmark to evaluate the automatic\\nsimilarity calculation on these news articles. The cosine\\nsimilarity, Jaccard coefﬁcient, and Euclidean distance are\\napplied. The result of all three measures is shown in\\nTables 5, 6 and 7.\\nToprovideabetterunderstandingofthethreecompared\\nmeasures,theresults areshownonabargraph asdepicted\\nin Fig. 4.\\nFigure 5 shows the similarity measures bar graph for\\ndifferent news stories about the same topic.\\nFigure 6 shows the similarity measures bar graph for\\ncompletely dissimilar news.\\nThe performance measures used in the experiment are\\naccuracy,precision,recallandF-measures.Thesemeasures\\nare calculated by determining the number of news articles\\ncorrectlyidentiﬁedassimilarordissimilarcomparedtothe\\ndecisionsbyhumanexperts[21].Inotherwords,usingthe\\nhuman decisions as a benchmark the number of true pos-\\nitive (TP) which is equivalent to actual similar news cor-\\nrectly identiﬁed as similar, true negative (TN) which is\\nequivalent to actual dissimilar news correctly identiﬁed as\\ndissimilar,falsepositive(FP)whichisequivalenttoactual\\nsimilar news incorrectly identiﬁed as dissimilar, and false-\\ntext for a lot of data [20]. So this paper load all news\\nnegative(FN)whichisequivalenttoactualdissimilarnews\\narticles in a list called corpus then calculate the feature\\nincorrectly identiﬁed as similar are determined. Then, the\\nvectors from the documents and ﬁnally compute the\\naccuracy is calculated as (TP ? TN)/all data, precision is\\nEuclideandistanceandthentocheckhowsimilartheyare.\\nTP/(TP ? FP),recallisTP/(TP ? FN)andtheF-measures\\nGreaterthedistance,lesssimilartheyare.Thispaperusesa\\nas the harmonic mean of precision and recall, which is\\nmodule or library called sklearn which is a machine\\nequal to 2TP/(2TP ? FP ? FN) [21]. The results are pre-\\nlearning library.\\nsented in the next section.\\nResults and Discussion\\nResult and Analysis\\nFigure 7 presents the graph of similarity measurements of\\nProposed algorithms are implemented using Python\\nthe sample pair of news articles using Euclidean, Jaccard\\n3.7.3(64-bit).Fortheexperiment,around1000newsstories\\nand cosine similarity measures for each representation\\nwererandomlypickedfromthedataset.Thealgorithmruns\\nschemes i.e. tf-idf, and a bag of word representation. As\\non that dataset, and it measures and compares the various\\ncan be learned from Fig. 7, Cosine performs similar to\\nsimilarity score. Every news article’s similarity has been\\nbenchmarkfornewswiththesamemeaning(pair1–6)and\\ncalculated against itself and every other article.\\ndifferent news about the same topic (pair 7–11) and how-\\neverforcompletelydissimilarnews(pair12–16)Jaccard’s\\nComparative Analysis\\nand Euclidean score are similar to the human benchmark.\\nToproveourpointfurther,wecalculatedthecorrelation\\nTo analyze the performance of the representation method\\nscores for each similarity measures against the human\\non different similarity measures, the experiment was\\nbenchmark as shown in Table 8.\\n123\\nJ.Inst.Eng.IndiaSer.B\\nTable5 Similaritymeasuresofcompletelysamenews Cosine Jaccard Euclidean\\n1\\nS.no. Cosinesimilarity Jaccardsimilarity Euclideansimilarity\\n0.8\\n1 0.8931 0.38075 0.04679\\n0.6\\n2 0.856 0.2134 0.02764\\n3 0.8476 0.3589 0.04861 0.4\\n4 0.7434 0.3289 0.04610 0.2\\n5 0.8034 0.2086 0.08609 0\\n6 0.7899 0.2756 0.02440 News 1 News 2 News 3 News 4 News 5\\nFig.5 Comparison of similarity coefﬁcients for different news\\narticlesaboutthesametopic\\nTable6 Similarity measures for different news stories about the\\nCosine Jaccard Euclidean\\nsametopic\\n1\\nS.no. Cosinesimilarity Jaccardsimilarity Euclideansimilarity\\n0.8\\n7 0.7063 0.1168 0.02311\\n0.6\\n8 0.5301 0.047 0.01377\\n0.4\\n9 0.5459 0.1516 0.04511\\n0.2\\n10 0.6316 0.1196 0.03771\\n0\\n11 0.7182 0.132 0.02990\\nNews 1 News 2 News 3 News 4 News 4\\nFig.6 Comparison of similarity coefﬁcients for completely dissim-\\nilarnews\\nTable7 Similaritymeasuresofcompletelydissimilarnews\\nS.no. Cosinesimilarity Jaccardsimilarity Euclideansimilarity\\nHuman Cosine Jaccard Euclidean\\n12 0.3447 0.066 0.0434 1\\n13 0.4032 0.0705 0.00804 es 0.8\\nur\\n14 0.4843 0.0996 0.0334 s\\nea 0.6\\n15 0.5490 0.1003 0.0298 M\\ny  0.4\\n16 0.3466 0.08503 0.02949 arit\\nmil 0.2\\nsi\\n0\\n1 2 3 4 5 6 7 8 9 10111213141516\\n Cosine Jaccard Euclidean News Hadlines\\n1\\nFig.7 Similarityscoregraph\\n0.8\\n0.6\\n0.4 (Tables 9, 10, 11) to ﬁnd out their accuracy, precision,\\nrecallandF-measuresasexplainedintheprevioussection.\\n0.2\\nTable 12 gives a clear picture of the performance of\\n0\\neach similarity measure. Analyzing the results we see that\\nNews 1 News 2 News  3 News 4 News 5 News 6\\nthePrecisionvalueofJaccardmeasuresis1.0or100%but\\nFig.4 Comparison of similarity coefﬁcients for articles of same less than 50% in Euclidean Distance. However, Euclidean\\nnews gives a high value of Recall as compared to Precision.\\nCosine measuregives a good accuracy level andF1 score,\\nFrom the correlation score in Table 8, it can be per- but the difference between Recall value and Precision is\\nceived that the Cosine and Jaccard similarity is more cor- high. But, among these three methods cosine similarity\\nrelated to the benchmark scores. We further analyze the usingtf-idfshowedgreateraccuracy,recallandF-measure\\nproduced result by calculating the Confusion Matrix [3] scores of 81.25%, 100% and 76.92%, respectively.\\n123\\nJ.Inst.Eng.IndiaSer.B\\nConclusion\\nTable8 Correlationofthesimilarityscorestothebenchmark\\nMethod Correlation\\nThis ongoing research conducted a comparison of three\\nCosineandbenchmark 0.919847 different methods to estimate the semantic similarity\\nJaccardandbenchmark 0.816131 among two news articles on (nearly) the same topic/event\\nEuclideanandbenchmark 0.422671 to measure the similarity between them in two different\\nlanguages (Hindi and English). The experiment was tested\\nusing the GoogleNews data sets. The three methodologies\\narethesimilarityofCosinewithtf-idfvectors,similarityof\\nJaccard with tf-idf vectors, Bag of words Euclidean dis-\\nTable9 Confusionmatrixforcosinesimilarity tance.Allthreeofthesemethodsshowedpromisingresults,\\nbut among these three methods, cosine similarity using tf-\\n16News Predicted:No Predicted:Yes\\nidf showed greater accuracy, recall and F-measure scores\\nTotal: 8 8 of 81.25%, 100% and 76.92%, respectively. The accuracy\\nActual:No TN=8 FP=3 of the other two methods may be improved with the\\nActual:Yes FN=0 TP=5 Doc2Vec model [6], which takes text corpus as input and\\nThresholdvalue:0.788 generates document vectors as output. This experiment is\\nTotalreﬁnednews:8 also looking to expand the work to other languages.\\nTable10 ConfusionmatrixforJaccardsimilarity References\\n16News Predicted:No Predicted:Yes\\n1. G.Atkins,M.Weigle,andM.Nelson,Measuringnewssimilarity\\nacrosstenU.S.newssites,arXivpreprintarXiv,pp.1–11,2018\\nTotal: 12 4\\n2. J. Gibson, B. Wellner, and S. Lubar, Identiﬁcation of duplicate\\nActual:No TN=8 FP=0\\nnews stories in web pages, in The MITRE Corporation 202\\nActual:Yes FN=4 TP=4 Burlington Rd. Bedford MA 01730 USA, 202 Burlington Rd.\\nThresholdvalue:0.245 BedfordMA01730USA,2008\\n3. M. Singh, D.A. Kumar, D.V. Goyal, Review of techniques for\\nTotalreﬁnednews:4\\nextraction of bilingual lexicon from comparable corpora. Int.\\nJ.Eng.Technol.7(2),16–20(2018)\\n4. S.Montalvo,R.Mart´ınez,A.Casilla,BilingualNewsClustering\\nUsing Named Entities and Fuzzy Similarity (Springer, Heidel-\\nTable11 ConfusionmatrixforEuclideansimilarity berg,2007),pp.108–114\\n5. S. Mohd Saad and s. S. Kamarudin, Comparative analysis of\\nNews Predicted:No Predicted:Yes\\nsimilarity measuresforsentence levelsemanticmeasurement of\\nTotal: 8 8 text. in IEEE International Conference on Control System,\\nComputingandEngineering,pp.90–94,2013\\nActual:No TN=8 FP=7\\n6. P. Sitikhu, A Comparison of Semantic Similarity Methods for\\nActual:Yes FN=0 TP=1 Maximum Human Interpretability, 2019. arXiv:1910.09129v2\\nThresholdvalue:0.0529 [cs.IR]\\n7. K. Baraniak and M. Sydow, News articles similarity for auto-\\nTotalreﬁnedNews:8\\nmaticmediabiasdetectioninPolishnewsportals,inProceedings\\nof the Federated Conference on Computer Science and Infor-\\nmationSystems,2018\\n8. M. B. Magara and T. Zuva, A comparative analysis of text\\nsimilarity measures and algorithms in research paper recom-\\nmendersystems,inConferenceonInformationCommunications\\nTable12 Accuracylevelofeachsimilaritymeasures\\nTechnologyandSociety(ICTAS),2018\\nSimilaritymeasures Performancemeasures 9. V. Thada, D.V. Jaglan, Comparison of Jaccard, dice, cosine\\nsimilarity coefﬁcient to ﬁnd best ﬁtness value for web retrieved\\nAccuracy Precision Recall F-Measure documents using genetic algorithm.Int. J. Innov.Eng. Technol.\\n(IJIET)2(4),202–204(2013)\\nCosine 0.8125 62.5 1.0 0.7692 10. M.I.Nasab,Anewapproachforﬁndingsemanticsimilarscien-\\nJaccard 0.750 1.0 0.50 0.666 tiﬁc articles. J. Adv. Comput. Sci. Technol. (JACST) 4, 563-59\\nEuclidean 0.5625 0.125 1.0 0.222 (2015)\\n11. M.Snover,B.Dorr,andR.Schwartz, Languageandtranslation\\nTheHighestvalueisshowninbold model adaptation using comparable corpora, in Proceedings of\\n123\\nJ.Inst.Eng.IndiaSer.B\\nthe2008ConferenceonEmpiricalMethodsinNaturalLanguage https://dataaspirant.com/2015/04/11/ﬁve-most-popular-\\nProcessing,Honolulu,2008 similarity-measures-implementation-in-python/\\n12. Q.LongHuaandW.HongLing,Bilinguallexiconconstruction 17. M. Goswami, A. Babu, B. Purkayastha, A comparative analysis\\nfromcomparable corpora viadependencymapping, inProceed- of similarity measures to. Int. J. Manag. Technol. Eng. 8(XI),\\ningsofCOLING,2012 786–797(2018)\\n13. Y.YDaniDeahl@danideahl,GoogleNewsisgettinganoverhaul 18. A.Ali,Textualsimilarity,ISSN2011-19,2011\\nandcustomizednewsfeeds,THEVERGE,8May2018.[Online]. 19. TextBlob: simpliﬁed text processing, [Online]. Available:\\nAvailable: https://textblob.readthedocs.io/en/dev/\\nhttps://www.theverge.com/2018/5/8/17329074/google-news- 20. Bag of words Euclidean distance, [Online]. Available:\\nupdate-new-features-newsstand-io-2018 https://pythonprogramminglanguage.com/bag-of-words-\\n14. H. Hu, ‘‘GoogleNews.PyPI,’’ PyPI.org, Mar 13, 2020. [Online]. Euclidean-distance/\\nAvailable:https://pypi.org/project/GoogleNews/ 21. S.S. Kamaruddi, Graph-based representation for sentence simi-\\n15. J.Brownlee,Howtocleantextformachinelearningwithpython, larity measure: a comparative analysis. Int. J. Eng. Technol.\\nMachine Learning Mastery, October 18, 2017. [Online]. Avail- 7(2.4),32–35(2018)\\nable:\\nhttps://machinelearningmastery.com/clean-text-machine- Publisher’s Note Springer Nature remains neutral with regard to\\nlearning-python/ jurisdictionalclaimsinpublishedmapsandinstitutionalafﬁliations.\\n16. S. Polamuri, Five most popular similarity measures implemen-\\n123\\n'],\n",
       "      dtype='<U60629')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6e0718ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_corpus = u.normalize_corpus(corpus)\n",
    "norm_corpus = u.preprocess_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e6a95333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ieeeacm st international conference software engineering icse ieeeacm st international conference software engineering icse guiding deep learning system testing using surprise adequacy jinhan kim robert feldt shin yoo school computing dept computer science engineering dept software engineering kaist chalmers university blekinge inst technology daejeon republic korea gothenburg sweden karlskrona sweden jinhankimshinyookaistackr robertfeldtchalmersse robertfeldtbthse abstractdeep learning dl systems rapidly anothervehicletoyieldinoneoftherarercircumstancesand adopted safety security critical domains urgently calling crashed vehicle expectation proved ways test correctness robustness testing incorrect urgent need verify validate dl systems traditionally relied manual collection behaviours dl systems however signicant part labelling data recently number coverage criteria based neuron activation values proposed criteria existing software testing technique directly applicable essentiallycountthenumberofneuronswhoseactivationduring dl systems notably traditional whitebox testing execution dl system satised certain properties techniques aim increase structural coverage predened thresholds however existing cover veryusefulfordlsystemsastheirbehaviourisnotexplicitly age criteria sufciently ne grained capture subtle encoded control ow structures behavioursexhibitedbydlsystemsmoreoverevaluationshave focused showing correlation adversarial examples number novel approaches towards testing veri proposed criteria rather evaluating guiding cation dl systems recently proposed use actual testing dl systems propose novel test gap techniques adequacy criterion testing dl systems called surprise share two assumptions rst assumption essentially adequacy deep learning systems sadl based generalisation essence metamorphic testing behaviour dl systems respect training data measure surprise input difference two inputs dl system similar respect dl systems behaviour input training humansensetheoutputsshouldalsobesimilarforexample data ie learnt training subsequently deeptest checks whether autonomous driving system developthisasanadequacycriterionagoodtestinputshouldbe behavesinthesamewaywhentheinputimageistransformed sufcientlybutnotovertlysurprisingcomparedtotrainingdata asifthesamesceneisunderadifferentweatherconditionthe empirical evaluation using range dl systems simple image classiers autonomous driving car platforms shows second assumption also based traditional software systematic sampling inputs based surprise testing results diverse set input improveclassicationaccuracyofdlsystemsagainstadversarial effective testing dl system one perform examples via retraining forexampledeepxplorepresentedtheneuroncoverage index termstest adequacy deep learning systems ratio neurons whose activation values predened threshold measure diversity neuron introduction behaviour subsequently showed inputs violating deeplearningdlsystemshaveachievedsignicant rst assumption also increase neuron coverage progress many domains including image recognition recently introduced techniques made sig speech recognition machine transla nicant advances manual ad hoc testing dl systems tion based capability match even major limitation coverage criteria proposed surpass human performance dl systems increasingly fararenotsufcientlynegrainedinasensethatallofthem adopted part larger systems safety simply count neurons whose activation values satisfy certain securitycriticaldomainssuchasautonomousdriving conditions aggregation counting allow malware detection tester quantify test effectiveness given input adoption dl systems calls new challenges setitconveyslittleinformationaboutindividualinputsfor critically important larger systems exampleitisnotimmediatelyclearwhenaninputwithhigher correctandpredictable despitetheirimpressive experimental nc considered better another lower nc performances dl systems known exhibit unexpected certain inputs may naturally activate neurons behaviours certain circumstances example threshold others vice versa another reported incident autonomous driving vehicle expected example kmultisection neuron coverage 22255 // 1199 //$$ 3311 .. 0000 ©© 22001199 iieeeeee 11003399 ddooii 1100 .. 11110099 // iiccssee .. 22001199 .. 0000110088 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . partitionstherangesofactivationvaluesofneurons , observed ing additional adversarial examples , sampling additional training , k buckets , count number total inputs broader sa values improve accuracy bucketscoveredbyasetofinputs . whenmeasuredforasingle retraining 77 . 5 %. 1 input , thecoveragewillbeeither iftheinputactivateseach • weundertakeallourexperimentsusingpubliclyavailable k neuronwithavaluefromoneofthekbuckets , orsmallerthan dlsystemsrangingfromsmallbenchmarks ( mnistand thatifsomeneuronsactivateoutsidetherangeobservedduring cifar - 10 ) large system autonomous driving training . , theinformationabouthowfarsuchactivations vehicles ( dave - 2 [ 6 ] chauffeur [ 1 ]). implemen - go beyond observed range lost aggregation , making tations available online . 2 hard evaluate relative value input . test remaining paper organised follows . sec - adequacy criterion practically useful , able tion ii introduces surprise adequacy dl systems , sadl : toguidetheselectionofindividualinputs , eventuallyresulting two variants sadl presented along algorithms improvements accuracy dl system measure . section iii sets research questions investigation . section iv describes experimental set - overcome limitations , propose new test empirical evaluations . section v presents results adequacy dl systems , called surprise adequacy dl empiricalevaluations . sectionviaddressesthreatstovalidity . systems ( sadl ). intuitively , good test input set sectionviipresentsrelatedwork , andsectionviiiconcludes . dl system systematically diversiﬁed include inputs ranging similar training data ii . surpriseadequacyfordeeplearningsystems signiﬁcantly different adversarial . 1 individual input existing test adequacy criteria dl systems aim granularity , sadl measures surprising input measure diversity input set . neuron coverage dl system respect data system trained ( nc ) [ 34 ] posits higher number neurons : theactualmeasureofsurprisecanbeeitherbasedonthe activated predeﬁned threshold , diverse likelihood system seen similar input inputthedlsystemhasbeenexecutedwith . deepgauge [ 27 ] training ( respect probability density distributions proposed range ﬁner grained adequacy criteria including extrapolated training process using kernel density k - multisection neuron coverage , measures ratio estimation [ 41 ]), distance vectors representing activation value buckets covered across theneuronactivationtracesofthegiveninputandthetraining neurons , andneuronboundarycoverage , whichmeasuresthe data ( heresimplyusingeuclideandistance ). subsequently , ratioofneuronsthatareactivatedbeyondtherangesobserved surpriseadequacy ( sa ) ofasetoftestinputsismeasuredby training . therangeofindividualsurprisevaluesthesetcovers . weshow argue diversity testing dl systems sadl sufﬁciently ﬁne grained training adversarial meaningful measured respect training example classiﬁers based sadl values produce data , asdlsystemsarelikelytobemoreerrorproneforinputs higher accuracy compared state art . also unfamiliar , . e ., diverse . furthermore , neuron show sampling inputs according sadl retraining activation thresholds , beyond observed ranges , may dl systems result higher accuracy , thus showing closely related diversity given input , sadlisanindependentvariablethatcanpositivelyaffectthe measure degree activations network effectiveness dl system testing . one input differs activations another input . technical contributions paper follows : fundamentally discretisations utilize fact • propose sadl , ﬁne grained test adequacy metric neuron activations continuous quantities . contrast , thatmeasuresthesurpriseofaninput , . e ., thedifference aim deﬁne adequacy criterion quantitatively behaviour dl system given input measures behavioural differences observed given set training data . two concrete instances sadl inputs , relative training data . areproposedbasedondifferentwaystoquantifysurprise . . activation trace surprise adequacy shown correlated existing coverage criteria dl systems . let n = { n1 , n2 ,...} set neurons constitutes • show sadl sufﬁciently ﬁne grained cap - dl system , let x ={ x1 , x2 ,...} set inputs . turing behaviour dl systems training highly denote activation value single neuron n accurate adversarial example classiﬁer . adversarial respect input x αn ( x ). ordered ( sub ) set example classiﬁer shows much 100 % 94 . 53 % neurons , let n ⊆ n , αn ( x ) denote vector activation roc - auc score applied mnist [ 25 ] values , eachelementcorrespondingtoanindividualneuronin cifar - 10 [ 21 ] dataset , respectively . n : cardinality αn ( x ) equal | n |. call αn ( x ) • show sadl metrics used sample activation trace ( ) x neurons n . similarly , effective test input sets . retraining dl systems us - letan ( x ) beasetofactivationtraces , observedoverneurons inn , forasetofinputsx : ( x )={ αn ( x )| x ∈ x }. 1experimentsshowbeneﬁtsofdiversityforgeneraltesting [ 15 ] andbeneﬁts ofa ‘ scaleofdistances ’ oftestinputsforrobustnesstestingintroducedin [ 35 ]. 2pleaserefertohttps :// github . com / coinse / sadl . 11004400 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . note activation trace trivially available since want measure surprise input x , execution network given input . needametricthatincreaseswhenprobabilitydensitydecreases since behaviours dl systems driven along data - ( . e ., input rarer compared training data ), ﬂow control - ﬂow , assume activation traces vice versa ( . e ., input similar training data ). observedoverallnwithrespecttox , ( x ), fullycaptures adopting common approach converting probability density behaviours dl system investigation measure rareness [ 26 ], [ 39 ], deﬁne lsa executed using x . 3 negative log density : surprise adequacy ( sa ) aims measure relative nov - elty ( . e ., surprise ) given new input respect lsa ( x )=− log ( fˆ ( x )) ( 2 ) inputs used training . given training set , ﬁrst computean ( ) byrecordingactivationvaluesofallneurons note extra information input types used tomakelsamoreprecise . forexample , givenadlclassiﬁer usingeveryinputinthetrainingdataset . subsequently , givena , expect inputs share class label newinputx , wemeasurehowsurprisingxiswhencompared comparing activation trace x ( ). sreimplialcairnagtts . wwiethca { nxex ∈ pltoit | thdis ( xb ) = comc } puftoinrgcllasssacp . ewrcelaussse , quantitative similarity measure called surprise adequacy per - class lsa dl classiﬁers empirical evaluation . ( sa ). introduce two variants sa , different way measuring similarity x ( ). 4 note certain types dl tasks allow us focus x1b x2b partsofthetrainingsetttogetmorepreciseandmeaningful measurement sa . example , suppose testing c2 boundary classiﬁer new input x , classiﬁed dl x1 b1 b2 learnt dl system investigation class c . case , c1 a1 x2 surpriseofxismoremeaningfullymeasuredagainstan ( tc ), x1a a2 tc subset members classiﬁed x2a c . basically , input might surprising example fig . 1 : anexampleofdistance - basedsa . blackdotsrepresent class c even surprising relation full set ats training data inputs , whereas grey dots represent ats training examples . ofnewinputs , x1andx2 . comparedtodistancesfromx1aand b . likelihood - based surprise adequacy x2a toclassc2 , atofx1 isfartheroutfromclassc1 thanthat kernel density estimation ( kde ) [ 41 ] way esti - ofx2 , . e ., ab11 > ab22 ( seeequations3 , 4 , and5 ). consequently , decide x1 surprising x2 w . r . . class c1 . mating probability density function given random variable . resulting density function allows estimation ofrelativelikelihoodofaspeciﬁcvalueoftherandomvariable . c . distance - based surprise adequacy likelihood - based sa ( lsa ) uses kde estimate proba - analternativetolsaissimplytousethedistancebetween bility density activation value ( ), obtains ats measure surprise . , deﬁne distance - surprise new input respect estimated basedsurprise adequacy ( dsa ) usingtheeuclideandistance density . extension existing work uses kde new input x ats observed detect adversarial examples [ 14 ]. reduce dimensionality training . distance metric , dsa ideally suited computational cost , consider neurons exploit boundaries inputs , seen selected layer nl ⊆ n , yields set activation classiﬁcationexampleinfigure1 . bycomparingthedistances traces , anl ( x ). reduce computational cost , a1 a2 ( . e ., distance new input ﬁlter neurons whose activation values show variance thereferencepoint , whichisthenearestatoftrainingdatain lowerthanapre - deﬁnedthreshold , , astheseneuronswillnot c1 ) distances b1 b2 ( . e ., distance c2 measured contribute much information kde . cardinality reference point ), get sense close class tracewillbe | nl |. givenabandwidthmatrixh andgaussian boundary new inputs . posit , classiﬁcation kernelfunctionk , theactivationtraceofthenewinputx , problems , inputs closer class boundaries xi ∈ , kde produces density function fˆas follows : surprising valuable terms test input diversity . hand , tasks without boundaries ( cid : 2 ) 1 inputs , prediction appropriate steering angle fˆ ( x )= | anl ( )| xi ∈ tkh ( αnl ( x )− αnl ( xi )) ( 1 ) awuittohnonmoocluasssdbriovuinngdacriaers ,, dansaatmoafyannoetwbienpeuatsibleyinagppfalircfarbolme . 3for sake simplicity , assume possible get another training input guarantee new completeactivationtracesfromalltheneuronsinadlsystem . fornetwork input surprising , may still located crowded architectures loops , recurrent neural nets ( rnns ) [ 18 ], parts space . consequently , apply dsa possibletounrolltheloopsuptoapredeﬁnedbound [ 40 ]. classiﬁcation tasks , effective 4however , themainideaisgeneralandother , speciﬁcvariantswouldresult ifusingothersimilarityfunctions . lsa ( see section v - v - b details ). 11004411 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . let us assume dl system , consists surprising . however , input arbitrarily high sa value set neurons n , trained classiﬁcation task may simply irrelevant , least less interesting , set classes c , using training dataset . given set problem domain ( e . g ., image trafﬁc sign activationtracesan ( ), anewinputx , andapredictedclass irrelevant testing animal photo classiﬁers ). , new input cx ∈ c , deﬁne reference point xa sc measured respect pre - deﬁned upper closest neighbour x shares class . bound , inthesamewaythetheoreticallyinﬁnitepathcoverage distance x xa follows deﬁnition : bounded parameter [ 44 ]. second , sc render itselftoacombinatorialsetcoverproblem , whichthetestsuite xa = argmin ( cid : 4 ) αn ( x )− αn ( xi )( cid : 4 ), minimisation often formulated [ 43 ]. ( xi )= cx ( 3 ) single input yields single sa value cannot belong dista =( cid : 4 ) αn ( x )− αn ( xa )( cid : 4 ) tomultiplesabuckets . thesenseofredundancywithrespect sc coverage criteria weaker structural subsequently , fromxa , weﬁndtheclosestneighbourofxa coverage , single input cover multiple targets . inaclassotherthancx , xb , andthedistancedistb , asfollows : aim show sa guide better selection inputs , rigorous study optimisation test suites dl xb = argmin ( cid : 4 ) αn ( xa )− αn ( xi )( cid : 4 ), systemsremainsafuturework . however , asweshowwithour ( xi )∈ c \\\\{ cx } ( 4 ) empirical studies , sc still guide test input selection . distb =( cid : 4 ) αn ( xa )− αn ( xb )( cid : 4 ) iii . researchquestions intuitively , dsa aims compare distance new input x known ats belonging class , empirical evaluation designed answer follow - cx , known distance ats class cx ats ing research questions . classes c \\\\{ cx }. former relatively larger rq1 . surprise : sadl capable capturing relative latter , x would surprising input class cx surprise input dl system ? classifying dl system . multiple ways formalise select simple one calculate dsa provide answers rq1 different angles . first , ratio dista distb . investigation wecomputethesaofeachtestinputincludedintheoriginal complicated formulations left future work . dataset , see dl classiﬁer ﬁnds inputs higher surprise difﬁcult correctly classify . expect dsa ( x )= dista ( 5 ) surprising input harder correctly classify . second , distb evaluate whether possible detect adversarial examples . surprise coverage based sa values , expect adversarial examples surprising well cause different behaviours givenaset ofinputs , wecanalsomeasuretherangeofsa dl systems . using different techniques , multiple sets values set covers , called surprise coverage ( sc ). since adversarial examples generated compared lsa dsa deﬁned continuous spaces , sa values . finally , train adversarial example classiﬁers use bucketing discretise space surprise deﬁne using logistic regression sa values . adversarial bothlikelihood - basedsurprisecoverage ( lsc ) anddistance - attackstrategy , wegenerate10 , 000adversarialexamplesusing based surprise coverage ( dsc ). given upper bound u , buckets b ={ b1 , b2 ,..., bn } divide ( 0 , u ] n sa 10 , 000 original test images provided mnist cifar - 10 . using 1 , 000 original test images 1 , 000 adversarial segments , sc set inputs x deﬁned follows : examples , allchosenrandomly , wetrainthelogisticregression classiﬁers . finally , weevaluatethetrainedclassiﬁersusingthe sc ( x )= |{ bi |∃ x ∈ x : sa ( x )∈( u · − n1 , u · ni ]}| ( 6 ) remaining 9 , 000 original test images 9 , 000 adversarial n examples . ifsavaluescorrectlycapture thebehaviourofdl set inputs high sc diverse set inputs systems , expect sa based classiﬁers successfully ranging similar seen training ( . e ., low detect adversarial examples . use area curve sa ) different seen training receiver operator characteristics ( roc - auc ) evaluation ( . e ., high sa ). argue input set dl system captures true false positive rates [ 8 ]. diversiﬁed , systematically diversiﬁed rq2 . layer sensitivity : selection layers considering sa . recent results also validate notion neurons used sa computation impact showingthatmoredistanttestinputsweremorelikelytolead accurately sa reﬂects behaviour dl systems ? exceptions might relevant testing [ 35 ]. whileweusethetermcoverandcoverage , theimplications bengio et al . suggest deeper layers represent higher ofsabasedcoverageisdifferentfromthetraditionalstructural levelfeaturesoftheinput [ 5 ]: subsequentworkthatintroduced coverage . first , unlikemostofthestructuralcoveragecriteria , kde based adversarial example detection technique [ 14 ] ﬁnite set targets cover , statement assumes deepest ( . e ., last hidden ) layer contain branchcoverage : aninputcan , atleastintheory , bearbitrarily information helpful detection . evaluate 11004422 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . table : list datasets models used study . dataset description dnnmodel # ofneuron syntheticinputs performance mnist handwritten digit images composed aﬁvelayerconvnetwith 320 fgsm , bim - , bim - 99 . 31 % ( ac - 50 , 000imagesfortrainingand10 , 000im - max - pooling dropout b , jsma , c & w . curacy ) agesfortest . layers . cifar - 10 object recognition dataset ten different 12 layer convnet 2 , 208 fgsm , bim - , bim - 82 . 27 % ( ac - classes composed 50 , 000 images max - pooling dropout b , jsma , c & w . curacy ) trainingand10 , 000imagesfortest . layers . udacity self - drivingcardatasetthatcontains dave - 2 [ 6 ] architecture 1 , 560 deepxplore ’ test 0 . 09 ( mse ) self - drivingcar cameraimagesfromthevehicle , composed fromnvidia . input generation via challenge of101 , 396imagesfortrainingand5 , 614 jointoptimization . imagesfortest . thegoalofthechallenge chauffeur [ 1 ] architecture 1 , 940 deeptest ’ combined 0 . 10 ( mse ) istopredictsteeringwheelangle . withcnnandlstm . transformation . assumption context sa calculating lsa u upper bound used rq3 compute sc , dsa individual layers , subsequently comparing divide range sa [ 0 , u ] four overlapping subsets : adversarialexampleclassiﬁerstrainedonsafromeachlayer . ﬁrst subset including low 25 % sa values ([ 0 , u ]), 4 2u rq3 . correlation : sc correlated existing coverage second including lower half ([ 0 , 4 ]), third including 3u criteria dl systems ? lower 75 % ([ 0 , 4 ]), ﬁnally entire range , [ 0 , u ]. thesefoursubsetsareexpectedtorepresentincreasinglymore addition capturing input surprise , want sc diversesetsofinputs . wesettherangertooneofthesefour , consistent existing coverage criteria based counting randomly sample 100 images r , train existing aggregation . ifnot , thereisariskthatscisinfactmeasuring models ﬁve additional epochs . finally , measure something input diversity . , check model ’ performance ( accuracy mnist cifar - 10 , whether sc correlated criteria . control mse dave - 2 ) entire adversarial synthetic input diversity cumulatively adding inputs generated inputs , respectively . expect retraining diverse differentmethod ( . e ., differentadversarialexamplegeneration subset result higher performance . techniques input synthesis techniques ), execute studied dl systems input , compare observed iv . experimentalsetup changes various coverage criteria including sc four evaluate sadl four different dl systems using ( ) existing ones : deepxplore ’ neuron coverage ( nc ) [ 40 ] original test sets , ( b ) adversarial examples generated three neuron - level coverages ( nlcs ) introduced deep - ﬁve attack strategies , ( c ) synthetic inputs generated gauge [ 27 ]: k - multisection neuron coverage ( kmnc ), neu - deepxplore [ 34 ] deeptest [ 40 ]. section describes ronboundarycoverage ( nbc ), andstrongneuronactivation studied dl systems input generation methods . coverage ( snac ). mnist cifar - 10 , start original test . datasets dl systems data provided dataset ( 10 , 000 images ), add 1 , 000 tableiliststhesubjectdatasetsandmodelsofdlsystems . adversarial examples , generated fgsm , bim - , bim - b , mnist [ 25 ] cifar - 10 [ 21 ] widely used datasets jsma , andc & w , ateachstep . fordave - 2 , westartfromthe machine learning research , collection originaltestdata ( 5 , 614images ) andadd700syntheticimages images ten different classes . mnist , adopt generated deepxplore step . chauffeur , widely studied ﬁve layer convolutional neural network stepadds1 , 000syntheticimages ( set1toset3 ), eachproduced ( convnet ) withmax - poolinganddropoutlayersandtrainitto applying random number deeptest transformations . achieve 99 . 31 % accuracy provided test set . similarly , adopted model cifar 12 - layer convnet rq4 . guidance : sa guide retraining dl systems max - pooling dropout layers , trained achieve 82 . 27 % improve accuracy adversarial examples accuracy provided test set . synthetic test inputs generated deepxplore ? evaluation sadl dl systems safety criti - evaluate whether sadl guide additional training cal domains , use udacity self - driving car challenge existing dl systems aim improved accuracy dataset [ 2 ], contains collection camera images againstadversarialexamples , weaskwhethersacanguidethe driving car . aim predict steering wheel selectionofinputforadditionaltraining . fromtheadversarial angle , model accuracy measured using mean squared examplesandsynthesisedinputsforthesemodels5 , wechoose error ( mse ) actual predicted steering angles . four sets 100 images four different sa ranges . given use pre - trained dave - 2 model [ 6 ], public artefactprovidedbydeepxplore6 , andapre - trainedchauffeur 5we could resume training chauffeur model additional ﬁve epochs , whichiswhyitisabsentfromrq4 . 6deepxploreisavailablefrom : https :// github . com / peikexin9 / deepxplore . 11004433 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . model [ 1 ], madepubliclyavailablebytheudacityself - driving neurons computationally infeasible due precision car challenge . dave - 2 consists nine layers including ﬁve loss . rq2 , set activation variance threshold convolutional layers , achieves 0 . 09 mse . chauffeur layers activation 7 activation 8 cifar - 10 10 − 4 , consists convnet lstm sub - model , reduces number neurons used computa - achieves 0 . 10 mse . tion lsa , consequently , computational cost . computation coverage criteria rq3 , use b . adversarial examples synthetic inputs conﬁgurations table ii . threshold nc set 0 . 5 . sadl evaluated using adversarial examples fornlcs , weallsetthenumberofsections ( k ) to1 , 000 . synthetic test inputs . adversarial examples crafted ap - lsc dsc , manually choose layer , number plying , totheoriginalinput , smallperturbationsimperceptible buckets ( n ), upper bound ( ub ). rq4 , humans , dl system investigation behaves layers chosen mnist cifar - 10 activation 3 incorrectly [ 16 ]. werelyonadversarialattackstogeneratein - activation 5respectively . weperform20runsofretrainingfor putimagesformnistandcifar - 10 : thesegeneratedimages subject report statistics . likely reveal robustness issues dl systems allexperimentswereperformedonmachinesequippedwith test inputs provided original datasets . use inteli7 - 8700cpu , 32gbram , runningubuntu16 . 04 . 4lts . ﬁve widely studied attack strategies evaluate sadl : fast mnist cifar - 10 implemented using keras v . 2 . 2 . 0 . gradient sign method ( fgsm ) [ 16 ], basic iterative method ( bim - , bim - b ) [ 23 ], jacobian - based saliency map attack v . result ( jsma ) [ 33 ], carlini & wagner ( c & w ) [ 9 ]. imple - duetothespacelimit , wecannotincludeallplotsandtables mentation strategies based cleverhans [ 32 ] make available online : https :// coinse . github . io / sadl . framework et al . [ 30 ]. . input surprise ( rq1 ) dave - 2 chauffeur , use state - - - art synthetic input generation algorithms , deepxplore [ 34 ] figure 2 shows classiﬁcation accuracy changes deeptest [ 40 ]. bothalgorithmsaredesignedtosynthesisenew classify sets images growing sizes test input existing ones aim detecting erro - test inputs included mnist cifar - 10 dataset . neous behaviours autonomous driving vehicle . dave - 2 , sets images corresponding red dots ( ascending use deepxplore ’ input generation via joint optimization sa ) start images lowest sa , increasingly algorithm , whose aim generate inputs lead multiple include images higher sa ascending order sa ; dlsystemstrainedindependently , butusingthesametraining sets images corresponding blue dots grow data , disagree . using dave - 2 two opposite direction ( . e ., images highest sa variants , dave - dropout dave - norminit , collect syn - lower sa ). reference , green dots show mean theticinputsgeneratedbylightingeffect ( light ), occlusionby accuracy randomly growing sets across 20 repetitions . asingleblackrectangle ( singleocc ), andocclusionbymultiple clear including images higher lsa values , . e ., black rectangles ( multiocc ). chauffeur , synthesise surprising images , leads lower accuracy . visual new inputs iteratively applying random transformations conﬁrmation another dataset , also chose sets inputs provided deeptest original input images : translation , synthesised chauffeur deeptest , three distinct scale , shear , rotation , contrast , brightness , blur . 7 levelsoflsavalues : figure3showsthatthehigherthelsa values , harder recognise images visually . table ii : conﬁgurations rq3 . quantitatively visually , observed trend supports claim sadl captures input surprise : even unseen dnn nc nlcs lsc dsc model th k layer n ub n ub inputs , sa measure surprising given input , mnist 0 . 5 1 , 000 activation 3 1 , 000 2 , 000 1 , 000 2 . 0 whichisdirectlyrelatedtotheperformanceofthedlsystem . cifar - 10 0 . 5 1 , 000 activation 3 1 , 000 100 1 , 000 2 . 0 figure4showsplotsofsorteddsavaluesof10 , 000adver - dave - 2 0 . 5 1 , 000 block1 conv2 1 , 000 150 n / chauffeur 0 . 5 1 , 000 convolution2d 11 1 , 000 5 n / sarial examples , generated ﬁve techniques , wellastheoriginaltestinputs . figure5containssimilarplots based lsa values 2 , 000 randomly selected adversarial c . conﬁgurations examples original test set , different layers research questions , default activation variance mnist cifar - 10 . mnist cifar - 10 , threshold lsa set 10 − 5 , bandwidth test inputs provided datasets ( represented blue kde set using scott ’ rule [ 36 ]. remaining colour ) tend least surprising , whereas majority section details rq speciﬁc conﬁgurations . rq1 , use adversarial examples clearly separated test theactivation 2layerformnist , andactivation 6forcifar - inputsbytheirhighersavalues . thissupportsourclaimthat 10 , computing lsa values . computation lsa based sadl capture differences dl system ’ behaviours adversarial examples . 7atthetimeofourexperiments , thepubliclyavailableversionofdeeptest finally , table iii shows roc - auc results dsa - didnotinternallysupportrealisticimagetransformationssuchasfogandrain effects . based classiﬁcation using neurons mnist cifar - 11004444 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . ( ) lowlsa ( ) selectedtestinputsbasedonlsainmnist ( b ) mediumlsa ( c ) highlsa ( b ) selectedtestinputsbasedondsainmnist fig . 3 : synthetic images chauffeur model generated deeptest . images higher lsa values tend harder recognise interpret visually . ( c ) selectedtestinputsbasedonlsaincifar - 10 ( ) selectedtestinputsbasedondsaincifar - 10 fig . 2 : accuracy test inputs mnist cifar - 10 dataset , selected input lowest sa , increas - fig . 4 : sorteddsavaluesofadversarialexamplesformnist ingly including inputs higher sa , vice versa ( . e ., cifar - 10 . fromtheinputwiththehighestsatoinputswithlowersa ). sadl capture relative surprise inputs . inputs 10 . 8 theresultsshowthatthegapindsavaluesobservedin higher sa harder correctly classify ; adversarial figure4canbeusedtoclassifyadversarialexampleswithhigh examples show higher sa values classiﬁed based accuracy . relatively simpler mnist model , dsa - sa accordingly . based classiﬁer detect adversarial examples roc - auc ranging 96 . 97 % 99 . 38 %. dsa - based clas - b . impact layer selection ( rq2 ) siﬁcation complicated cifar - 10 model shows table iv shows roc - auc classiﬁcation adver - lowerroc - aucvalues , butanswerstorq2suggestthatdsa sarial examples , resulting row corresponding fromspeciﬁclayerscanproducesigniﬁcantlyhigheraccuracy classiﬁer trained lsa dsa speciﬁc layer ( see section v - b ). mnist , respectively . rows ordered depth , . e ., basedonthreedifferentanalyses , theanswertorq1isthat activation 3isthedeepestandthelasthiddenlayerinmnist . highest roc - auc values attack strategy 8lsa - based classiﬁcation possible subsets neurons due typeset bold . mnist , clear evidence thecomputationalcostofkde ; henceweintroducetheresultsoflsa - based classiﬁcationwhenansweringtheimpactoflayerselectionforrq2 . deepest layer effective . 11004455 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . fig . 5 : sorted lsa randomly selected 2 , 000 adversarial examples mnist cifar - 10 different layers table iii : roc - auc dsa - based classiﬁcation adver - themostaccurateclassiﬁerforbim - . moreimportantly , per - sarial examples mnist cifar - 10 layer dsa values produce much accurate classiﬁcation dataset fgsm bim - bim - b jsma c & w results neuron dsa values , seen comparison table iii table iv & v . identical mnist 98 . 34 % 99 . 38 % 96 . 97 % 97 . 10 % 99 . 04 % cifar - 10 76 . 81 % 72 . 93 % 71 . 66 % 88 . 96 % 92 . 84 % models used produce results tables . tablev : roc - aucresultsofsaperlayersoncifar - 10 . table iv : roc - auc results sa per layers mnist . sa layer fgsm bim - bim - b jsma c & w sa layer fgsm bim - bim - b jsma c & w activation 1 72 . 91 % 61 . 59 % 63 . 30 % 76 . 85 % 74 . 01 % activation 1 100 . 00 % 99 . 94 % 100 . 00 % 98 . 17 % 99 . 48 % activation 2 89 . 59 % 62 . 17 % 73 . 20 % 80 . 33 % 75 . 98 % lsa activation 2 100 . 00 % 99 . 46 % 100 . 00 % 94 . 42 % 99 . 23 % pool 1 93 . 31 % 61 . 79 % 78 . 89 % 82 . 64 % 73 . 48 % pool 1 100 . 00 % 99 . 73 % 100 . 00 % 99 . 08 % 99 . 61 % activation 3 86 . 75 % 62 . 69 % 76 . 93 % 80 . 33 % 79 . 02 % activation 3 93 . 29 % 81 . 70 % 86 . 73 % 94 . 45 % 37 . 96 % activation 4 83 . 31 % 62 . 73 % 86 . 15 % 80 . 86 % 80 . 42 % activation 1 100 . 00 % 99 . 85 % 100 . 00 % 97 . 79 % 99 . 39 % lsa pool 2 82 . 82 % 61 . 16 % 89 . 69 % 80 . 61 % 73 . 85 % activation 2 100 . 00 % 99 . 39 % 99 . 99 % 97 . 59 % 99 . 69 % activation 5 83 . 80 % 60 . 64 % 96 . 31 % 79 . 56 % 64 . 60 % dsa pool 1 100 . 00 % 99 . 32 % 99 . 99 % 98 . 21 % 99 . 69 % activation 6 63 . 85 % 51 . 90 % 99 . 74 % 66 . 99 % 60 . 40 % activation 3 98 . 45 % 99 . 43 % 97 . 40 % 97 . 07 % 99 . 10 % pool 3 63 . 46 % 51 . 86 % 99 . 77 % 67 . 62 % 56 . 21 % activation 7 67 . 96 % 61 . 09 % 92 . 18 % 83 . 02 % 76 . 85 % activation 8 59 . 28 % 52 . 66 % 99 . 60 % 73 . 26 % 62 . 15 % activation 1 65 . 00 % 62 . 25 % 61 . 57 % 73 . 85 % 79 . 09 % cases roc - auc 100 % explained activation 2 77 . 63 % 64 . 73 % 67 . 95 % 78 . 16 % 81 . 59 % figure 5 : lsa values activation 1 mnist , pool 1 80 . 22 % 64 . 89 % 70 . 94 % 78 . 96 % 82 . 03 % activation 3 83 . 25 % 68 . 48 % 73 . 49 % 79 . 89 % 84 . 16 % example , show clear separation original test activation 4 81 . 77 % 68 . 94 % 77 . 94 % 80 . 55 % 84 . 62 % inputs fgsm , bim - , bim - b : choosing appro - dsa pool 2 82 . 51 % 69 . 28 % 81 . 43 % 80 . 92 % 84 . 81 % activation 5 81 . 45 % 70 . 29 % 83 . 28 % 82 . 15 % 85 . 15 % priatethreshold , itispossibletocompletelyseparatetestinputs activation 6 71 . 71 % 70 . 92 % 71 . 15 % 84 . 05 % 85 . 42 % adversarial examples . similarly , plot lsa pool 3 71 . 75 % 70 . 35 % 74 . 65 % 83 . 57 % 85 . 17 % activation 3 mnist shows c & w lsa line crossing activation 7 71 . 04 % 71 . 44 % 81 . 46 % 89 . 94 % 92 . 98 % activation 8 70 . 35 % 70 . 65 % 90 . 47 % 90 . 46 % 94 . 53 % original test data ( . e ., c & w adversarial examples less surprising original test data ): based results , answer rq2 dsa results low roc - auc value 37 . 96 %. sensitive selection layers computed , table v contains roc - auc values lsa - dsa - beneﬁts choosing deeper layer . however , basedclassiﬁers , trainedoneachlayerofthecifar - 10model : lsa , clear evidence supporting deeper attack strategy , highest roc - auc values layer assumption . layer sensitivity varies across different typeset bold . interestingly , lsa dsa show different adversarial example generation strategies . trendswithcifar - 10 . withlsa , thereisnostrongevidence c . correlation sc criteria ( rq3 ) deepest layer produces accurate classiﬁers . however , dsa , deepest layer produces table vi shows different coverage criteria respond accurateclassiﬁersforthreeoutofﬁveattackstrategies ( bim - increasing diversity levels9 . columns represent steps , b , jsma , andc & w ), whiletheseconddeepestlayerproduces 9seehttps :// coinse . github . io / sadlforplots . 11004466 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . inputs added original test set . highest accuracy mnist cifar - 10 , lowest increaseincoverageatastepislessthan0 . 1percentagepoint mse dave - 2 ). best performance typeset bold . 4 compared previous step , value underlined . thefullrange , , producesthebestretrainingperformance 4 2 3 threshold 0 . 1 percentage point based ﬁnest 13 conﬁgurations , followed ( 5 conﬁgurations ), ( 3 4 4 1 step change possible lsc , dsc , well kmnc , conﬁgurations ), ( 3 conﬁgurations ). note 4 2 threeusebucketingwithk = 1 , 000 . weacknowledgethatthe conﬁguration cifar - 10 bim - b , ranges 4 3 threshold arbitrary , provide supporting aid . produces best retraining performance . 4 notethatdsccannotbecomputedforthesetwodlsystems , largest improvement observed retraining mnist 4 classiﬁers ( see section ii - c ). againstfgsmusingdsa : theaccuracyofthe rangeshows 4 overall , studied criteria increase additional 77 . 5 % increasefromthatof 1 ( . e ., from15 . 60 % to28 . 69 %). 4 inputs added step . notable exception nc , retraining mnist bim - b using dsa shows whichplateausagainstmanysteps . thisisinlinewithresults evengreaterimprovement ( from9 . 40 % to40 . 94 %), wesuspect existing work [ 27 ]. exists interplay outlier accuracy ranges 1 2 4 4 type added inputs different criteria respond : signiﬁcantly smaller compared conﬁgurations . snac , kmnc , nbc show signiﬁcant increases observations limited dl systems addition bim - b examples cifar - 10 , change little inputgenerationtechniquesstudiedhere , weanswerrq4that whenc & winputsareadded . however , onlysnacandnbc sa provide guidance effective retraining exhibit similar increase addition input set 1 adversarial examples based interpretation chauffeur , whilekmncincreasesmoresteadily . overall , observed trend . exception nc , answer rq3 sc correlated dnn sa r fgsm bim - bim - b jsma c & w coverage criteria introduced far . model μ σ μ σ μ σ μ σ μ σ ∅ 11 . 65 - 9 . 38 - 9 . 38 - 18 . 88 - 8 . 92 - dnn criteria test step1 step2 step3 step4 step5 1 / 4 25 . 81 1 . 95 95 . 14 0 . 69 41 . 00 0 . 01 72 . 67 3 . 09 92 . 51 0 . 51 lsc 29 . 50 + f3g4s . 9m0 + bi3m7 .- 1a0 + b5im6 . 3 - b0 + j6s1m . 9a0 + c62 &. 0w0 mnist lsa 234 /// 444 222893 ... 467560 234 ... 969138 999555 ... 789170 000 ... 497189 444000 ... 999873 000 ... 111208 777557 ... 043387 221 ... 667805 999222 ... 545516 010 ... 607737 dsc 46 . 00 56 . 10 65 . 00 67 . 20 70 . 90 72 . 30 nc 42 . 73 42 . 73 43 . 03 43 . 03 43 . 03 45 . 45 1 / 4 15 . 60 2 . 12 93 . 67 3 . 42 9 . 90 1 . 05 74 . 56 2 . 62 12 . 80 0 . 96 mnist kmnc 68 . 42 70 . 96 72 . 24 75 . 82 77 . 31 77 . 37 dsa 23 // 44 1296 .. 6377 46 .. 3125 9955 .. 7387 00 .. 7903 490 .. 4801 00 .. 0252 7768 .. 1061 21 .. 6897 1122 .. 4367 11 .. 0104 nbc 6 . 52 14 . 55 16 . 36 36 . 06 38 . 03 43 . 48 4 / 4 27 . 69 5 . 59 95 . 31 0 . 98 40 . 94 0 . 04 76 . 60 2 . 38 13 . 61 1 . 19 snac 10 . 91 19 . 39 19 . 39 53 . 33 57 . 27 57 . 27 ∅ 6 . 13 - 0 . 00 - 0 . 00 - 2 . 68 - 0 . 31 - lsc 46 . 20 54 . 70 55 . 8 57 . 70 61 . 10 63 . 20 dsc 66 . 20 70 . 10 70 . 6 80 . 90 83 . 40 84 . 10 1 / 4 11 . 07 1 . 20 32 . 34 1 . 70 0 . 59 1 . 76 32 . 80 2 . 05 34 . 38 2 . 83 cifar - 10 nknsncmbacncc 2216862 .... 17555768 2216973 .... 23278061 226971 ... 3253 . 8108 23246437 .... 20919861 2326444 ... 7330 . 3112 23247447 .... 04871140 cifar - 10 lsa 2341 //// 4444 11112224 .... 97586936 2212 .... 11118796 33322225 .... 61798494 2222 .... 04297099 0000 .... 88609901 2210 .... 11700060 33333554 .... 88894132 2222 .... 58502141 44442554 .... 95729841 2222 .... 72008342 dnn criteria test + singleocc + multiocc + light dsa 23 // 44 1143 .. 6841 11 .. 9855 2391 .. 5993 32 .. 5727 00 .. 0011 00 .. 0000 3345 .. 4691 12 .. 8490 4446 .. 7196 22 .. 3425 4 / 4 13 . 12 1 . 41 32 . 17 2 . 36 0 . 60 1 . 76 37 . 32 1 . 58 46 . 21 2 . 72 lsc 30 . 00 42 . 00 42 . 00 76 . 00 nc 79 . 55 80 . 26 80 . 45 83 . 14 ( ) mnistandcifar - 10 dave - 2 kmnc 33 . 53 35 . 15 35 . 91 37 . 94 nbc 0 . 51 5 . 29 5 . 32 6 . 60 snac 1 . 03 10 . 58 10 . 64 13 . 21 dnn sa r singleocc multiocc light dnn criteria test + set1 + set2 + set3 model μ σ μ σ μ σ lsc 48 . 90 53 . 50 56 . 10 58 . 40 ∅ 0 . 4212 - 0 . 0964 - 0 . 3822 - nc 22 . 14 22 . 65 22 . 70 22 . 83 1 / 4 0 . 0586 0 . 0142 0 . 0539 0 . 0003 0 . 0573 0 . 0057 chauffeur kmnc 48 . 08 50 . 79 52 . 20 53 . 21 dave - 2 2 / 4 0 . 0540 0 . 0012 0 . 0562 0 . 0060 0 . 0560 0 . 0042 nbc 3 . 05 16 . 88 17 . 96 19 . 13 lsa 3 / 4 0 . 0554 0 . 0041 0 . 0544 0 . 0009 0 . 0570 0 . 0133 snac 3 . 93 18 . 37 19 . 41 20 . 93 4 / 4 0 . 0553 0 . 0028 0 . 0561 0 . 0042 0 . 0601 0 . 0111 table vi : changes various coverage criteria ( b ) dave - 2 increasing input diversity . put additional inputs table vii : retraining guided sa : sample 100 inputs original test inputs observe changes coverage values . u 2u four increasingly wider ranges sa : [ 0 , ], [ 0 , ], 4 4 3u [ 0 , ], [ 0 , u ], andretrainforﬁveadditionalepochsusing 4 . retraining guidance ( rq4 ) samples training data , measure accuracy table vii shows impact sa - based guidance mse entire adversarial synthetic inputs . retraining mnist , cifar - 10 , dave - 2 models . samplingfromwiderrangesimprovestheretrainingaccuracy . 1 4 column r represents increasingly wider 4 4 ranges sa inputs additional training sampled ; rows r = ∅ show performance vi . threatstovalidity dl system retraining . overall , 23 retraining primary threat internal validity study conﬁgurations ( 2 sa types × 2 dl systems × 5 adversarial correctness implementation studied dl systems , attackstrategies , and1satype × 1dlsystem × threeinput well computation sa values . used publicly synthesismethods ), eachofwhichisevaluatedagainstfoursa available architectures pre - trained models subjects rangeswith20repetitions . columnsμandσcontainthemean avoid incorrect implementation . sa computation depends standard deviation observed performance metric ( . e ., widely used computation library , scipy , 11004477 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . stood public scrutiny . threats external validity mostly dl systems . et al . proposed deepct , views concerns number models input generation ranges neuron activation values parameter choices techniques study . possible sadl less appliescombinatorialinteractiontesting ( cit ) tomeasurein - effectiveagainstotherdlsystems . whilewebelievethecore teractioncoverage [ 29 ]. scisdifferentfromdeepctassadl principleofmeasuringinputsurpriseisuniversallyapplicable , aimsto quantify amountofsurprise , ratherthan simply experimentations reduce particular risk . detectsurpriseviaincreaseincoverage . deepmutationapplies finally , threats construct validity asks whether principle mutation testing dl systems mutating measuring correct factors draw conclusion . training data , test data , well dl system , based studied dl systems , activation traces immediate artefacts source model level mutation operators [ 28 ]. oftheirexecutionsandthemeaningofoutputaccuracyiswell viii . conclusion established , minimising risk threat . propose sadl , surprise adequacy framework dl vii . relatedwork systems quantitatively measure relative surprise adversarial examples pose signiﬁcant threats perfor - input respect training data , call mance dl systems [ 7 ]. existing work surpriseadequacy ( sa ). usingsa , wealsodevelopsurprise machine learning community detection inputs . coverage ( sc ), measures coverage discretised feinman et al . [ 14 ] ﬁrst introduced kde means input surprise ranges , rather count neurons similarity measurement , aim detecting adversarial speciﬁc activation traits . empirical evaluation shows examples . sadl improves upon existing work sa sc capture surprise inputs accurately number different ways . first , generalise concept aregoodindicatorsofhowdlsystemswillreacttounknown ofsurpriseadequacy ( sa ) andintroducedistance - basedsa . inputs . sa correlated difﬁcult dl system ﬁnds second , ourevaluationisinthecontextofdlsystemtesting . input , used accurately classify adversarial third , ourevaluationofsadlincludesmorecomplicatedand examples . sccanbeusedtoguideselectionofinputsformore practical dl systems , well testing techniques effectiveretrainingofdlsystemsforadversarialexamplesas deepxplore deeptest . finally , show choice well inputs synthesised deepxplore . neurons limited impact lsa . acknowledgement arangeoftechniqueshasbeenrecentlyproposedtotestand work supported engineering research verify dl systems . existing techniques largely based center program national research foundation two assumptions . ﬁrst assumption variation korea funded korean government ( msit ) ( nrf - metamorphictesting [ 11 ],[ 31 ],[ 42 ]. supposeadlsystemn 2018r1a5a1059921 ), institute information & commu - producesanoutputowhengiveniastheinput , . e ., n ( )= . expect n ( ( cid : 3 )) ( cid : 7 ) ( cid : 3 ) ( cid : 7 ) . huang et al . [ 19 ] nications technology promotion grant funded ko - rean government ( msit ) ( . 1711073912 ), next - proposed veriﬁcation technique automatically gen - generation information computing development program erate counter - examples violate assumption . pei et national research foundation korea funded al . introduced deepxplore [ 34 ], white - box technique korean government ( msit ) ( 2017m3c4a7068179 ). generates test inputs cause disagreement among set dl systems , . e ., nm ( ) ( cid : 8 )= nn ( ) independently trained robert feldt acknowledges projects tocsyc ( swedish knowledge foundation , kks , num . 20130085 ) baseit dl systems nm nn . tian et al . presented deeptest , ( swedishsciencecouncil , vr , num . 2015 - 04913 ) forfunding whose metamorphic relations include simple geometric parts work paper . perturbations well realistic weather effects [ 40 ]. secondassumptionisthatthemorediverseasetofinputis , references moreeffectiveitwillbefortestingandvalidatingdlsystems . [ 1 ] autonomous driving model : chauffeur . https :// github . com / udacity / pei et al . proposed neuron coverage ( nc ), measures self - driving - car / tree / master / steering - models / community - models / ratio neurons whose activation values chauffeur . predeﬁned threshold [ 34 ]. shown adding test [ 2 ] udacity open source self - driving car project . https :// github . com / udacity / self - driving - car . inputs violate ﬁrst assumption increases diversity [ 3 ] google accident 2016 : google self - driving car caused crash measured nc . similarly , deepgauge introduced ﬁrst time http :// www . theverge . com / 2016 / 2 / 29 / 11134344 / set multi - granularity coverage criteria thought google - self - driving - car - crash - report , 2016 . [ 4 ] paul ammann jeff offutt . introduction software testing . reﬂect behaviours dl systems ﬁner granularity [ 27 ]. cambridgeuniversitypress , 2016 . criteria capture input diversity , [ 5 ] yoshuabengio , gre ´ goiremesnil , yanndauphin , andsalahrifai . better essentially count neurons unlike sa , therefore cannot mixingviadeeprepresentations . corr , abs / 1207 . 4404 , 2012 . [ 6 ] mariusz bojarski , davide del testa , daniel dworakowski , bernhard bedirectlylinkedtobehavioursofdlsystems . weshowthat firner , beatflepp , prasoongoyal , lawrencedjackel , mathewmon - sa closely related behaviours training accurate fort , ursmuller , jiakaizhang , etal . endtoendlearningforself - driving adversarial example classiﬁers based sa . cars . arxivpreprintarxiv : 1604 . 07316 , 2016 . [ 7 ] nicholas carlini david wagner . adversarial examples apart coverage criteria , concepts traditional easily detected . proceedings 10th acm workshop artiﬁcial softwaretestinghavebeenreformulatedandappliedtotesting intelligenceandsecurity - aisec ’ 17 , 2017 . 11004488 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply . [ 8 ] nicholas carlini david wagner . adversarial examples [ 29 ] lei , fuyuan zhang , minhui xue , bo li , yang liu , jianjun zhao , easily detected : bypassing ten detection methods . proceedings yadong wang . combinatorial testing deep learning systems . 10th acm workshop artiﬁcial intelligence security , pages arxivpreprintarxiv : 1806 . 07723 , 2018 . 3 – 14 . acm , 2017 . [ 30 ] xingjun , bo li , yisen wang , sarah erfani , sudanthi wi - [ 9 ] nicholascarlinianddavida . wagner . towardsevaluatingtherobust - jewickrema , michael e houle , grant schoenebeck , dawn song , nessofneuralnetworks . corr , abs / 1608 . 04644 , 2016 . jamesbailey . characterizingadversarialsubspacesusinglocalintrinsic [ 10 ] chenyichen , ariseff , alainkornhauser , andjianxiongxiao . deepdriv - dimensionality . arxivpreprintarxiv : 1801 . 02613 , 2018 . ing : learningaffordancefordirectperceptioninautonomousdriving . [ 31 ] christian murphy , kuang shen , gail kaiser . automatic system proceedingsoftheieeeinternationalconferenceoncomputervision , testing programs without test oracles . proceedings 18th pages2722 – 2730 , 2015 . internationalsymposiumonsoftwaretestingandanalysis , issta2009 , [ 11 ] . . chen , f .- c . kuo , . h . tse , zhi quan zhou . metamorphic pages189 – 200 . acmpress , 2009 . testing beyond . proceedings international workshop [ 32 ] nicolas papernot , fartash faghri , nicholas carlini , ian goodfellow , softwaretechnologyandengineeringpractice ( step2003 ), pages94 – reuben feinman , alexey kurakin , cihang xie , yash sharma , tom 100 , september2004 . brown , aurkoroy , alexandermatyasko , vahidbehzadan , karenham - [ 12 ] zhihuacui , feixue , xingjuancai , yangcao , gai - gewang , andjinjun bardzumyan , zhishuai zhang , yi - lin juang , zhi li , ryan sheatsley , chen . detection malicious code variants based deep learning . abhibhav garg , jonathan uesato , willi gierke , yinpeng dong , david ieeetransactionsonindustrialinformatics , 14 ( 7 ): 3187 – 3196 , 2018 . berthelot , paul hendricks , jonas rauber , rujun long . technical [ 13 ] clementfarabet , camillecouprie , laurentnajman , andyannlecun . report cleverhans v2 . 1 . 0 adversarial examples library . arxiv learninghierarchicalfeaturesforscenelabeling . ieeetransactionson preprintarxiv : 1610 . 00768 , 2018 . patternanalysisandmachineintelligence , 35 ( 8 ): 1915 – 1929 , 2013 . [ 33 ] nicolaspapernot , patrickd . mcdaniel , someshjha , mattfredrikson , z . berkay celik , ananthram swami . limitations deep [ 14 ] reuben feinman , ryan r curtin , saurabh shintre , andrew b learninginadversarialsettings . corr , abs / 1511 . 07528 , 2015 . gardner . detecting adversarial samples artifacts . arxiv preprint [ 34 ] kexin pei , yinzhi cao , junfeng yang , suman jana . deepxplore : arxiv : 1703 . 00410 , 2017 . automatedwhiteboxtestingofdeeplearningsystems . inproceedingsof [ 15 ] robert feldt , simon poulding , david clark , shin yoo . test set the26thsymposiumonoperatingsystemsprinciples , sosp ’ 17 , pages diameter : quantifyingthediversityofsetsoftestcases . inproceedings 1 – 18 , newyork , ny , usa , 2017 . acm . oftheieeeinternationalconferenceonsoftwaretesting , veriﬁcation , [ 35 ] simonpouldingandrobertfeldt . generatingcontrollablyinvalidand andvalidation , icst2016 , pages223 – 233 , 2016 . atypicalinputsforrobustnesstesting . insoftwaretesting , veriﬁcation [ 16 ] ian goodfellow , jonathon shlens , christian szegedy . explaining validation workshops ( icstw ), 2017 ieee international confer - harnessing adversarial examples . international conference enceon , pages81 – 84 . ieee , 2017 . learningrepresentations , 2015 . [ 36 ] david w scott . multivariate density estimation : theory , practice , [ 17 ] geoffrey hinton , li deng , dong yu , george e dahl , abdel - rahman visualization . johnwiley & sons , 2015 . mohamed , navdeepjaitly , andrewsenior , vincentvanhoucke , patrick [ 37 ] ilya sutskever , oriol vinyals , quoc v le . sequence sequence nguyen , tara n sainath , et al . deep neural networks acoustic learning neural networks . advances neural information modeling speech recognition : shared views four research processingsystems , pages3104 – 3112 , 2014 . groups . ieeesignalprocessingmagazine , 29 ( 6 ): 82 – 97 , 2012 . [ 38 ] christianszegedy , weiliu , yangqingjia , pierresermanet , scottreed , [ 18 ] sepp hochreiter ju ¨ rgen schmidhuber . long short - term memory . dragomiranguelov , dumitruerhan , vincentvanhoucke , andandrew neuralcomputation , 9 ( 8 ): 1735 – 1780 , 1997 . rabinovich . going deeper convolutions . proceedings [ 19 ] xiaoweihuang , martakwiatkowska , senwang , andminwu . safety ieeeconferenceoncomputervisionandpatternrecognition , pages1 – veriﬁcation deep neural networks . rupak majumdar viktor 9 , 2015 . kuncˇak , editors , computeraidedveriﬁcation , pages3 – 29 , cham , 2017 . [ 39 ] l . tarassenko . biosign ™ : multi - parameter monitoring early springerinternationalpublishing . warning patient deterioration . iet conference proceedings , pages [ 20 ] se ´ bastienjean , kyunghyuncho , rolandmemisevic , andyoshuaben - 71 – 76 ( 5 ), january2005 . gio . onusingverylargetargetvocabularyforneuralmachinetranslation . [ 40 ] yuchi tian , kexin pei , suman jana , baishakhi ray . deeptest : proceedings 53rd annual meeting association automatedtestingofdeep - neural - network - drivenautonomouscars . computational linguistics 7th international joint conference proceedingsofthe40thinternationalconferenceonsoftwareengineer - onnaturallanguageprocessing ( volume1 : longpapers ), volume1 , ing , pages303 – 314 . acm , 2018 . pages1 – 10 , 2015 . [ 41 ] matt p wand chris jones . kernel smoothing . chapman [ 21 ] alex krizhevsky , vinod nair , geoffrey hinton . cifar - 10 hall / crc , 1994 . dataset . online : http :// www . cs . toronto . edu / kriz / cifar . html , 2014 . [ 42 ] shin yoo . metamorphic testing stochastic optimisation . pro - [ 22 ] alex krizhevsky , ilya sutskever , geoffrey e hinton . imagenet ceedingsofthe3rdinternationalworkshoponsearch - basedsoftware classiﬁcation deep convolutional neural networks . advances testing , sbst2010 , pages192 – 201 , 2010 . inneuralinformationprocessingsystems , pages1097 – 1105 , 2012 . [ 43 ] shinyooandmarkharman . regressiontestingminimisation , selection [ 23 ] alexey kurakin , ian j . goodfellow , samy bengio . adversarial andprioritisation : asurvey . softwaretesting , veriﬁcation , andrelia - examplesinthephysicalworld . corr , abs / 1607 . 02533 , 2016 . bility , 22 ( 2 ): 67 – 120 , march2012 . [ 24 ] yann lecun , yoshua bengio , geoffrey hinton . deep learning . [ 44 ] hongzhu , patricka . v . hall , andjohnh . r . may . softwareunittest nature , 521 ( 7553 ): 436 , 2015 . coverageandadequacy . acmcomput . surv ., 29 ( 4 ): 366 – 427 , december [ 25 ] yann lecun , corinna cortes , cj burges . mnist handwritten 1997 . digit database . & labs [ online ]. available : http :// yann . lecun . com / exdb / mnist , 2 , 2010 . [ 26 ] stijn luca , peter karsmakers , kris cuppens , tom croonenborghs , anoukvandevel , bertenceulemans , lievenlagae , sabinevanhuffel , andbartvanrumste . detectingrareeventsusingextremevaluestatistics applied epileptic convulsions children . artiﬁcial intelligence medicine , 60 ( 2 ): 89 – 96 , 2014 . [ 27 ] leima , felixjuefei - xu , jiyuansun , chunyangchen , tingsu , fuyuan zhang , minhui xue , bo li , li li , yang liu , jianjun zhao , yadongwang . deepgauge : comprehensiveandmulti - granularitytesting criteria gauging robustness deep learning systems . corr , abs / 1803 . 07519 , 2018 . [ 28 ] leima , fuyuanzhang , jiyuansun , minhuixue , boli , felixjuefei - xu , chaoxie , lili , yangliu , jianjunzhao , etal . deepmutation : mutation testing deep learning systems . arxiv preprint arxiv : 1805 . 05206 , 2018 . 11004499 authorized licensed use limited : universidad nacional autonoma de mexico ( unam ). downloaded march 08 , 2021 20 : 33 : 46 utc ieee xplore . restrictions apply .',\n",
       "       'comparative analysis government plans peruvian presidential candidates sdoun state policies national agreement based nlp honorio apaza alanoca josimar chire jimy oblitas data science research group national university moquegua ilo moquegua peru institute mathematics computer science icmc r university sao paulo usp sao carlos sp brazil p facultad de ingeniera universidad privada del norte cajamarca peru hapazaaunamedupe jecsuspbr jimyoblitasupnedupe abstract analysis government proposal elections c political parties vital choose next authorities city country paper use text mining approach analyze c documentsandprovideaneasyvisualizationtosupportaneasyanalysis besides comparison national plan based sustainable devel opmentobjectivesofununitednationsfromagendaisperfomed v using natural language techniques keywords natural language processing text mining data science system recommender elections politics peru south america introduction election authorities important event citizens choose peoplewhowillrepresentthemandpurposeprojectstoimprovethenationalre v gional context traditionally political parties promote candidates x mass media ie radio television social networks candidates travel visit cities gain electors r peru participate president elections requirement send gov ernment proposal plan jurado nacional de elecciones national elections jury document summarizes proposal candidates considering themostimportantproblemsforthepartyandsolutionsthattheypurposeusu ally documents dozens pages read citizens choose next authority besides united nations un purposed agenda summarize important issues need special attention governments related poverty communication discrimination united nations un adopted new international develop ment agenda agenda includes sustainable development honorio apaza alanoca josimar chire jimy oblitas goals targets agenda species need actions strengthen sustainable economic growth decent employment industrialization countriescaribbean theagendaconsidersacomplexcombinationoffairlydetailedthematic targets comprehensive approach requires addressing sustainable development necessary integration social economic environmen tal axes nieto although recognized country priorities agenda reference government plans seeking adequate sustainabledevelopmentofperuthereforemeasuringthealignmentorpossible evolution government plans presidential candidates necessary task context use software tools text mining emerges quick interesting proposal measure trends addition fact peruvian context tools used yet contrasts global trends use software tools already established cam paignsoftrumpandbolsonarointheunitedstatesusaandbrazilwhichil lustratepolicyfactsthathavebeenfavoredbyictsgarcianunes et al natural language processing shown potential promising tool ex ploit urban data sources authors cai suggest use urban big data sources still starting studied areas urban governance management public health land use functional zones mo bility urban design useful expanding study scales reducing research costs text mining area uses wellknow data mining approach data col lection exploration analysis visualization text mining focuses text analysis uses natural language techniques nlp many studies per formed analyze dierent problems dierent areas ie epidemiology chire saire oblitas cruz politicssharma shekhar mar keting etc applicationsoftextmininginpoliticsandelectionsieanticipatingpolit ical behaviour sangar et al study voting patterns bagui et al ., 2007 ], fraudidentiﬁcation [ poloni formolo , 2015 ], sentimentalanalysisofcitizens [ sharma ghose , 2020 ], electionresultprediction [ ramteke et al ., 2016 ] . objective paper analyze government proposal peruvian candidates president elections using text mining approach support easyunderstandingofthedocuments . besides , performamatchingprocesswith national plan adapted 2030 agenda , check important objective political parties . sectioniincludesthereviewofthebibliography , sectioniidevelopsthework proposal , section iii discloses results research section iv gives conclusions , last section presents future work . analysis government plans peruvian presidential candidates 3 2 proposal natural language processing process transformation text information numeric data [ di giuda et al ., 2020 ]. work based following research process : data collection data analysis reporting select retrieve data comparative analysis ( government plan report research results algorithm jaro candidates findings . winkler . presidency peru ). fig . 1 : research process , process planed used [ kim et al ., 2017 ] 2 . 1 data collection present work , 18 government plans candidates presidency republic peru collected . also sustainable development goals policies state national agreement , sustainable devel - opment goals ( sdgs ) promoted united nations , whose predecessor millennium development goals , constitute inclusive global agenda goals 2030 [ secretaria ejecutivo del acuerdo nacional , 2017 ]. 2 . 2 data analysis jarowinkleristhemainalgorithmtoperformcomparativetextanalysisofdoc - uments ( governmentplansofthecandidates ) withthesustainabledevelopment goals ( sdgs ) promoted united nations . ( cid : 40 ) 0 = 0 sim ( , )= ( 1 ) j 1 2 1 ( + + − ) 3 s1 s2 objective calculate distance strings texts written plans government candidates objectives policies sustainable development state national agreement . ﬁrst preliminary test research interested knowing results obtained jaro winkler . 2 . 3 reporting finally , thelaststageoftheresearchistomakeareportontheresultsobtained , case results jaro winkler distance plans candidates ’ governmentandtheobjectivesandsustainabledevelopmentpolicies state national agreement . 4 honorio apaza alanoca , josimar chire jimy oblitas 3 results section shows result frequency terms word cloud , seen candidate highlights particular topic , : system , health , program , etc . result due fact currently nation world suﬀering global pandemic , therefore , plans candi - dates ’ government propose proposals solve problems related health . also shows important issues education , economics , etc . beenneglected . especiallyissuesrelatedtosustainabledevelopmentgoals ( sdg ) promoted united nations . accion popular partido morado avanza pais alianza para el progreso apra democracia directa frente amplio frente esperanza fuerza popular juntos por el peru partido nacionalista peru libre patria segura podemos peru ppc renovavion popular somos peru union por el peru victoria nacional fig . 2 : cloud words plans government candidates among candidates ’ plans , one stands gov - ernment plan political party avanza pais economic issue , also seen accion popular political party uniform distribution government plan issues economy , health , education politics . seen figure 3 . inthiscasewecanvarytheissueswewanttomeasure , thiscanbeaccording tothecontextofthemomentanddiﬀerentsectorsofsociety , theyhavediﬀerent problems needs , important analyze points view , social classes thoughts . present graphical ( figure 4 ) representation similar government plans candidates presidency peru , figure 4 seen identical , see degree similarity , due fact government plans clearly address similar issues translate social problems ( health , economy , programs , etc .) government ( judiciary , corruption , congress , etc .). experiment , diﬀerences proliﬁc class also denoted , cases distance noticeable political parties considered analysis government plans peruvian presidential candidates 5 0 . 00035 0 . 00030 gobierno 0 . 00025 política educación 0 . 00020 salud 0 . 00015 economía religión 0 . 00010 accion popular partido morado avanza pais alianza para el progreso apra democracia directa frente amplio frente esperanza fuerza popular juntos por el peru partido nacionalista peru libre patria segura podemos peru ppc renovavion popular somos peru union por el peru victoria nacional 00 .. 0000000005 fig . 3 : important areas documents beontheleftwiththoseontheright . whichcanbesimilarinthedailyexercise , obviously diﬀerent thoughts , therefore diﬀerent proposals two sides peruvian politics . 1 . 0 0 . 0 2 . 5 0 . 9 5 . 0 7 . 5 0 . 8 10 . 0 0 . 7 12 . 5 15 . 0 0 . 6 17 . 5 0 . 0 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 17 . 5 fig . 4 : documents similarity 6 honorio apaza alanoca , josimar chire jimy oblitas section going analyze distance chains texts writteninthegovernmentplansofthecandidatesandtheobjectivesandpolicies sustainable development state national agreement , try diﬀerentiatethesimilaritiesbetweenthesetwodocuments , whenachainoftexts issimilartoanothermeansthatthedocumentcontainstextssimilartotheother . , could say government plan addresses one many sustainable development goals policies state national agreement . fin de la pobreza hambre cero 0 . 66 salud bienestar educacion de calidad 0 . 64 igualdad de genero agua limpia saneamiento 0 . 62 energia asequible contaminante trabajo decente crecimiento economico 0 . 60 industria innovacion e infreestructura reduccion de las desigualdades 0 . 58 ciudades comunidades sostenibles produccion consumo responsables 0 . 56 accion por el clima vida submarina 0 . 54 vida de ecosistemas terrestres paz justicia e instituciones solidas alianzas para lograr los objetivos 0 . 52 accion popular partido morado avanza pais alianza para el progreso apra democracia directa frente amplio frente esperanza fuerza popular juntos por el peru partido nacionalista peru libre patria segura podemos peru ppc renovavion popular somos peru union por el peru victoria nacional fig . 5 : documents similarity plan graph , seen government plan political partyavanzapaisaddressesmuchmorethanothersthegoalofpeace , justiceand solid institutions ( paz justicia e instituciones solidas ), followed political party renovacion ppular . however , little addressed objectives : underwater life ( vida submarina ), health well - ( salud bienestar ), end poverty ( ﬁn de la probreza ), etc . 4 conclusions algorithm jaro winkler based measuring distance text chains shows us interesting preliminary results , shows us diﬀerences government plans candidates presidency peru , well objectives sustainable development goals state policies national agreement . however , results reﬁned advanced artiﬁcial intelligence methods algorithms . analysis government plans peruvian presidential candidates 7 present want highlight way diﬀerences government plan documents graphically demonstrated , way showing document diﬀerences important electorate , withouthavingtoreadallthegovernmentplans , theycanobtainamoregeneral vision graphically . 5 future work oneofthefuturejobsistoexperimentwithhighlyadvancedartiﬁcialintelligence techniques discipline natural language processing text mining . would interesting study experience coherent argu - ments candidates debate government plan . must coherence ideas proposals written government plan candidate expresses debate , interviews press , etc . references bagui et al ., 2007 . bagui , ., mink , ., cash , p . ( 2007 ). data mining techniques study voting patterns us . data science journal , 6 ( 0 ): 46 – 63 . cai , 2021 . cai , .( 2021 ). naturallanguageprocessingforurbanresearch : asystem - atic review . heliyon , 7 ( 3 ): e06322 . caribbean , 2017 . caribbean , e . c . f . l . . . . ( 2017 ). 2030 agenda sustainable development . last modiﬁed : 2017 - 06 - 28t13 : 23 - 04 : 00 publisher : cepal . chire saire oblitas cruz , 2020 . chiresaire , j . andoblitascruz , j .( 2020 ). study ofcoronavirusimpactonparisianpopulationfromapriltojuneusingtwitterand text mining approach . pages 242 – 246 . di giuda et al ., 2020 . di giuda , g . ., locatelli , ., schievano , ., pellegrini , l ., pattini , g ., giana , p . e ., seghezzi , e . ( 2020 ). natural language processing informationandprojectmanagement , pages95 – 102 . springerinternationalpublish - ing , cham . garcia - nunes et al ., 2020 . garcia - nunes , p . ., rodrigues , p . ., oliveira , k . g ., da silva , . e . . ( 2020 ). computational tool weak signals classiﬁcation – detectingthreatsandopportunitiesonpoliticsinthecasesoftheunitedstatesand brazilian presidential elections . futures , 123 : 102607 . kim et al ., 2017 . kim , k ., joungpark , ., yun , ., andyun , h .( 2017 ). whatmakes touristsfeelnegativelyabouttourismdestinations ? applicationofhybridtextmining methodologytosmartdestinationmanagement . technologicalforecastingandsocial change , 123 : 362 – 369 . nieto , 2017 . nieto , . . ( 2017 ). crecimiento econo ´ mico e industrial - izacio ´ n en la agenda 2030 : perspectivas para ´ xico . problemas del desarrollo , 48 ( 188 ): 83 – 111 . poloni formolo , 2015 . poloni , . . andformolo , .( 2015 ). dataminingtoiden - tify fraud suspected electronic elections . 2015 ninth international conference complex , intelligent , software intensive systems , pages 19 – 23 . ramteke et al ., 2016 . ramteke , j ., shah , ., godhia , ., andshaikh , .( 2016 ). elec - tion result prediction using twitter sentiment analysis . 2016 international con - ference inventive computation technologies ( icict ), volume 1 , pages 1 – 5 . 8 honorio apaza alanoca , josimar chire jimy oblitas sangar et al ., 2013 . sangar , . b ., khaze , . r ., ebrahimi , l . ( 2013 ). participa - tion anticipating elections using data mining methods . secretaria ejecutivo del acuerdo nacional , 2017 . secretaria ejecutivo del acuerdo na - cional ( 2017 ). objetivos de desarrollo dostenible politicas del estado del acuerdo nacional . sharma ghose , 2020 . sharma , . ghose , u . ( 2020 ). sentimental analysis twitter data respect general elections india . procedia computer science , 173 : 325 – 334 . international conference smart sustainable intelligent computing applications icitetm2020 . sharma shekhar , 2020 . sharma , . shekhar , h . ( 2020 ). intelligent learning basedopinionminingmodelforgovernmentaldecisionmaking . procediacomputer science , 173 : 216 – 224 .',\n",
       "       'jinstengindiaserb httpsdoiorgs original contribution text similarity measures news articles vector space model using nlp ritika singh satwinder singh receivedjuneacceptedoctober cidtheinstitutionofengineersindia abstract present global size online news websites keywords bilingual news article similarity cid million according marketingprofs cosine similarity cid jaccard similarity cid euclidean distance morethanmillionarticlesarepublishedeverydayonthe web online news websites also circulated edi torialcontent overthe internetthat species articles introduction display websites home pages articles highlight eg broad text size main news articles huge increase number online newspaper pub many articles posted news website lishing digital technology innova similar many news websites selective tions modern world much information reporting top news headlines also similarity appears tremendous speed readers need nd among news across various news associations well reading true news false news false news identiedbutnotverywellcalculatedthispaperidenties information endanger confuse persons top news items news sites measures lifebutalsoanentiresocietysoitisveryimportanttond similarity two news items two languages source information compare hindi english referring event news study interest extracting online accomplish highlighted headline link extractor news platforms specically measure similarity created extract top news hindi news articles across various sites article provides englishfromgooglesnewsfeedfirsttranslatethehindi detailsaboutwhatnewsisbeingconsideredhowitisbeing news article english using google translator presented highlighted website news compare english news articles second articles published website usually appear used cosine similarity jaccard similarity euclidean similar rectied form several different websites distance measure calculate news similarity score similar almost identical news confusing users frequency nouns next word nouns similarity slows process discovering new news articles also extracted methodology clearly information topic potentially leads missing showsthatwecanefcientlyidentifytopnewsarticlesand informationiftheusermistakenlyrecognizestwonewsas measure similarity news reports similarwheninfactonecontainsnewdataitismuchmore difcult locate similar news items websites large amount miscellaneous content material articles although main news article ritikasingh text similar two different web pages extra ritikasinghoutlookcom neous material pages may satwindersingh fore traditional approaches equivalent news satwindersinghcupeduin determination would fail first paper developed departmentofcomputerscienceandtechnologycentral method scraping top news headline text web universityofpunjabbathindabathindaindia pagesiegooglenewsfeedwebsiteswhicharepresentin jinstengindiaserb two different languages hindi english referring rst created headline link extractor sameevent use extracted textto classify news parses selected news websites searched ten us pairs content avoiding irrelevant based news site home pages months use informationonthearticlesbymeasuringasimilarityscore parser extract k news site max news pairs based method called cosine similarity imum number articles second author uses cal jaccard similarity euclidean similarity culation cosine similarity quantify similarity research distinguish similar news articles well news also provide techniques work differentonesthepurposeofthispaperisalsotodiscover assistinanalyzingarchivednewswebpagesbyintroducing bilingual news articles comparable corpus tools parsing select html news sites hero particular study dealing representation headlinestoriesusingcssselectorsauthorsstudiesover news measurement similarity among new months shown overall similarity decreased articles experiment uses similarly named entities asthenumberofarticlesincreasedstudiesfromtheauthor include representative features news indicate would set synchronous stories toassessthesimilaritybetweenarticlesofthesamenews given day besides relevant national events approach research proposing new method focused canbeusedtofurtherexaminetheoccasionalelectionsthat knowledge base framework aims provide human held information value category named entities katarzyna baraniak marcin sydow work tools within news comparable corpus news would support detection analysis hindi english compared approach tradi information bias author uses methods auto tional one obtains better results similarity also matically identify articles reporting sub distance measures calculate similarity two docu ject event entity use comparative mentsorsentencesintoasinglenumericalvalueandbrings analysisortoconstructatestortrainingcollectionwithin degree semantic similarity distance paper author explains representations doc oneanotherseveralsimilaritymeasureshavebeenusedby ument text method similarity measures text researchers much work done clustering include tests cosine similarity similarity newspapers study aims compare euclideandistancejaccardcoefcientpearsoncoefcient semanticsimilaritybetweentwoarticlesofthesamenews correlation averaged kullbackleibler diver present two different languages hindi english gence author also applies machine learning optimize human understanding basic concept approach recognize similar article develop measuring news similarities identify feature articles machine learning model detects similar articles auto vectors thereafter measure difference maticallyidentifyingfragments oftext concerningsimilar features low distance features eventsandidentifyingbiasinthemisexpectedtheauthor implies high level similarity value large dis - also working expand research study lan - tance features implies low level guages ( e . g ., polish , english ). similarity value [ 6 ]. euclidean distance , cosine distance , maake benard magara et al ., suggest system use jaccardcoefﬁcientmetricsaresomeofthedistancemetrics 220 artiﬁcial intelligent research paper written 8 artiﬁ - used document similarity computation . study cial intelligence experts [ 8 ]. work uses recursive explores twoseparatemethods ofgeneratingfeatures partitioning , random forest , improved machine texts : ( 1 ) tf - idf vectors , ( 2 ) bag words also learning algorithms average accuracy implements two methods calculating textual similarity timing efﬁciency 80 . 73 2 . 354628 . seconds , news articles : ( 1 ) cosine similarity jaccard algorithm typically performed quite well compared similarity tf - idf vectors ( 2 ) euclidean distance boosted even random forest algorithms . using bag words . sophisticated models used future studies much like latent semanticanalysis ( lsa ), sincedocuments canbeidentiﬁedasbelongingtothesameclassevenifthey literature rereview similar words phrases . vikas thada dr . vivek jaglan authors used cosine similarity , dice literature , similarity measures used coefﬁcient , jaccard similarity algorithms [ 9 ]. work various purposes . section , proposals completedontheﬁrst10pagesofthegooglesearchresult reviewed . expanded 30 – 35 pages reliable efﬁ - atkins et al . [ 1 ] describe technique assess top ciency estimate future study . cosine similarity news headline story selected set us - based news eventually concluded best ﬁtness compared websites , calculate correlations across . others dataset . summary , initial 123 j . inst . eng . indiaser . b ﬁndings promising , still long way go achieve greatest crawling efﬁciency possible . sys - tematicmethodproposedbynasabetal .[ 10 ] thefollowing points determine similarities . ( 1 ) article texts divided three sections headings , abstracts keywords . ( 2 ) abstract , keywords , based linkto title article weighing . ( 3 ) weighted mean esti - mated based description , abstract , keyword use pearson ’ correlation method ﬁnd similarity person machine scores . 87 % accuracy proposed technique . use specialized fig . 1 aframeworkforcomparativeanalysis wordnetitcanalsoconcentrateonarticlesimilarities . proposedframeworkcanbeusedforothertextsthatrequire wordnet language , texts persian languages . . snover et al ., explore new way usingmonolingualtargetdatatoenhancetheefﬁciencyofa statisticalorpredictivemachinetranslationfornewsstories jaccard similarity measures . ﬁnal step frame - [ 11 ]. thismethodemployscomparabletextvarioustextsin work compare analyze produced results . target language explore equivalent explain steps detail . stories mentioned source language document . dataset used paper known ‘ google large monolingual datasetfor source documenttobe news ’, publicly available [ 13 ]. google news : translated target language , searched google offering special experience google news documents may similar source documents . combines news items one . provides experimental results paper generated constant , personalized ﬂow newspapers thousands thedifferenceofthelanguageandtranslationmodelsshow ofpublishersandmagazinesgroupedaround . googlenews vital improvements baseline framework . combination global events , local news news qian et al . [ 12 ] using comparable corpus , bilingual stories ’ reading . turn dependency mapping model bilingual lexicon building headlines show top news world . english chinese . model considers additional sections allow delve various dependent words relationships measuring topics sports , business technology . similarity bilingual words thus offers greatest value service delivered news 35 precise less noisy representation . author ’ also languages using google news experiment extracts illustrated bilingual dependency mappings news articles hindi english languages . created optimized automatically without human input , contributing medium - sized set dependency map - headline link extractor pings impacts bilingual lexicon con - struction ( blc ) fully exploited weight basic python library searching downloading live learningusingasimplebuteffectiveperceptronalgorithm , news articles google news feeds googlenews making approach quickly adaptable several gnewsclient [ 14 ]. using , one pick top language pairs . headlines running google ’ news websites check top headlines particular subject ( keyword ). experiment use , extract links hindi methodology english news related event . major steps methodology given . article scraping figure 1 presents framework work . textual news data ﬁrst pre - processed rep - ‘ newspaper ’ isapythonmoduleusedtoextractnewspaper resented structural format . two represen - articles parse . newspapers using special - tationmethodsofgeneratingfeaturesfromthetextthatare ized web scrapping algorithms extract valuable investigatedinthisstudyaretf - idf , andbagofword . textfromawebsite . thisworksextremelywellonwebsites represented three representation methods , online newspapers . experiment extracted represented method compared three similarity links hindi english news , also measures shown fig . 1 . e . cosine , euclidean extract text using newspaper module . 123 j . inst . eng . indiaser . b translator idf vectors used feature vectors measure similarity articles news - results . using package , google offers language translation package python ; words taken similarities measures hindi news articles translated different languages ( english language ). either hindi corpus translated similarityfunctionisareal - valuedfunctionthatcalculates intoenglishorenglishcorpuscanbetranslatedintohindi . similarity two items . calculation sim - translated hindi corpus english . ilarity achieved mapping distances similarities translation performed level sentences . withinthevectorspace . thisexperimentprovidestwotests translation also generates map words various lan - ofsimilarity : cosinesimilarity , similaritywithjaccard , guages , english . research used bilingual dic - euclidean distance . tionaries ranging hindi english . ( 1 ) cosine similarity cosine angle n - di - mensionalspace , betweentwon - dimensionalvectors . pre - processing data cleaning isthedotproductofthetwovectors , dividedby - productof two vectors ’ lengths ( magnitudes ) [ 16 ]. simi - pre - processingstepssuchastheeliminationofstop - words , larity cosine measured using following lemmatization , parsing letters , punctuation marks , formula : numbers completed . words lemma - : b pn ( cid : 4 ) b tized wordnetlemmatizer nltk library took similarityða ; bþ¼jjajj ( cid : 4 ) jjbjj¼pﬃpﬃﬃﬃﬃﬃnﬃﬃﬃiﬃ¼ﬃﬃaﬃ1ﬃﬃ2ﬃﬃpi ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ english stop - words [ 15 ]. i¼1 i¼1 ð2þ vector space model asshowninfig . 2 , supposethere istwopoint ’ sp1and p2 , distance within points increases amathematicalmodelisalsocalledthetermvectormodel , similarity points decreases vice versa . whichdescribestextdocumentsasidentiﬁervariables , 1 ( cid : 5 ) cosinesimilarity¼cosinedistance terms tokens . course , term depends comparisons , usually , words , keywords sen - theresultoftheanglewillshow theresult . ifthe angle tences compared . is0betweenthedocumentvectorsthenthecosinefunction 1 documents . angel feature vectors value cosine function less 1 . angle reach - 1 documents artiﬁcial intelligence feature vector n - dimen - completely different ? thus way calculating sional vector computational features describe cosine angle vectors p1 p2 decides entity . really important method calculating vectors pointing direction . semanticsimilarityamongtexts . methodswereusedduring ( 2 ) jaccard similarity jaccard similarity calculates experiment measure function vectors tf - idf similaritiesamongsets . ’ sdeﬁnedastheintersectionsize ( term frequency - inverse document frequency ) sim - ple algorithm transforming text meaningful representation numbers . tf - idf weight measure factwhichevaluatestheimportanceofaspeciﬁcwordina text . mathematics , ( cid : 2 ) ( cid : 3 ) x n tfidf weight¼ tf ( cid : 3 ) log ð1þ ; df i2d document , tf number occurrences , theithterm , df isthenumber ofdocumentswhichcontain ithterm ; nis total number documents . thesklearn - vectorized function wasused toconstructatf - idffunction . thiswholemodelwasconstructedbyusingthedocuments , andagroupofsuchtf - idfvectors wasgeneratedconsisting tf idf weight term documents . tf - fig . 2 cosinesimilarity 123 j . inst . eng . indiaser . b divided bythe unionsize oftwo sets . jaccard similitude ac2 ¼ab2þbc2 ð6þ determined using formula [ 16 ] . pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ac ¼ ab2þbc2 ð7þ b b jða ; bþ¼ ¼ ð3þ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ b ‘ jajþjbj ( cid : 5 ) ja bj ac ¼ ðx2 ( cid : 5 ) x1þ2þðy2 ( cid : 5 ) y1þ2 ð8þ \\\\ representsintersectionand [ representstheunion . sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ inthispaper , aandbarebagsofwordsthatcontainnews jx ! yj¼ xðx ( cid : 5 ) yþ2 ð9þ articles . i¼1 • jaccard ( , ) = 1 table 1 shows comparative analysis methods • jaccard ( , b ) = 0 b = 0 based relative pros cons . table also • b ’ size describes application areas selected • always assign number 0 1 . techniques used . jaccard distance instead similarity measures similarity score dissimilarity found subtracting jaccard similarity coefﬁcient 1 : similarityscoremeansthattwodatasetsarehowsimilarto jdða ; bþ¼1 ( cid : 5 ) jdða ; bþ ð4þ one another . data collection include two separate ja [ bj ( cid : 5 ) ja [ bj textsasinthiscase . thesimilaritybetweenthetwotextsis jdða ; bþ¼ ð5þ ja [ bj evaluated according scoring system . euclidean dis - tance ﬁnd similarity texts , ( 3 ) euclidean distance another similarity measure ﬁndsthemetric , thedistancebetweenbothtexts [ 18 ]; vector space model euclidean distance l2 different ways calculate similarity : distance , euclidean norm . similarity measure 1 differentiates similarity measurements similarityða ; bþ¼ ð10þ 1þdistanceða ; bþ vector space model judging angle like rest rather direct distance vector noun phrase extraction inputs . asshowninfig . 3 , iftherearetwopointslike ( x1 , y1 ) noun phrase extraction technique text analysis , ( x2 , y2 ) let us consider dimension point consistingoftheautomatedextractionofnounsinatext . one wants ﬁnd distance ( x1 , y1 ) helps summarize contents text identify ( x2 , y2 ) basically use particular parameter like key topics discussed . paper concludes euclideandistancetocheckthatifthisparticularpointsare extraction frequency noun phrases fre - nearertoeachotherthanitwillconsiderthatthistwo - point quencyofthenextwordofthenounfromnewsarticlescan similar . euclidean distance calcu - considerably improve similarity measures . textblob latedbasedonthepythagorastheorem . letdrepresentthe python module used extract noun [ 19 ]. measureofdistancesbetween ( x1 , y1 ) ( x2 , y2 ). hence distance c expressed : proposed method paper introduces two methods calculating similaritybetweentwoarticlesofthesamenews , whichare present two different languages ( hindi english ), based methods calculating feature vector similarity measures . cosine similarity jaccard similarity tf - idf vectors thepre - processednewsarticleswereturnedintovectorsof tf - idf using vectorized model tf - idf . vectors obtainedwereasparse - matrixcontainingtf - idfweightsfor news article word dimensions [ number fig . 3 euclideandistance 123 j . inst . eng . indiaser . b table1 comparisonoftheprosandconsofdifferentmeasuresandtheirapplicationarea si . similarity pros cons applicationarea measures 1 cosine bothcontinuousandcategoricalvariablesmaybe ’ tworkeffectively textmining , documentsimilarity similarity used withnominaldata [ 17 ] 2 jaccard bothcontinuousandcategoricalvariablesmaybe ’ tworkeffectively documentclassiﬁcation coefﬁcient used withnominaldata 3 euclidean easytocomputeandworkwellwithadatasetwith doesnotworkwithimage applicationinvolvingintervaldata , dna distance compactorisolatedclusters [ 17 ] dataefﬁciently analysis , k - meanalgorithm news articles * number features ( distinct words )] [ 16 ]. bag words euclidean distance tf - idf weight matrix used featureforeverytext , andsimilarityamongnewsarticlesis pre - processed documents described calculated using cosine similarity jaccard similarity . vector frequency word compare sklearn ’ sbuilt - incosineandjaccardsimilaritymodulewas similar comparing bag vector words . used measure similarity . experiment uses bag - - words model computer processes vectors much faster vast ﬁle table2 samplepairofcompletelysimilarnews table3 samplepairofdifferentnewsstoriesaboutthesametopic 123 j . inst . eng . indiaser . b table4 samplepairofcompletelydissimilarnews performedonpairsofnewsheadlineobtainedfromgoogle news [ 14 ]. thechosennewsarticlesarelistedintables 2 , 3and4 . thenewsarticlesweregiventoahumanexpertto judge similarity dissimilarity . result , human expert determined 6 pairs ( pair 1 – 6 ) com - pletely similar news 5 pairs ( pair 7 – 11 ) different news topic 5 pairs ( pair 12 – 16 ) completely dissimilar news . expert judg - ment used benchmark evaluate automatic similarity calculation news articles . cosine similarity , jaccard coefﬁcient , euclidean distance applied . result three measures shown tables 5 , 6 7 . toprovideabetterunderstandingofthethreecompared measures , theresults areshownonabargraph asdepicted fig . 4 . figure 5 shows similarity measures bar graph different news stories topic . figure 6 shows similarity measures bar graph completely dissimilar news . performance measures used experiment accuracy , precision , recallandf - measures . thesemeasures calculated determining number news articles correctlyidentiﬁedassimilarordissimilarcomparedtothe decisionsbyhumanexperts [ 21 ]. inotherwords , usingthe human decisions benchmark number true pos - itive ( tp ) equivalent actual similar news cor - rectly identiﬁed similar , true negative ( tn ) equivalent actual dissimilar news correctly identiﬁed dissimilar , falsepositive ( fp ) whichisequivalenttoactual similar news incorrectly identiﬁed dissimilar , false - text lot data [ 20 ]. paper load news negative ( fn ) whichisequivalenttoactualdissimilarnews articles list called corpus calculate feature incorrectly identiﬁed similar determined . , vectors documents ﬁnally compute accuracy calculated ( tp ? tn )/ data , precision euclideandistanceandthentocheckhowsimilartheyare . tp /( tp ? fp ), recallistp /( tp ? fn ) andthef - measures greaterthedistance , lesssimilartheyare . thispaperusesa harmonic mean precision recall , module library called sklearn machine equal 2tp /( 2tp ? fp ? fn ) [ 21 ]. results pre - learning library . sented next section . results discussion result analysis figure 7 presents graph similarity measurements proposed algorithms implemented using python sample pair news articles using euclidean , jaccard 3 . 7 . 3 ( 64 - bit ). fortheexperiment , around1000newsstories cosine similarity measures representation wererandomlypickedfromthedataset . thealgorithmruns schemes . e . tf - idf , bag word representation . dataset , measures compares various learned fig . 7 , cosine performs similar similarity score . every news article ’ similarity benchmarkfornewswiththesamemeaning ( pair1 – 6 ) calculated every article . different news topic ( pair 7 – 11 ) - everforcompletelydissimilarnews ( pair12 – 16 ) jaccard ’ comparative analysis euclidean score similar human benchmark . toproveourpointfurther , wecalculatedthecorrelation analyze performance representation method scores similarity measures human different similarity measures , experiment benchmark shown table 8 . 123 j . inst . eng . indiaser . b table5 similaritymeasuresofcompletelysamenews cosine jaccard euclidean 1 . . cosinesimilarity jaccardsimilarity euclideansimilarity 0 . 8 1 0 . 8931 0 . 38075 0 . 04679 0 . 6 2 0 . 856 0 . 2134 0 . 02764 3 0 . 8476 0 . 3589 0 . 04861 0 . 4 4 0 . 7434 0 . 3289 0 . 04610 0 . 2 5 0 . 8034 0 . 2086 0 . 08609 0 6 0 . 7899 0 . 2756 0 . 02440 news 1 news 2 news 3 news 4 news 5 fig . 5 comparison similarity coefﬁcients different news articlesaboutthesametopic table6 similarity measures different news stories cosine jaccard euclidean sametopic 1 . . cosinesimilarity jaccardsimilarity euclideansimilarity 0 . 8 7 0 . 7063 0 . 1168 0 . 02311 0 . 6 8 0 . 5301 0 . 047 0 . 01377 0 . 4 9 0 . 5459 0 . 1516 0 . 04511 0 . 2 10 0 . 6316 0 . 1196 0 . 03771 0 11 0 . 7182 0 . 132 0 . 02990 news 1 news 2 news 3 news 4 news 4 fig . 6 comparison similarity coefﬁcients completely dissim - ilarnews table7 similaritymeasuresofcompletelydissimilarnews . . cosinesimilarity jaccardsimilarity euclideansimilarity human cosine jaccard euclidean 12 0 . 3447 0 . 066 0 . 0434 1 13 0 . 4032 0 . 0705 0 . 00804 es 0 . 8 ur 14 0 . 4843 0 . 0996 0 . 0334 ea 0 . 6 15 0 . 5490 0 . 1003 0 . 0298 0 . 4 16 0 . 3466 0 . 08503 0 . 02949 arit mil 0 . 2 si 0 1 2 3 4 5 6 7 8 9 10111213141516 cosine jaccard euclidean news hadlines 1 fig . 7 similarityscoregraph 0 . 8 0 . 6 0 . 4 ( tables 9 , 10 , 11 ) ﬁnd accuracy , precision , recallandf - measuresasexplainedintheprevioussection . 0 . 2 table 12 gives clear picture performance 0 similarity measure . analyzing results see news 1 news 2 news 3 news 4 news 5 news 6 theprecisionvalueofjaccardmeasuresis1 . 0or100 % fig . 4 comparison similarity coefﬁcients articles less 50 % euclidean distance . however , euclidean news gives high value recall compared precision . cosine measuregives good accuracy level andf1 score , correlation score table 8 , per - difference recall value precision ceived cosine jaccard similarity cor - high . , among three methods cosine similarity related benchmark scores . analyze usingtf - idfshowedgreateraccuracy , recallandf - measure produced result calculating confusion matrix [ 3 ] scores 81 . 25 %, 100 % 76 . 92 %, respectively . 123 j . inst . eng . indiaser . b conclusion table8 correlationofthesimilarityscorestothebenchmark method correlation ongoing research conducted comparison three cosineandbenchmark 0 . 919847 different methods estimate semantic similarity jaccardandbenchmark 0 . 816131 among two news articles ( nearly ) topic / event euclideanandbenchmark 0 . 422671 measure similarity two different languages ( hindi english ). experiment tested using googlenews data sets . three methodologies arethesimilarityofcosinewithtf - idfvectors , similarityof jaccard tf - idf vectors , bag words euclidean dis - table9 confusionmatrixforcosinesimilarity tance . allthreeofthesemethodsshowedpromisingresults , among three methods , cosine similarity using tf - 16news predicted : predicted : yes idf showed greater accuracy , recall f - measure scores total : 8 8 81 . 25 %, 100 % 76 . 92 %, respectively . accuracy actual : tn = 8 fp = 3 two methods may improved actual : yes fn = 0 tp = 5 doc2vec model [ 6 ], takes text corpus input thresholdvalue : 0 . 788 generates document vectors output . experiment totalreﬁnednews : 8 also looking expand work languages . table10 confusionmatrixforjaccardsimilarity references 16news predicted : predicted : yes 1 . g . atkins , . weigle , andm . nelson , measuringnewssimilarity acrosstenu . . newssites , arxivpreprintarxiv , pp . 1 – 11 , 2018 total : 12 4 2 . j . gibson , b . wellner , . lubar , identiﬁcation duplicate actual : tn = 8 fp = 0 news stories web pages , mitre corporation 202 actual : yes fn = 4 tp = 4 burlington rd . bedford 01730 usa , 202 burlington rd . thresholdvalue : 0 . 245 bedfordma01730usa , 2008 3 . . singh , . . kumar , . v . goyal , review techniques totalreﬁnednews : 4 extraction bilingual lexicon comparable corpora . int . j . eng . technol . 7 ( 2 ), 16 – 20 ( 2018 ) 4 . . montalvo , r . mart ´ ınez , . casilla , bilingualnewsclustering using named entities fuzzy similarity ( springer , heidel - table11 confusionmatrixforeuclideansimilarity berg , 2007 ), pp . 108 – 114 5 . . mohd saad . . kamarudin , comparative analysis news predicted : predicted : yes similarity measuresforsentence levelsemanticmeasurement total : 8 8 text . ieee international conference control system , computingandengineering , pp . 90 – 94 , 2013 actual : tn = 8 fp = 7 6 . p . sitikhu , comparison semantic similarity methods actual : yes fn = 0 tp = 1 maximum human interpretability , 2019 . arxiv : 1910 . 09129v2 thresholdvalue : 0 . 0529 [ cs . ir ] 7 . k . baraniak . sydow , news articles similarity auto - totalreﬁnednews : 8 maticmediabiasdetectioninpolishnewsportals , inproceedings federated conference computer science infor - mationsystems , 2018 8 . . b . magara . zuva , comparative analysis text similarity measures algorithms research paper recom - mendersystems , inconferenceoninformationcommunications table12 accuracylevelofeachsimilaritymeasures technologyandsociety ( ictas ), 2018 similaritymeasures performancemeasures 9 . v . thada , . v . jaglan , comparison jaccard , dice , cosine similarity coefﬁcient ﬁnd best ﬁtness value web retrieved accuracy precision recall f - measure documents using genetic algorithm . int . j . innov . eng . technol . ( ijiet ) 2 ( 4 ), 202 – 204 ( 2013 ) cosine 0 . 8125 62 . 5 1 . 0 0 . 7692 10 . . . nasab , anewapproachforﬁndingsemanticsimilarscien - jaccard 0 . 750 1 . 0 0 . 50 0 . 666 tiﬁc articles . j . adv . comput . sci . technol . ( jacst ) 4 , 563 - 59 euclidean 0 . 5625 0 . 125 1 . 0 0 . 222 ( 2015 ) 11 . . snover , b . dorr , andr . schwartz , languageandtranslation thehighestvalueisshowninbold model adaptation using comparable corpora , proceedings 123 j . inst . eng . indiaser . b the2008conferenceonempiricalmethodsinnaturallanguage https :// dataaspirant . com / 2015 / 04 / 11 / ﬁve - - popular - processing , honolulu , 2008 similarity - measures - implementation - - python / 12 . q . longhuaandw . hongling , bilinguallexiconconstruction 17 . . goswami , . babu , b . purkayastha , comparative analysis fromcomparable corpora viadependencymapping , inproceed - similarity measures . int . j . manag . technol . eng . 8 ( xi ), ingsofcoling , 2012 786 – 797 ( 2018 ) 13 . . ydanideahl @ danideahl , googlenewsisgettinganoverhaul 18 . . ali , textualsimilarity , issn2011 - 19 , 2011 andcustomizednewsfeeds , theverge , 8may2018 .[ online ]. 19 . textblob : simpliﬁed text processing , [ online ]. available : available : https :// textblob . readthedocs . io / en / dev / https :// www . theverge . com / 2018 / 5 / 8 / 17329074 / google - news - 20 . bag words euclidean distance , [ online ]. available : update - new - features - newsstand - io - 2018 https :// pythonprogramminglanguage . com / bag - - words - 14 . h . hu , ‘‘ googlenews . pypi ,’’ pypi . org , mar 13 , 2020 . [ online ]. euclidean - distance / available : https :// pypi . org / project / googlenews / 21 . . . kamaruddi , graph - based representation sentence simi - 15 . j . brownlee , howtocleantextformachinelearningwithpython , larity measure : comparative analysis . int . j . eng . technol . machine learning mastery , october 18 , 2017 . [ online ]. avail - 7 ( 2 . 4 ), 32 – 35 ( 2018 ) able : https :// machinelearningmastery . com / clean - text - machine - publisher ’ note springer nature remains neutral regard learning - python / jurisdictionalclaimsinpublishedmapsandinstitutionalafﬁliations . 16 . . polamuri , five popular similarity measures implemen - 123'],\n",
       "      dtype='<U55839')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c8cd515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3727 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4272 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5e054880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24, 25,  3, ...,  7,  2,  0],\n",
       "       [ 2,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  1,  1,  1]], dtype=int64)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dense representation \n",
    "# warning might give a memory error if data is too big\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c7a70caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3727)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "51d6cf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3727"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f7e1293d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>0000000005</th>\n",
       "      <th>0000110088</th>\n",
       "      <th>00010</th>\n",
       "      <th>00015</th>\n",
       "      <th>00020</th>\n",
       "      <th>00025</th>\n",
       "      <th>0003</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬁndsthemetric</th>\n",
       "      <th>ﬁne</th>\n",
       "      <th>ﬁner</th>\n",
       "      <th>ﬁnest</th>\n",
       "      <th>ﬁnite</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬁtness</th>\n",
       "      <th>ﬁve</th>\n",
       "      <th>ﬂow</th>\n",
       "      <th>ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  0000000005  0000110088  00010  00015  00020  00025  0003  \\\n",
       "0  24   25     3           0           1      0      0      0      0     1   \n",
       "1   2    0     0           1           0      1      1      1      1     0   \n",
       "2   0    0     0           0           0      0      0      0      0     0   \n",
       "\n",
       "   ...  ﬁndsthemetric  ﬁne  ﬁner  ﬁnest  ﬁnite  ﬁrst  ﬁtness  ﬁve  ﬂow  \\\n",
       "0  ...              0    3     2      1      1     6       0    7    2   \n",
       "1  ...              0    0     0      0      0     1       0    0    0   \n",
       "2  ...              1    0     0      0      0     1       2    1    1   \n",
       "\n",
       "   ﬃpﬃﬃﬃﬃﬃnﬃiﬃﬃﬃﬃﬃbﬃﬃﬃ2ﬃﬃ  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       1  \n",
       "\n",
       "[3 rows x 3727 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bc1c5e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.121543</td>\n",
       "      <td>0.126626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.121543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.134751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126626</td>\n",
       "      <td>0.134751</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.121543  0.126626\n",
       "1  0.121543  1.000000  0.134751\n",
       "2  0.126626  0.134751  1.000000"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_matrix = cosine_similarity(cv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfdd4a1",
   "metadata": {},
   "source": [
    "## Extracting pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74411fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
